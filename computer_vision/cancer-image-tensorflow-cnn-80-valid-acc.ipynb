{"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Author:** Raoul Malm  \n\n**Abstract:**  \n\nThe dataset consists of 5547 breast histology images each of pixel size 50 x 50 x 3. The goal is to classify cancerous images (IDC : invasive ductal carcinoma) vs non-IDC images. In a first step we analyze the images and look at the distribution of the pixel intensities. Then, the images are normalized and we try out some basic classification algorithms like logistic regregession, random forest, decision tree and so on. We validate and compare each of these base models.  After that we implement the following neural network architecture:\n\n- input layer: [., 50, 50, 3]\n- layer: Conv1 -> ReLu -> MaxPool: [., 25, 25, 36] \n- layer: Conv2 -> ReLu -> MaxPool: [., 13, 13, 36]\n- layer: Conv3 -> ReLu -> MaxPool: [., 7, 7, 36]\n- layer: FC -> ReLu: [., 576]\n- output layer: FC -> ReLu: [., 2]\n\nThe neural network is implemented as a python class and the complete TensorFlow session can be saved to or restored from a file. We also implement tensor summaries, which can be visualized with TensorBoard. The output layer gives for each image a probability for IDC=0 and IDC=1. In order to prevent overfitting the training data we generate new images by rotations, translations and zoom.\n\n**Results:** \n\n- Using 10-fold cross-validation the best base models can achieve an accuracy of roughly 76% on the validation sets. \n\n- Implementing a 90%/10% split for the training and validation data and training the neural network for 30 epochs while using data augmentation we can achieve an accuracy of 80% on the validation sets. Still, I think there is a lot of room for improvement here since I have not spend much time on tuning the model. This takes roughly 45 minutes on the kaggle hardware. \n\n**Outline:**  \n\n[1. Libraries and settings](#1-bullet)  \n[2. Analyze data](#2-bullet)  \n[3. Manipulate data](#3-bullet)  \n[4. Try out sklearn base models](#4-bullet)  \n[5. Build TensorFlow Graph](#5-bullet)  \n[6. Train and validate the neural network](#6-bullet)  \n\n**Reference:**\n\n[Predicting IDC in Breast Cancer Histology Images by paultimothymooney](https://www.kaggle.com/paultimothymooney/predicting-idc-in-breast-cancer-histology-images/notebook)\n\n\n","metadata":{"_cell_guid":"d1978fcb-da9e-437c-b6c7-137d0c99d55a","_uuid":"9ac48023caa236a18a2150800b7aebcb1d6d6a30"}},{"cell_type":"markdown","source":"# 1. Libraries and settings <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{"_cell_guid":"0dfad1cf-75be-4e55-95d0-923ae65b86fe","_uuid":"f081ddc6c72e6f0569f484e015f43a2eef46ad7e"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport keras.preprocessing.image\nimport sklearn.preprocessing\nimport sklearn.model_selection\nimport sklearn.metrics\nimport sklearn.linear_model\nimport sklearn.naive_bayes\nimport sklearn.tree\nimport sklearn.ensemble\nimport os;\nimport datetime  \nimport cv2 \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm  \n%matplotlib inline","metadata":{"_cell_guid":"9151d862-1b62-4dce-b96d-146ab2cd372d","_uuid":"09cde641a643ba8ce053e996d81653b732f23ff3","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Analyze data <a class=\"anchor\" id=\"2-bullet\"></a> ","metadata":{"_cell_guid":"96fbb1cb-4cf2-4824-a441-7e8ee00e0d95","_uuid":"8e97b9dabec517637accb2990120593d0c9651ed"}},{"cell_type":"code","source":"## import data\n\n# load images of shape (5547, 50, 50, 3)\nx_images = np.load('../input/X.npy')  \n\n# load labels of shape (5547,1); (0 = no cancer, 1 = cancer)\ny_images = np.load('../input/Y.npy')   \n\n# shuffle data\nperm_array = np.arange(len(x_images))\nnp.random.shuffle(perm_array)\nx_images = x_images[perm_array]\ny_images = y_images[perm_array]\n\nprint('x_images.shape =', x_images.shape)\nprint('x_images.min/mean/std/max = %.2f/%.2f/%.2f/%.2f'%(x_images.min(),\n                        x_images.mean(), x_images.std(), x_images.max()))\nprint('')\nprint('y_images.shape =', y_images.shape)\nprint('y_images.min/mean/std/max = %.2f/%.2f/%.2f/%.2f'%(y_images.min(),\n                        y_images.mean(), y_images.std(), y_images.max()))","metadata":{"_cell_guid":"05e1d9a4-3da9-4ffe-b3db-2a39a08b21f4","_uuid":"6af80912511dd39bf1788901b9da7d873b290864","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## plot some images  \n\nimgs_0 = x_images[y_images == 0] # 0 = no cancer\nimgs_1 = x_images[y_images == 1] # 1 = cancer\n\nplt.figure(figsize=(20,20))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    plt.title('IDC = %d'%y_images[i])\n    plt.imshow(x_images[i])","metadata":{"_cell_guid":"e1530ff1-ca8e-4f07-ac37-d45090581277","_uuid":"9d69ecf282ffe5d6525228ca75c6a66985c4cee6","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,3))\n\nfor i in range(2):\n    n_img = np.random.randint(len(x_images))\n    plt.subplot(1,4,1+2*i)\n    plt.title('IDC = %d'%y_images[n_img])\n    plt.imshow(x_images[n_img])\n    plt.subplot(1,4,2+2*i)\n    plt.title('IDC = %d'%y_images[n_img])\n    plt.ylabel('Count')\n    plt.xlabel('Pixel Intensity')\n    plt.hist(x_images[n_img,:,:,0].flatten(), bins=30, lw = 0,color='r', alpha=0.5);\n    plt.hist(x_images[n_img,:,:,1].flatten(), bins=30, lw = 0, color='g', alpha=0.5);\n    plt.hist(x_images[n_img,:,:,2].flatten(), bins=30, lw = 0, color='b', alpha=0.5);\n\nprint('red channel: min/mean/std/max = %.2f/%.2f/%.2f/%.2f'%(x_images[:,:,:,0].min(),\n    x_images[:,:,:,0].mean(), x_images[:,:,:,0].std(), x_images[:,:,:,0].max()))\nprint('green channel: min/mean/std/max = %.2f/%.2f/%.2f/%.2f'%(x_images[:,:,:,1].min(),\n    x_images[:,:,:,1].mean(), x_images[:,:,:,1].std(), x_images[:,:,:,1].max()))\nprint('blue channel: min/mean/std/max = %.2f/%.2f/%.2f/%.2f'%(x_images[:,:,:,2].min(),\n    x_images[:,:,:,2].mean(), x_images[:,:,:,2].std(), x_images[:,:,:,2].max()))\n","metadata":{"_cell_guid":"7d17d697-9774-4b21-9d4e-f111bdf02cdb","_uuid":"925b520d65b5c3f00a1c5d629912a4c8bb96cf94","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Manipulate data <a class=\"anchor\" id=\"3-bullet\"></a> ","metadata":{"_cell_guid":"09f95d13-686f-4a16-8234-31d0b0dfe37a","_uuid":"347bc719fa624a36e2ed95b94c27f54c30a160af"}},{"cell_type":"code","source":"## normalize data\n\n# convert one-hot encodings into labels\ndef one_hot_to_dense(labels_one_hot):\n    return np.argmax(labels_one_hot,1)\n\n# convert class labels from scalars to one-hot vectors e.g. 1 => [0 1], 0 => [1 0]\ndef dense_to_one_hot(labels_dense, num_classes):\n    num_labels = labels_dense.shape[0]\n    index_offset = np.arange(num_labels) * num_classes\n    labels_one_hot = np.zeros((num_labels, num_classes))\n    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n    return labels_one_hot\n\n# function to normalize data\ndef normalize_data(data): \n    # scale features using statistics that are robust to outliers\n    #rs = sklearn.preprocessing.RobustScaler()\n    #rs.fit(data)\n    #data = rs.transform(data)\n    #data = (data-data.mean())/(data.std()) # standardisation\n    data = data / data.max() # convert from [0:255] to [0.:1.]\n    #data = ((data / 255.)-0.5)*2. # convert from [0:255] to [-1.:+1.]\n    return data\n\n# training and validation data\nx_train_valid = normalize_data(x_images)\n\n# use one-hot encoding for labels 0,1\ny_train_valid = dense_to_one_hot(y_images, 2).astype(np.uint8)\n\n# dictionaries for saving results\ny_valid_pred = {}\ny_train_pred = {}\ny_test_pred = {}\ntrain_loss, valid_loss = {}, {}\ntrain_acc, valid_acc = {}, {}\ncnf_valid_matrix = {}\n\nprint('x_train_valid.shape =', x_train_valid.shape)\nprint('x_train_valid.min/mean/std/max = %.2f/%.2f/%.2f/%.2f'%(x_train_valid.min(),\n                        x_train_valid.mean(), x_train_valid.std(), x_train_valid.max()))\nprint('')\nprint('y_train_valid.shape =', y_train_valid.shape)\nprint('y_train_valid.min/mean/std/max = %.2f/%.2f/%.2f/%.2f'%(y_train_valid.min(),\n                        y_train_valid.mean(), y_train_valid.std(), y_train_valid.max()))","metadata":{"_cell_guid":"b08e6183-1bdf-473f-add8-86680723dea8","_uuid":"ec57b92daa066a70aa5d5fcf1f7e8199fde45b18","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## augment data\n\n# generate new images via rotations, translations, zoom using keras\ndef generate_images(imgs):\n    \n    # rotations, translations, zoom\n    image_generator = keras.preprocessing.image.ImageDataGenerator(\n        rotation_range = 10, width_shift_range = 0.1 , height_shift_range = 0.1,\n        zoom_range = 0.1)\n\n    # get transformed images\n    imgs = image_generator.flow(imgs.copy(), np.zeros(len(imgs)),\n                                batch_size=len(imgs), shuffle = False).next()    \n    return imgs[0]\n\n# check image generation\nfig,axs = plt.subplots(5,6, figsize=(20,15))\nfor i in range(5):\n    n = np.random.randint(0,x_images.shape[0]-2)\n    axs[i,0].imshow(x_images[n])\n    axs[i,1].imshow(generate_images(x_images[n:n+1])[0].astype('uint8'))\n    axs[i,2].imshow(generate_images(x_images[n:n+1])[0].astype('uint8'))\n    axs[i,3].imshow(generate_images(x_images[n:n+1])[0].astype('uint8'))\n    axs[i,4].imshow(generate_images(x_images[n:n+1])[0].astype('uint8'))\n    axs[i,5].imshow(generate_images(x_images[n:n+1])[0].astype('uint8'))","metadata":{"_cell_guid":"db5de7e3-88f3-4f6c-a56b-42c1cbb7f321","_uuid":"cb909e1a3db6eeb8a93415d970da4efc677eb956","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Try out sklearn base models <a class=\"anchor\" id=\"4-bullet\"></a> ","metadata":{"_cell_guid":"ba202f81-ea52-4f92-8675-609d393bd804","_uuid":"d639f1bf65ca525071f8d38fd304021ed5a183b8"}},{"cell_type":"code","source":"## First try out some basic sklearn models\n\n# computet the accuracy of label predictions\ndef accuracy_from_dense_labels(y_target, y_pred):\n    y_target = y_target.reshape(-1,)\n    y_pred = y_pred.reshape(-1,)\n    return np.mean(y_target == y_pred)\n\nlogreg = sklearn.linear_model.LogisticRegression(verbose=0, solver='lbfgs',\n                                                 multi_class = 'multinomial')\ndecision_tree = sklearn.tree.DecisionTreeClassifier()\nextra_trees = sklearn.ensemble.ExtraTreesClassifier(verbose=0, max_depth=4)\ngradient_boost = sklearn.ensemble.GradientBoostingClassifier(verbose=0)\nrandom_forest = sklearn.ensemble.RandomForestClassifier(verbose=0, max_depth=4, n_estimators=10)\ngaussianNB = sklearn.naive_bayes.GaussianNB()\n\n\n# store models in dictionary\nbase_models = {'logreg': logreg, 'extra_trees': extra_trees,\n               'gradient_boost': gradient_boost, 'random_forest': random_forest, \n               'decision_tree': decision_tree, 'gaussianNB': gaussianNB}\n\n# choose models for out-of-folds predictions\n#take_models = ['logreg','random_forest','extra_trees','gaussianNB']\ntake_models = ['logreg', 'random_forest', 'extra_trees', 'gaussianNB']\n\nfor mn in take_models:\n    train_acc[mn] = []\n    valid_acc[mn] = []\n    cnf_valid_matrix[mn] = []\n\n# start timer\nstart = datetime.datetime.now();\nprint(datetime.datetime.now().strftime('%d-%m-%Y %H:%M:%S'),': start training')\n       \n# cross validations\ncv_num = 10 # cross validations default = 20 => 5% validation set\nkfold = sklearn.model_selection.KFold(cv_num, shuffle=True) #, random_state=123)\n\nfor i,(train_index, valid_index) in enumerate(kfold.split(x_train_valid)):\n\n    # start timer\n    start = datetime.datetime.now();\n\n    # train and validation data of original images\n    x_train = x_train_valid[train_index].reshape(-1,7500)\n    y_train = y_train_valid[train_index]\n    x_valid = x_train_valid[valid_index].reshape(-1,7500)\n    y_valid = y_train_valid[valid_index]\n\n    for mn in take_models:\n\n        # create cloned model from base models\n        model = sklearn.base.clone(base_models[mn])\n        model.fit(x_train, one_hot_to_dense(y_train))\n\n        # predictions\n        y_train_pred[mn] = model.predict_proba(x_train)\n        y_valid_pred[mn] = model.predict_proba(x_valid)\n        \n        # accuracies\n        train_acc[mn].append(accuracy_from_dense_labels(one_hot_to_dense(y_train_pred[mn]),\n                                                        one_hot_to_dense(y_train)))\n        valid_acc[mn].append(accuracy_from_dense_labels(one_hot_to_dense(y_valid_pred[mn]),\n                                                        one_hot_to_dense(y_valid)))\n        \n        # normalized confusion matrix\n        cnf_valid_matrix_tmp = sklearn.metrics.confusion_matrix(\n            y_pred = one_hot_to_dense(y_valid_pred[mn]), \n            y_true = one_hot_to_dense(y_valid)).astype(np.float32)\n        cnf_valid_matrix_tmp[0,:] = cnf_valid_matrix_tmp[0,:]/cnf_valid_matrix_tmp[0,:].sum()\n        cnf_valid_matrix_tmp[1,:] = cnf_valid_matrix_tmp[1,:]/cnf_valid_matrix_tmp[1,:].sum()\n        cnf_valid_matrix[mn].append(cnf_valid_matrix_tmp)\n\n        print(i,': '+mn+' train/valid accuracy = %.3f/%.3f'%(train_acc[mn][-1], \n                                                             valid_acc[mn][-1]))\n\nprint('running time for training: ', datetime.datetime.now() - start)\nprint('')\nfor mn in train_acc.keys():\n    print(mn + ' : averaged train/valid accuracy = %.3f/%.3f'%(np.mean(train_acc[mn]),\n                                                              np.mean(valid_acc[mn])))","metadata":{"_cell_guid":"f14cfb2b-6420-4a77-8c3c-08cde8e12131","_uuid":"94aa742773fcdf3aeeeaf789691711adb7182f6f","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## show confusion matrix of one model\n\n# choose model\nmn = 'logreg'\n\n# plot\nlabels_array = ['IDC = 0', 'IDC = 1']\nfig, ax = plt.subplots(1,figsize=(5,5))\nax = sns.heatmap(cnf_valid_matrix[mn][0], ax=ax, cmap=plt.cm.Greens, annot=True)\nax.set_xticklabels(labels_array)\nax.set_yticklabels(labels_array)\nplt.title('Confusion matrix of validation set')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show();","metadata":{"_cell_guid":"369d004c-a848-4b51-ba73-832afce2a2c7","_uuid":"fa5f9cc4a2c87e3400204ea931266d8b539f5555","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## compare base models\n\n# boxplot algorithm comparison\nfig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(1,2,1)\nplt.title('Train accuracy')\nplt.boxplot([train_acc[mn] for mn in train_acc.keys()])\nax.set_xticklabels([mn for mn in train_acc.keys()])\nax.set_ylabel('Accuracy');\n\nax = fig.add_subplot(1,2,2)\nplt.title('Valid accuracy')\nplt.boxplot([valid_acc[mn] for mn in train_acc.keys()])\nax.set_xticklabels([mn for mn in train_acc.keys()])\nax.set_ylabel('Accuracy');\n\nfor mn in train_acc.keys():\n    print(mn + ' averaged train/valid accuracy = %.3f/%.3f'%(np.mean(train_acc[mn]),\n                                                             np.mean(valid_acc[mn])))","metadata":{"_cell_guid":"e9a4fe6c-9019-45dd-8e52-ba4784901b52","_uuid":"be15b534b10bf382cfbddbb65905008404dc8837","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Build neural network with TensorFlow <a class=\"anchor\" id=\"5-bullet\"></a> ","metadata":{"_cell_guid":"5aa27bda-0173-49af-a49a-5511c34e1fff","_uuid":"6e932f1749f2c4ae37dd5d7be0d16b76998699b4"}},{"cell_type":"code","source":"## build the neural network class\n\nclass nn_class:\n# class that implements the neural network\n\n    # constructor\n    def __init__(self, nn_name = 'tmp', log_step = 0.1, keep_prob = 0.33, mb_size = 50,\n                 width = 50, height = 50, n_channel = 3, n_output = 2):\n\n        # tunable hyperparameters for nn architecture\n        self.s_f_conv1 = 3 # filter size of first convolution layer (default = 3)\n        self.n_f_conv1 = 36 # number of features of first convolution layer (default = 36)\n        self.s_f_conv2 = 3 # filter size of second convolution layer (default = 3)\n        self.n_f_conv2 = 36 # number of features of second convolution layer (default = 36)\n        self.s_f_conv3 = 3 # filter size of third convolution layer (default = 3)\n        self.n_f_conv3 = 36 # number of features of third convolution layer (default = 36)\n        self.n_n_fc1 = 576 # number of neurons of first fully connected layer (default = 576)\n        self.n_channel = n_channel\n        self.width = width\n        self.height = height\n        self.n_output = n_output\n        \n        # tunable hyperparameters for training\n        self.mb_size = mb_size # mini batch size\n        self.keep_prob = keep_prob # keeping probability with dropout regularization \n        self.learn_rate_array = [10*1e-4, 7.5*1e-4, 5*1e-4, 2.5*1e-4, 1*1e-4, 1*1e-4,\n                                 1*1e-4, 0.75*1e-4, 0.5*1e-4, 0.25*1e-4, 0.1*1e-4, \n                                 0.1*1e-4, 0.075*1e-4,0.050*1e-4, 0.025*1e-4, 0.01*1e-4, \n                                 0.0075*1e-4, 0.0050*1e-4,0.0025*1e-4,0.001*1e-4]\n        self.learn_rate_step_size = 3 # in terms of epochs\n        \n        # helper variables\n        self.learn_rate = self.learn_rate_array[0]\n        self.learn_rate_pos = 0 # current position pointing to current learning rate\n        self.index_in_epoch = 0 \n        self.current_epoch = 0\n        self.log_step = log_step # log results in terms of epochs\n        self.n_log_step = 0 # counting current number of mini batches trained on\n        self.use_tb_summary = False # True = use tensorboard visualization\n        self.use_tf_saver = False # True = use saver to save the model\n        self.nn_name = nn_name # name of the neural network\n        self.perm_array = np.array([]) # permutation array\n        \n    # function to get the next mini batch\n    def next_mini_batch(self):\n\n        start = self.index_in_epoch\n        self.index_in_epoch += self.mb_size\n        self.current_epoch += self.mb_size/len(self.x_train)  \n        \n        # adapt length of permutation array\n        if not len(self.perm_array) == len(self.x_train):\n            self.perm_array = np.arange(len(self.x_train))\n        \n        # shuffle once at the start of epoch\n        if start == 0:\n            np.random.shuffle(self.perm_array)\n\n        # at the end of the epoch\n        if self.index_in_epoch > self.x_train.shape[0]:\n            np.random.shuffle(self.perm_array) # shuffle data\n            start = 0 # start next epoch\n            self.index_in_epoch = self.mb_size # set index to mini batch size\n            \n            if self.train_on_augmented_data:\n                # use augmented data for the next epoch\n                self.x_train_aug = normalize_data(self.generate_images(self.x_train))\n                self.y_train_aug = self.y_train\n                \n        end = self.index_in_epoch\n        \n        if self.train_on_augmented_data:\n            # use augmented data\n            x_tr = self.x_train_aug[self.perm_array[start:end]]\n            y_tr = self.y_train_aug[self.perm_array[start:end]]\n        else:\n            # use original data\n            x_tr = self.x_train[self.perm_array[start:end]]\n            y_tr = self.y_train[self.perm_array[start:end]]\n        \n        return x_tr, y_tr\n               \n    # generate new images via rotations, translations, zoom using keras\n    def generate_images(self, imgs):\n    \n        print('generate new set of images')\n        \n        # rotations, translations, zoom\n        image_generator = keras.preprocessing.image.ImageDataGenerator(\n            rotation_range = 10, width_shift_range = 0.1 , height_shift_range = 0.1,\n            zoom_range = 0.1)\n\n        # get transformed images\n        imgs = image_generator.flow(imgs.copy(), np.zeros(len(imgs)),\n                                    batch_size=len(imgs), shuffle = False).next()    \n\n        return imgs[0]\n\n    # weight initialization\n    def weight_variable(self, shape, name = None):\n        initial = tf.truncated_normal(shape, stddev=0.1)\n        return tf.Variable(initial, name = name)\n\n    # bias initialization\n    def bias_variable(self, shape, name = None):\n        initial = tf.constant(0.1, shape=shape) #  positive bias\n        return tf.Variable(initial, name = name)\n\n    # 2D convolution\n    def conv2d(self, x, W, name = None):\n        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', name = name)\n\n    # max pooling\n    def max_pool_2x2(self, x, name = None):\n        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n                              padding='SAME', name = name)\n\n    # attach summaries to a tensor for TensorBoard visualization\n    def summary_variable(self, var, var_name):\n        with tf.name_scope(var_name):\n            mean = tf.reduce_mean(var)\n            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n            tf.summary.scalar('mean', mean)\n            tf.summary.scalar('stddev', stddev)\n            tf.summary.scalar('max', tf.reduce_max(var))\n            tf.summary.scalar('min', tf.reduce_min(var))\n            tf.summary.histogram('histogram', var)\n    \n    # function to create the graph\n    def create_graph(self):\n\n        # reset default graph\n        tf.reset_default_graph()\n\n        # variables for input and output \n        self.x_data_tf = tf.placeholder(dtype=tf.float32, shape=[None,self.height,\n                                        self.width,self.n_channel], name='x_data_tf')\n        self.y_data_tf = tf.placeholder(dtype=tf.float32, shape=[None, self.n_output],\n                                        name='y_data_tf')\n\n        # 1.layer: convolution + max pooling\n        self.W_conv1_tf = self.weight_variable([self.s_f_conv1, self.s_f_conv1, self.n_channel,\n                                                self.n_f_conv1], name='W_conv1_tf') # (3,3,3,36)\n        self.b_conv1_tf = self.bias_variable([self.n_f_conv1], name = 'b_conv1_tf') # (36)\n        self.h_conv1_tf = tf.nn.relu(self.conv2d(self.x_data_tf, self.W_conv1_tf) \n                                     + self.b_conv1_tf, name = 'h_conv1_tf') # (.,50,50,36)\n        self.h_pool1_tf = self.max_pool_2x2(self.h_conv1_tf, \n                                            name = 'h_pool1_tf') # (.,25,25,36)\n        \n        # 2.layer: convolution + max pooling\n        self.W_conv2_tf = self.weight_variable([self.s_f_conv2, self.s_f_conv2, \n                                self.n_f_conv1, self.n_f_conv2], name = 'W_conv2_tf')\n        self.b_conv2_tf = self.bias_variable([self.n_f_conv2], name = 'b_conv2_tf')\n        self.h_conv2_tf = tf.nn.relu(self.conv2d(self.h_pool1_tf, \n                        self.W_conv2_tf) + self.b_conv2_tf, name ='h_conv2_tf') #(.,25,25,36)\n        self.h_pool2_tf = self.max_pool_2x2(self.h_conv2_tf, name = 'h_pool2_tf') #(.,13,13,36)\n\n        # 3.layer: convolution + max pooling\n        self.W_conv3_tf = self.weight_variable([self.s_f_conv3, self.s_f_conv3, \n                        self.n_f_conv2, self.n_f_conv3], name = 'W_conv3_tf')\n        self.b_conv3_tf = self.bias_variable([self.n_f_conv3], name = 'b_conv3_tf')\n        self.h_conv3_tf = tf.nn.relu(self.conv2d(self.h_pool2_tf, self.W_conv3_tf) + \n                                     self.b_conv3_tf, name ='h_conv3_tf') #(.,13,13,36)\n        self.h_pool3_tf = self.max_pool_2x2(self.h_conv3_tf, name='h_pool3_tf') # (.,7,7,36)\n        \n        # 4.layer: fully connected\n        self.W_fc1_tf = self.weight_variable([7*7*self.n_f_conv3, self.n_n_fc1], \n                                             name='W_fc1_tf') # (7*7*36, 1024)\n        self.b_fc1_tf = self.bias_variable([self.n_n_fc1], name = 'b_fc1_tf') # (1024)\n        self.h_pool3_flat_tf = tf.reshape(self.h_pool3_tf, [-1,7*7*self.n_f_conv3], \n                                          name = 'h_pool3_flat_tf') # (.,1024)\n        self.h_fc1_tf = tf.nn.relu(tf.matmul(self.h_pool3_flat_tf, \n                           self.W_fc1_tf) + self.b_fc1_tf, name = 'h_fc1_tf') # (.,1024)\n      \n        # add dropout\n        self.keep_prob_tf = tf.placeholder(dtype=tf.float32, name = 'keep_prob_tf')\n        self.h_fc1_drop_tf = tf.nn.dropout(self.h_fc1_tf, self.keep_prob_tf, \n                                           name = 'h_fc1_drop_tf')\n\n        # 5.layer: fully connected\n        self.W_fc2_tf = self.weight_variable([self.n_n_fc1, self.n_output],\n                                             name='W_fc2_tf') # (1024,1)\n        self.b_fc2_tf = self.bias_variable([self.n_output], name='b_fc2_tf') # (1024)\n        self.z_pred_tf = tf.add(tf.matmul(self.h_fc1_drop_tf, self.W_fc2_tf), \n                                self.b_fc2_tf, name = 'z_pred_tf')# => (.,1)\n\n        # cost function\n        self.cross_entropy_tf = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n            labels=self.y_data_tf, logits=self.z_pred_tf), name = 'cross_entropy_tf')\n     \n        # optimisation function\n        self.learn_rate_tf = tf.placeholder(dtype=tf.float32, name=\"learn_rate_tf\")\n        self.train_step_tf = tf.train.AdamOptimizer(self.learn_rate_tf).minimize(\n            self.cross_entropy_tf, name = 'train_step_tf')\n\n        # predicted probabilities in one-hot encoding\n        self.y_pred_proba_tf = tf.nn.softmax(self.z_pred_tf, name='y_pred_proba_tf') \n        \n        # tensor of correct predictions\n        self.y_pred_correct_tf = tf.equal(tf.argmax(self.y_pred_proba_tf, 1),\n                                          tf.argmax(self.y_data_tf, 1),\n                                          name = 'y_pred_correct_tf')  \n        \n        # accuracy \n        self.accuracy_tf = tf.reduce_mean(tf.cast(self.y_pred_correct_tf, dtype=tf.float32),\n                                         name = 'accuracy_tf')\n\n        # tensors to save intermediate accuracies and losses during training\n        self.train_loss_tf = tf.Variable(np.array([]), dtype=tf.float32, \n                                         name='train_loss_tf', validate_shape = False)\n        self.valid_loss_tf = tf.Variable(np.array([]), dtype=tf.float32, \n                                         name='valid_loss_tf', validate_shape = False)\n        self.train_acc_tf = tf.Variable(np.array([]), dtype=tf.float32, \n                                        name='train_acc_tf', validate_shape = False)\n        self.valid_acc_tf = tf.Variable(np.array([]), dtype=tf.float32, \n                                        name='valid_acc_tf', validate_shape = False)\n     \n        # number of weights and biases\n        num_weights = (self.s_f_conv1**2*self.n_f_conv1*self.n_channel\n                       + self.s_f_conv2**2*self.n_f_conv1*self.n_f_conv2 \n                       + self.s_f_conv3**2*self.n_f_conv2*self.n_f_conv3 \n                       + 4*4*self.n_f_conv3*self.n_n_fc1 + self.n_n_fc1*self.n_output)\n        num_biases = self.n_f_conv1 + self.n_f_conv2 + self.n_f_conv3 + self.n_n_fc1\n        print('num_weights =', num_weights)\n        print('num_biases =', num_biases)\n        \n        return None  \n    \n    def attach_summary(self, sess):\n        \n        # create summary tensors for tensorboard\n        self.use_tb_summary = True\n        self.summary_variable(self.W_conv1_tf, 'W_conv1_tf')\n        self.summary_variable(self.b_conv1_tf, 'b_conv1_tf')\n        self.summary_variable(self.W_conv2_tf, 'W_conv2_tf')\n        self.summary_variable(self.b_conv2_tf, 'b_conv2_tf')\n        self.summary_variable(self.W_conv3_tf, 'W_conv3_tf')\n        self.summary_variable(self.b_conv3_tf, 'b_conv3_tf')\n        self.summary_variable(self.W_fc1_tf, 'W_fc1_tf')\n        self.summary_variable(self.b_fc1_tf, 'b_fc1_tf')\n        self.summary_variable(self.W_fc2_tf, 'W_fc2_tf')\n        self.summary_variable(self.b_fc2_tf, 'b_fc2_tf')\n        tf.summary.scalar('cross_entropy_tf', self.cross_entropy_tf)\n        tf.summary.scalar('accuracy_tf', self.accuracy_tf)\n\n        # merge all summaries for tensorboard\n        self.merged = tf.summary.merge_all()\n\n        # initialize summary writer \n        timestamp = datetime.datetime.now().strftime('%d-%m-%Y_%H-%M-%S')\n        filepath = os.path.join(os.getcwd(), 'logs', (self.nn_name+'_'+timestamp))\n        self.train_writer = tf.summary.FileWriter(os.path.join(filepath,'train'), sess.graph)\n        self.valid_writer = tf.summary.FileWriter(os.path.join(filepath,'valid'), sess.graph)\n\n    def attach_saver(self):\n        # initialize tensorflow saver\n        self.use_tf_saver = True\n        self.saver_tf = tf.train.Saver()\n\n    # function to train the graph\n    def train_graph(self, sess, x_train, y_train, x_valid, y_valid, n_epoch = 1, \n                    train_on_augmented_data = False):\n\n        # train on original or augmented data\n        self.train_on_augmented_data = train_on_augmented_data\n        \n        # training and validation data\n        self.x_train = x_train\n        self.y_train = y_train\n        self.x_valid = x_valid\n        self.y_valid = y_valid\n        \n        # use augmented data\n        if self.train_on_augmented_data:\n            print('generate new set of images')\n            self.x_train_aug = normalize_data(self.generate_images(self.x_train))\n            self.y_train_aug = self.y_train\n        \n        # parameters\n        mb_per_epoch = self.x_train.shape[0]/self.mb_size\n        train_loss, train_acc, valid_loss, valid_acc = [],[],[],[]\n        \n        # start timer\n        start = datetime.datetime.now();\n        print(datetime.datetime.now().strftime('%d-%m-%Y %H:%M:%S'),': start training')\n        print('learnrate =',self.learn_rate,', n_epoch =', n_epoch,\n              ', mb_size =', self.mb_size, ', nn_name =', self.nn_name)\n        # looping over mini batches\n        for i in range(int(n_epoch*mb_per_epoch)+1):\n\n            # adapt learn_rate\n            if not self.learn_rate_pos == int(self.current_epoch // self.learn_rate_step_size):\n                self.learn_rate_pos = int(self.current_epoch // self.learn_rate_step_size)\n                self.learn_rate = self.learn_rate_array[self.learn_rate_pos]\n                print(datetime.datetime.now()-start,': set learn rate to %.6f'%self.learn_rate)\n            \n            # get new batch\n            x_batch, y_batch = self.next_mini_batch() \n\n            # run the graph\n            sess.run(self.train_step_tf, feed_dict={self.x_data_tf: x_batch, \n                                                    self.y_data_tf: y_batch, \n                                                    self.keep_prob_tf: self.keep_prob, \n                                                    self.learn_rate_tf: self.learn_rate})\n            # store losses and accuracies\n            if i%int(self.log_step*mb_per_epoch) == 0 or i == int(n_epoch*mb_per_epoch):\n             \n                self.n_log_step += 1 # for logging the results\n                \n                feed_dict_train = {\n                    self.x_data_tf: self.x_train[self.perm_array[:len(self.x_valid)]], \n                    self.y_data_tf: self.y_train[self.perm_array[:len(self.y_valid)]], \n                    self.keep_prob_tf: 1.0}\n                \n                feed_dict_valid = {self.x_data_tf: self.x_valid, \n                                   self.y_data_tf: self.y_valid, \n                                   self.keep_prob_tf: 1.0}\n                \n                # summary for tensorboard\n                if self.use_tb_summary:\n                    train_summary = sess.run(self.merged, feed_dict = feed_dict_train)\n                    valid_summary = sess.run(self.merged, feed_dict = feed_dict_valid)\n                    self.train_writer.add_summary(train_summary, self.n_log_step)\n                    self.valid_writer.add_summary(valid_summary, self.n_log_step)\n                \n                train_loss.append(sess.run(self.cross_entropy_tf,\n                                           feed_dict = feed_dict_train))\n\n                train_acc.append(self.accuracy_tf.eval(session = sess, \n                                                       feed_dict = feed_dict_train))\n                \n                valid_loss.append(sess.run(self.cross_entropy_tf,\n                                           feed_dict = feed_dict_valid))\n\n                valid_acc.append(self.accuracy_tf.eval(session = sess, \n                                                       feed_dict = feed_dict_valid))\n\n                print('%.2f epoch: train/val loss = %.4f/%.4f, train/val acc = %.4f/%.4f'%(\n                    self.current_epoch, train_loss[-1], valid_loss[-1],\n                    train_acc[-1], valid_acc[-1]))\n     \n        # concatenate losses and accuracies and assign to tensor variables\n        tl_c = np.concatenate([self.train_loss_tf.eval(session=sess), train_loss], axis = 0)\n        vl_c = np.concatenate([self.valid_loss_tf.eval(session=sess), valid_loss], axis = 0)\n        ta_c = np.concatenate([self.train_acc_tf.eval(session=sess), train_acc], axis = 0)\n        va_c = np.concatenate([self.valid_acc_tf.eval(session=sess), valid_acc], axis = 0)\n   \n        sess.run(tf.assign(self.train_loss_tf, tl_c, validate_shape = False))\n        sess.run(tf.assign(self.valid_loss_tf, vl_c , validate_shape = False))\n        sess.run(tf.assign(self.train_acc_tf, ta_c , validate_shape = False))\n        sess.run(tf.assign(self.valid_acc_tf, va_c , validate_shape = False))\n        \n        print('running time for training: ', datetime.datetime.now() - start)\n        return None\n  \n    # save tensors/summaries\n    def save_model(self, sess):\n        \n        # tf saver\n        if self.use_tf_saver:\n            #filepath = os.path.join(os.getcwd(), 'logs' , self.nn_name)\n            filepath = os.path.join(os.getcwd(), self.nn_name)\n            self.saver_tf.save(sess, filepath)\n        \n        # tb summary\n        if self.use_tb_summary:\n            self.train_writer.close()\n            self.valid_writer.close()\n        \n        return None\n  \n    # forward prediction of current graph\n    def forward(self, sess, x_data):\n        y_pred_proba = self.y_pred_proba_tf.eval(session = sess, \n                                                 feed_dict = {self.x_data_tf: x_data,\n                                                              self.keep_prob_tf: 1.0})\n        return y_pred_proba\n    \n    # function to load tensors from a saved graph\n    def load_tensors(self, graph):\n        \n        # input tensors\n        self.x_data_tf = graph.get_tensor_by_name(\"x_data_tf:0\")\n        self.y_data_tf = graph.get_tensor_by_name(\"y_data_tf:0\")\n        \n        # weights and bias tensors\n        self.W_conv1_tf = graph.get_tensor_by_name(\"W_conv1_tf:0\")\n        self.W_conv2_tf = graph.get_tensor_by_name(\"W_conv2_tf:0\")\n        self.W_conv3_tf = graph.get_tensor_by_name(\"W_conv3_tf:0\")\n        self.W_fc1_tf = graph.get_tensor_by_name(\"W_fc1_tf:0\")\n        self.W_fc2_tf = graph.get_tensor_by_name(\"W_fc2_tf:0\")\n        self.b_conv1_tf = graph.get_tensor_by_name(\"b_conv1_tf:0\")\n        self.b_conv2_tf = graph.get_tensor_by_name(\"b_conv2_tf:0\")\n        self.b_conv3_tf = graph.get_tensor_by_name(\"b_conv3_tf:0\")\n        self.b_fc1_tf = graph.get_tensor_by_name(\"b_fc1_tf:0\")\n        self.b_fc2_tf = graph.get_tensor_by_name(\"b_fc2_tf:0\")\n        \n        # activation tensors\n        self.h_conv1_tf = graph.get_tensor_by_name('h_conv1_tf:0')  \n        self.h_pool1_tf = graph.get_tensor_by_name('h_pool1_tf:0')\n        self.h_conv2_tf = graph.get_tensor_by_name('h_conv2_tf:0')\n        self.h_pool2_tf = graph.get_tensor_by_name('h_pool2_tf:0')\n        self.h_conv3_tf = graph.get_tensor_by_name('h_conv3_tf:0')\n        self.h_pool3_tf = graph.get_tensor_by_name('h_pool3_tf:0')\n        self.h_fc1_tf = graph.get_tensor_by_name('h_fc1_tf:0')\n        self.z_pred_tf = graph.get_tensor_by_name('z_pred_tf:0')\n        \n        # training and prediction tensors\n        self.learn_rate_tf = graph.get_tensor_by_name(\"learn_rate_tf:0\")\n        self.keep_prob_tf = graph.get_tensor_by_name(\"keep_prob_tf:0\")\n        self.cross_entropy_tf = graph.get_tensor_by_name('cross_entropy_tf:0')\n        self.train_step_tf = graph.get_operation_by_name('train_step_tf')\n        self.z_pred_tf = graph.get_tensor_by_name('z_pred_tf:0')\n        self.y_pred_proba_tf = graph.get_tensor_by_name(\"y_pred_proba_tf:0\")\n        self.y_pred_correct_tf = graph.get_tensor_by_name('y_pred_correct_tf:0')\n        self.accuracy_tf = graph.get_tensor_by_name('accuracy_tf:0')\n        \n        # tensor of stored losses and accuricies during training\n        self.train_loss_tf = graph.get_tensor_by_name(\"train_loss_tf:0\")\n        self.train_acc_tf = graph.get_tensor_by_name(\"train_acc_tf:0\")\n        self.valid_loss_tf = graph.get_tensor_by_name(\"valid_loss_tf:0\")\n        self.valid_acc_tf = graph.get_tensor_by_name(\"valid_acc_tf:0\")\n  \n        return None\n    \n    # get losses of training and validation sets\n    def get_loss(self, sess):\n        train_loss = self.train_loss_tf.eval(session = sess)\n        valid_loss = self.valid_loss_tf.eval(session = sess)\n        return train_loss, valid_loss \n        \n    # get accuracies of training and validation sets\n    def get_accuracy(self, sess):\n        train_acc = self.train_acc_tf.eval(session = sess)\n        valid_acc = self.valid_acc_tf.eval(session = sess)\n        return train_acc, valid_acc \n    \n    # get weights\n    def get_weights(self, sess):\n        W_conv1 = self.W_conv1_tf.eval(session = sess)\n        W_conv2 = self.W_conv2_tf.eval(session = sess)\n        W_conv3 = self.W_conv3_tf.eval(session = sess)\n        W_fc1_tf = self.W_fc1_tf.eval(session = sess)\n        W_fc2_tf = self.W_fc2_tf.eval(session = sess)\n        return W_conv1, W_conv2, W_conv3, W_fc1_tf, W_fc2_tf\n    \n    # get biases\n    def get_biases(self, sess):\n        b_conv1 = self.b_conv1_tf.eval(session = sess)\n        b_conv2 = self.b_conv2_tf.eval(session = sess)\n        b_conv3 = self.b_conv3_tf.eval(session = sess)\n        b_fc1_tf = self.b_fc1_tf.eval(session = sess)\n        b_fc2_tf = self.b_fc2_tf.eval(session = sess)\n        return b_conv1, b_conv2, b_conv3, b_fc1_tf, b_fc2_tf\n    \n    # load session from file, restore graph, and load tensors\n    def load_session_from_file(self, filename):\n        tf.reset_default_graph()\n        filepath = os.path.join(os.getcwd(), filename + '.meta')\n        #filepath = os.path.join(os.getcwd(),'logs', filename + '.meta')\n        saver = tf.train.import_meta_graph(filepath)\n        print(filepath)\n        sess = tf.Session()\n        saver.restore(sess, mn)\n        graph = tf.get_default_graph()\n        self.load_tensors(graph)\n        return sess\n    \n    # receive activations given the input\n    def get_activations(self, sess, x_data):\n        feed_dict = {self.x_data_tf: x_data, self.keep_prob_tf: 1.0}\n        h_conv1 = self.h_conv1_tf.eval(session = sess, feed_dict = feed_dict)\n        h_pool1 = self.h_pool1_tf.eval(session = sess, feed_dict = feed_dict)\n        h_conv2 = self.h_conv2_tf.eval(session = sess, feed_dict = feed_dict)\n        h_pool2 = self.h_pool2_tf.eval(session = sess, feed_dict = feed_dict)\n        h_conv3 = self.h_conv3_tf.eval(session = sess, feed_dict = feed_dict)\n        h_pool3 = self.h_pool3_tf.eval(session = sess, feed_dict = feed_dict)\n        h_fc1 = self.h_fc1_tf.eval(session = sess, feed_dict = feed_dict)\n        h_fc2 = self.z_pred_tf.eval(session = sess, feed_dict = feed_dict)\n        return h_conv1,h_pool1,h_conv2,h_pool2,h_conv3,h_pool3,h_fc1,h_fc2\n    ","metadata":{"_cell_guid":"d423c525-ff48-4889-abc2-36c8e8892fc7","_uuid":"6e456002a14cd417180d83b1f8a73bda10582575","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Train and validate the neural network <a class=\"anchor\" id=\"6-bullet\"></a> ","metadata":{"_cell_guid":"8beaf2d9-5664-41b0-867d-e7ca2b009fd8","_uuid":"5913b77f9eca4145cfb5428a8f8bed9ff5c46db6"}},{"cell_type":"code","source":"## train the neural network graph\n\nnn_name = ['nn0','nn1','nn2','nn3','nn4','nn5','nn6','nn7','nn8','nn9']\n\n# cross validations\ncv_num = 10 # cross validations default = 20 => 5% validation set\nkfold = sklearn.model_selection.KFold(cv_num, shuffle=True, random_state=123)\n\nfor i,(train_index, valid_index) in enumerate(kfold.split(x_train_valid)):\n    \n    # start timer\n    start = datetime.datetime.now();\n    \n    # train and validation data of original images\n    x_train = x_train_valid[train_index]\n    y_train = y_train_valid[train_index]\n    x_valid = x_train_valid[valid_index]\n    y_valid = y_train_valid[valid_index]\n    \n    # create neural network graph\n    nn_graph = nn_class(nn_name = nn_name[i]) # instance of nn_class\n    nn_graph.create_graph() # create graph\n    nn_graph.attach_saver() # attach saver tensors\n    \n    # start tensorflow session\n    with tf.Session() as sess:\n        \n        # attach summaries\n        nn_graph.attach_summary(sess) \n        \n        # variable initialization of the default graph\n        sess.run(tf.global_variables_initializer()) \n    \n        # training on original data\n        nn_graph.train_graph(sess, x_train, y_train, x_valid, y_valid, n_epoch = 1.)\n        \n        # training on augmented data\n        nn_graph.train_graph(sess, x_train, y_train, x_valid, y_valid, n_epoch = 29.,\n                             train_on_augmented_data = True)\n\n        # save tensors and summaries of model\n        nn_graph.save_model(sess)\n    \n    break\n    \nprint('total running time for training: ', datetime.datetime.now() - start)\n    ","metadata":{"_cell_guid":"c1857878-8a1e-4f3e-aa3d-d37f9b4b5850","_uuid":"1a991a47e9007f43d4c07e79fb28fc4ad2d26819","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## visualization with tensorboard\nif False:\n    !tensorboard --logdir=./logs","metadata":{"_cell_guid":"da10eea8-3379-454f-98a9-fbf13c7ff770","_uuid":"33d667d05320d637f66e1f8a136c8dd6b0cf0f2a","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## loss and accuracy curves\n\n# choose neural network\nmn = nn_name[0]\nnn_graph = nn_class()\nsess = nn_graph.load_session_from_file(mn)\ntrain_loss[mn], valid_loss[mn] = nn_graph.get_loss(sess)\ntrain_acc[mn], valid_acc[mn] = nn_graph.get_accuracy(sess)\nsess.close()\n\nprint('final train/valid loss = %.4f/%.4f, train/valid accuracy = %.4f/%.4f'%(\n    train_loss[mn][-1], valid_loss[mn][-1], train_acc[mn][-1], valid_acc[mn][-1]))\n\nplt.figure(figsize=(10, 5));\nplt.subplot(1,2,1);\nplt.plot(np.arange(0,len(train_acc[mn])), train_acc[mn],'-b', label='Training')\nplt.plot(np.arange(0,len(valid_acc[mn])), valid_acc[mn],'-g', label='Validation')\nplt.legend(loc='lower right', frameon=False)\nplt.ylim(ymax = 1.1, ymin = 0.0)\nplt.ylabel('accuracy')\nplt.xlabel('log steps');\n\nplt.subplot(1,2,2)\nplt.plot(np.arange(0,len(train_loss[mn])), train_loss[mn],'-b', label='Training')\nplt.plot(np.arange(0,len(valid_loss[mn])), valid_loss[mn],'-g', label='Validation')\nplt.legend(loc='lower right', frameon=False)\nplt.ylim(ymax = 3.0, ymin = 0.0)\nplt.ylabel('loss')\nplt.xlabel('log steps');","metadata":{"_cell_guid":"8d03fd56-8e7a-4597-afb9-91a486e0e210","_uuid":"56f129a23591b77c4be8455164fc9a1770a61e6c","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## show confusion matrix\n\n# choose neural network\nmn = nn_name[0]\nnn_graph = nn_class()\nsess = nn_graph.load_session_from_file(mn)\ny_valid_pred[mn] = nn_graph.forward(sess, x_valid)\nsess.close()\n\n# confusion matrix\ncnf_valid_matrix['neural_network'] = sklearn.metrics.confusion_matrix(\n    y_pred = one_hot_to_dense(y_valid_pred[mn]), \n    y_true = one_hot_to_dense(y_valid)).astype(np.float32)\n\n# normalize\ncnf_valid_matrix['neural_network'][0,:] = cnf_valid_matrix['neural_network'][0,:]/cnf_valid_matrix['neural_network'][0,:].sum()  \ncnf_valid_matrix['neural_network'][1,:] = cnf_valid_matrix['neural_network'][1,:]/cnf_valid_matrix['neural_network'][1,:].sum()  \n\n# plot\nlabels_array = ['IDC = 0', 'IDC = 1']\nfig, ax = plt.subplots(1,figsize=(5,5))\nax = sns.heatmap(cnf_valid_matrix['neural_network'], ax=ax, cmap=plt.cm.Greens, annot=True)\nax.set_xticklabels(labels_array)\nax.set_yticklabels(labels_array)\nplt.title('Confusion matrix of validation set')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show();","metadata":{"_cell_guid":"34799605-b9ca-4464-94d1-1a1ce28c069a","_uuid":"fef784a264eda3ebf25295a6445c4c59d9aaa682","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]}]}