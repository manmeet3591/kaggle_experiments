{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# https://github.com/dragen1860/TensorFlow-2.x-Tutorials/tree/master/02-AutoGraph","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-10T19:18:49.818636Z","iopub.execute_input":"2021-09-10T19:18:49.819313Z","iopub.status.idle":"2021-09-10T19:18:49.835087Z","shell.execute_reply.started":"2021-09-10T19:18:49.819161Z","shell.execute_reply":"2021-09-10T19:18:49.834160Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## AutoGraph\nCompare static graph using @tf.function VS dynamic graph.\n\nAutoGraph helps you write complicated graph code using normal Python. Behind the scenes, AutoGraph automatically transforms your code into the equivalent TensorFlow graph code.\n\nLet's take a look at TensorFlow graphs and how they work.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nReLU_Layer = tf.keras.layers.Dense(100, input_shape=(784,), activation=tf.nn.relu)\nLogit_Layer = tf.keras.layers.Dense(10, input_shape=(100,))\n\n# X and y are labels and inputs","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:28:21.169738Z","iopub.execute_input":"2021-09-10T19:28:21.170040Z","iopub.status.idle":"2021-09-10T19:28:21.178909Z","shell.execute_reply.started":"2021-09-10T19:28:21.170012Z","shell.execute_reply":"2021-09-10T19:28:21.177604Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**TensorFlow 2.0:** Operations are executed directly and the computational graph is built on-the-fly. However, we can still write functions and pre-compile computational graphs from them like in TF 1.0 using the @tf.function decorator, allowing for faster execution.","metadata":{}},{"cell_type":"code","source":"# SGD_Trainer = tf.optimizers.SGD(1e-2)\n\n# @tf.function\n# def loss_fn(inputs=X, labels=y):\n#     hidden = ReLU_Layer(inputs)\n#     logits = Logit_Layer(hidden)\n#     entropy = tf.nn.softmax_cross_entropy_with_logits(\n#         logits=logits, labels=labels)\n#     return tf.reduce_mean(entropy)\n\n# for step in range(1000):\n#     SGD_Trainer.minimize(loss_fn, \n#         var_list=ReLU_Layer.weights+Logit_Layer.weights)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:28:31.932207Z","iopub.execute_input":"2021-09-10T19:28:31.933684Z","iopub.status.idle":"2021-09-10T19:28:31.942521Z","shell.execute_reply.started":"2021-09-10T19:28:31.933623Z","shell.execute_reply":"2021-09-10T19:28:31.941269Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}