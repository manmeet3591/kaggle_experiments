{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import  tensorflow as tf\nfrom    tensorflow import  keras\nfrom    tensorflow.keras import datasets, layers, optimizers, models\nfrom    tensorflow.keras import regularizers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-17T06:02:22.060780Z","iopub.execute_input":"2021-09-17T06:02:22.061187Z","iopub.status.idle":"2021-09-17T06:02:27.849423Z","shell.execute_reply.started":"2021-09-17T06:02:22.061098Z","shell.execute_reply":"2021-09-17T06:02:27.848389Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2021-09-17 06:02:22.792024: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","output_type":"stream"}]},{"cell_type":"code","source":"class VGG16(models.Model):\n\n\n    def __init__(self, input_shape):\n        \"\"\"\n        :param input_shape: [32, 32, 3]\n        \"\"\"\n        super(VGG16, self).__init__()\n\n        weight_decay = 0.000\n        self.num_classes = 10\n\n        model = models.Sequential()\n\n        model.add(layers.Conv2D(64, (3, 3), padding='same',\n                         input_shape=input_shape, kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation('relu'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.3))\n\n        model.add(layers.Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation('relu'))\n        model.add(layers.BatchNormalization())\n\n        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n        model.add(layers.Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation('relu'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.4))\n\n        model.add(layers.Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation('relu'))\n        model.add(layers.BatchNormalization())\n\n        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n        model.add(layers.Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation('relu'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.4))\n\n        model.add(layers.Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation('relu'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.4))\n\n        model.add(layers.Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation('relu'))\n        model.add(layers.BatchNormalization())\n\n        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n\n        model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation('relu'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.4))\n\n        model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation('relu'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.4))\n\n        model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation('relu'))\n        model.add(layers.BatchNormalization())\n\n        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n\n        model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation('relu'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.4))\n\n        model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation('relu'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.4))\n\n        model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation('relu'))\n        model.add(layers.BatchNormalization())\n\n        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n        model.add(layers.Dropout(0.5))\n\n        model.add(layers.Flatten())\n        model.add(layers.Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation('relu'))\n        model.add(layers.BatchNormalization())\n\n        model.add(layers.Dropout(0.5))\n        model.add(layers.Dense(self.num_classes))\n        # model.add(layers.Activation('softmax'))\n\n\n        self.model = model\n\n\n    def call(self, x):\n\n        x = self.model(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-09-17T06:04:47.036262Z","iopub.execute_input":"2021-09-17T06:04:47.036560Z","iopub.status.idle":"2021-09-17T06:04:47.067534Z","shell.execute_reply.started":"2021-09-17T06:04:47.036515Z","shell.execute_reply":"2021-09-17T06:04:47.066645Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import  os\nimport  tensorflow as tf\nfrom    tensorflow import  keras\nfrom    tensorflow.keras import datasets, layers, optimizers\nimport  argparse\nimport  numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-09-17T06:05:04.814116Z","iopub.execute_input":"2021-09-17T06:05:04.814433Z","iopub.status.idle":"2021-09-17T06:05:04.821324Z","shell.execute_reply.started":"2021-09-17T06:05:04.814405Z","shell.execute_reply":"2021-09-17T06:05:04.818911Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# from    network import VGG16\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n# argparser = argparse.ArgumentParser()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T06:05:36.942898Z","iopub.execute_input":"2021-09-17T06:05:36.943263Z","iopub.status.idle":"2021-09-17T06:05:36.948034Z","shell.execute_reply.started":"2021-09-17T06:05:36.943233Z","shell.execute_reply":"2021-09-17T06:05:36.946990Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def normalize(X_train, X_test):\n    # this function normalize inputs for zero mean and unit variance\n    # it is used when training a model.\n    # Input: training set and test set\n    # Output: normalized training set and test set according to the trianing set statistics.\n    X_train = X_train / 255.\n    X_test = X_test / 255.\n\n    mean = np.mean(X_train, axis=(0, 1, 2, 3))\n    std = np.std(X_train, axis=(0, 1, 2, 3))\n    print('mean:', mean, 'std:', std)\n    X_train = (X_train - mean) / (std + 1e-7)\n    X_test = (X_test - mean) / (std + 1e-7)\n    return X_train, X_test","metadata":{"execution":{"iopub.status.busy":"2021-09-17T06:05:51.157543Z","iopub.execute_input":"2021-09-17T06:05:51.157914Z","iopub.status.idle":"2021-09-17T06:05:51.165558Z","shell.execute_reply.started":"2021-09-17T06:05:51.157884Z","shell.execute_reply":"2021-09-17T06:05:51.164266Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def prepare_cifar(x, y):\n\n    x = tf.cast(x, tf.float32)\n    y = tf.cast(y, tf.int32)\n    return x, y","metadata":{"execution":{"iopub.status.busy":"2021-09-17T06:11:38.381752Z","iopub.execute_input":"2021-09-17T06:11:38.382512Z","iopub.status.idle":"2021-09-17T06:11:38.389107Z","shell.execute_reply.started":"2021-09-17T06:11:38.382480Z","shell.execute_reply":"2021-09-17T06:11:38.387078Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def compute_loss(logits, labels):\n    return tf.reduce_mean(\n      tf.nn.sparse_softmax_cross_entropy_with_logits(\n          logits=logits, labels=labels))","metadata":{"execution":{"iopub.status.busy":"2021-09-17T06:29:54.065049Z","iopub.execute_input":"2021-09-17T06:29:54.065355Z","iopub.status.idle":"2021-09-17T06:29:54.073408Z","shell.execute_reply.started":"2021-09-17T06:29:54.065325Z","shell.execute_reply":"2021-09-17T06:29:54.069862Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def main():\n\n    tf.random.set_seed(22)\n\n    print('loading data...')\n    (x,y), (x_test, y_test) = datasets.cifar10.load_data()\n    x, x_test = normalize(x, x_test)\n    print(x.shape, y.shape, x_test.shape, y_test.shape)\n    # x = tf.convert_to_tensor(x)\n    # y = tf.convert_to_tensor(y)\n    train_loader = tf.data.Dataset.from_tensor_slices((x,y))\n    train_loader = train_loader.map(prepare_cifar).shuffle(50000).batch(256)\n\n    test_loader = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n    test_loader = test_loader.map(prepare_cifar).shuffle(10000).batch(256)\n    print('done.')\n\n\n\n\n    model = VGG16([32, 32, 3])\n\n\n    # must specify from_logits=True!\n    criteon = keras.losses.CategoricalCrossentropy(from_logits=True)\n    metric = keras.metrics.CategoricalAccuracy()\n\n    optimizer = optimizers.Adam(learning_rate=0.0001)\n\n\n    for epoch in range(250):\n\n        for step, (x, y) in enumerate(train_loader):\n            # [b, 1] => [b]\n            y = tf.squeeze(y, axis=1)\n            # [b, 10]\n            y = tf.one_hot(y, depth=10)\n\n            with tf.GradientTape() as tape:\n                logits = model(x)\n                loss = criteon(y, logits)\n                # loss2 = compute_loss(logits, tf.argmax(y, axis=1))\n                # mse_loss = tf.reduce_sum(tf.square(y-logits))\n                # print(y.shape, logits.shape)\n                metric.update_state(y, logits)\n\n            grads = tape.gradient(loss, model.trainable_variables)\n            # MUST clip gradient here or it will disconverge!\n            grads = [ tf.clip_by_norm(g, 15) for g in grads]\n            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n            if step % 40 == 0:\n                # for g in grads:\n                #     print(tf.norm(g).numpy())\n                print(epoch, step, 'loss:', float(loss), 'acc:', metric.result().numpy())\n                metric.reset_states()\n\n\n        if epoch % 1 == 0:\n\n            metric = keras.metrics.CategoricalAccuracy()\n            for x, y in test_loader:\n                # [b, 1] => [b]\n                y = tf.squeeze(y, axis=1)\n                # [b, 10]\n                y = tf.one_hot(y, depth=10)\n\n                logits = model.predict(x)\n                # be careful, these functions can accept y as [b] without warnning.\n                metric.update_state(y, logits)\n            print('test acc:', metric.result().numpy())\n            metric.reset_states()\n\n\nif __name__ == '__main__':\n    main()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T06:35:29.940977Z","iopub.execute_input":"2021-09-17T06:35:29.941280Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"loading data...\nDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170500096/170498071 [==============================] - 6s 0us/step\nmean: 0.4733630004850874 std: 0.25156892506322026\n(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n","output_type":"stream"},{"name":"stderr","text":"2021-09-17 06:35:41.341860: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2021-09-17 06:35:41.345519: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n2021-09-17 06:35:41.400000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-17 06:35:41.401090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \npciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\ncoreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n2021-09-17 06:35:41.401179: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n2021-09-17 06:35:41.452147: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n2021-09-17 06:35:41.452294: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n2021-09-17 06:35:41.473175: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n2021-09-17 06:35:41.536185: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n2021-09-17 06:35:41.584489: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n2021-09-17 06:35:41.593640: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n2021-09-17 06:35:41.596338: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n2021-09-17 06:35:41.596559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-17 06:35:41.597614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-17 06:35:41.599826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n2021-09-17 06:35:41.602747: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2021-09-17 06:35:41.604061: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2021-09-17 06:35:41.604322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-17 06:35:41.605230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \npciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\ncoreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n2021-09-17 06:35:41.605296: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n2021-09-17 06:35:41.605356: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n2021-09-17 06:35:41.605407: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n2021-09-17 06:35:41.605439: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n2021-09-17 06:35:41.605468: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n2021-09-17 06:35:41.605496: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n2021-09-17 06:35:41.605527: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n2021-09-17 06:35:41.605556: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n2021-09-17 06:35:41.605673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-17 06:35:41.606688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-17 06:35:41.607609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n2021-09-17 06:35:41.609108: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n2021-09-17 06:35:43.426128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n2021-09-17 06:35:43.426197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n2021-09-17 06:35:43.426213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n2021-09-17 06:35:43.428834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-17 06:35:43.429914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-17 06:35:43.430972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-17 06:35:43.431900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14957 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n2021-09-17 06:35:44.089625: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1228800000 exceeds 10% of free system memory.\n","output_type":"stream"},{"name":"stdout","text":"done.\n","output_type":"stream"},{"name":"stderr","text":"2021-09-17 06:35:46.460320: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1228800000 exceeds 10% of free system memory.\n2021-09-17 06:35:47.278257: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n2021-09-17 06:35:47.283874: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2199995000 Hz\n2021-09-17 06:35:51.909002: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n2021-09-17 06:35:57.558108: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n2021-09-17 06:35:58.439174: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n","output_type":"stream"},{"name":"stdout","text":"0 0 loss: 2.3026366233825684 acc: 0.1015625\n0 40 loss: 2.1111652851104736 acc: 0.17568359\n0 80 loss: 1.901890516281128 acc: 0.25761718\n0 120 loss: 1.6934963464736938 acc: 0.309375\n0 160 loss: 1.5793256759643555 acc: 0.35957032\ntest acc: 0.3955\n","output_type":"stream"},{"name":"stderr","text":"2021-09-17 06:36:25.144794: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1228800000 exceeds 10% of free system memory.\n","output_type":"stream"},{"name":"stdout","text":"1 0 loss: 1.6360318660736084 acc: 0.39453125\n1 40 loss: 1.6101021766662598 acc: 0.40517578\n1 80 loss: 1.4893076419830322 acc: 0.42578125\n1 120 loss: 1.404839277267456 acc: 0.44570312\n1 160 loss: 1.5036845207214355 acc: 0.46074218\ntest acc: 0.5\n","output_type":"stream"},{"name":"stderr","text":"2021-09-17 06:36:54.546990: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1228800000 exceeds 10% of free system memory.\n","output_type":"stream"},{"name":"stdout","text":"2 0 loss: 1.291830062866211 acc: 0.5078125\n2 40 loss: 1.4052435159683228 acc: 0.48916015\n2 80 loss: 1.2300424575805664 acc: 0.5154297\n2 120 loss: 1.23406982421875 acc: 0.5293945\n2 160 loss: 1.1632024049758911 acc: 0.5492188\ntest acc: 0.5189\n","output_type":"stream"},{"name":"stderr","text":"2021-09-17 06:37:22.993340: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1228800000 exceeds 10% of free system memory.\n","output_type":"stream"},{"name":"stdout","text":"3 0 loss: 1.2291834354400635 acc: 0.56640625\n3 40 loss: 1.125185251235962 acc: 0.57304686\n3 80 loss: 1.0842115879058838 acc: 0.58388674\n3 120 loss: 1.098068118095398 acc: 0.5907227\n3 160 loss: 1.0191938877105713 acc: 0.6017578\ntest acc: 0.6032\n4 0 loss: 0.9743990898132324 acc: 0.67578125\n4 40 loss: 1.0130958557128906 acc: 0.62294924\n4 80 loss: 0.9724045991897583 acc: 0.6345703\n4 120 loss: 0.9800336956977844 acc: 0.6310547\n4 160 loss: 0.926780104637146 acc: 0.634375\ntest acc: 0.6166\n5 0 loss: 1.037205457687378 acc: 0.578125\n5 40 loss: 0.9445794820785522 acc: 0.66015625\n5 80 loss: 0.8424376845359802 acc: 0.66367185\n5 120 loss: 0.946578860282898 acc: 0.68671876\n5 160 loss: 0.995648205280304 acc: 0.6738281\ntest acc: 0.6531\n6 0 loss: 0.892898678779602 acc: 0.6875\n6 40 loss: 0.7250820398330688 acc: 0.69804686\n6 80 loss: 0.8000730276107788 acc: 0.71904296\n6 120 loss: 0.7295761704444885 acc: 0.7071289\n6 160 loss: 0.7882013916969299 acc: 0.72382814\ntest acc: 0.658\n7 0 loss: 0.7645267844200134 acc: 0.734375\n7 40 loss: 0.7068495750427246 acc: 0.7439453\n7 80 loss: 0.7615444660186768 acc: 0.7493164\n7 120 loss: 0.6526275873184204 acc: 0.74316406\n7 160 loss: 0.7223901748657227 acc: 0.7553711\ntest acc: 0.7095\n8 0 loss: 0.6979736089706421 acc: 0.73828125\n8 40 loss: 0.6731366515159607 acc: 0.78447264\n8 80 loss: 0.6290377378463745 acc: 0.7892578\n8 120 loss: 0.6758801937103271 acc: 0.79375\n8 160 loss: 0.5240039229393005 acc: 0.79003906\ntest acc: 0.7182\n9 0 loss: 0.46883904933929443 acc: 0.8359375\n9 40 loss: 0.4637921154499054 acc: 0.83076173\n9 80 loss: 0.5379783511161804 acc: 0.83310544\n9 120 loss: 0.6761248111724854 acc: 0.8299805\n9 160 loss: 0.40696513652801514 acc: 0.8322266\ntest acc: 0.7027\n10 0 loss: 0.4810155928134918 acc: 0.8125\n10 40 loss: 0.39254048466682434 acc: 0.8658203\n10 80 loss: 0.3570956289768219 acc: 0.87333983\n10 120 loss: 0.4186350703239441 acc: 0.8587891\n10 160 loss: 0.40491414070129395 acc: 0.8689453\ntest acc: 0.7188\n11 0 loss: 0.3007681667804718 acc: 0.8984375\n11 40 loss: 0.25957298278808594 acc: 0.9042969\n11 80 loss: 0.27476590871810913 acc: 0.9049805\n11 120 loss: 0.2732197642326355 acc: 0.90625\n11 160 loss: 0.3159053325653076 acc: 0.88544923\ntest acc: 0.7325\n12 0 loss: 0.1438845545053482 acc: 0.95703125\n12 40 loss: 0.2125348597764969 acc: 0.93115234\n12 80 loss: 0.1970611959695816 acc: 0.92763674\n12 120 loss: 0.21296082437038422 acc: 0.9229492\n12 160 loss: 0.23328712582588196 acc: 0.9212891\ntest acc: 0.745\n13 0 loss: 0.19510862231254578 acc: 0.9375\n13 40 loss: 0.19864732027053833 acc: 0.95\n13 80 loss: 0.20615215599536896 acc: 0.94677734\n13 120 loss: 0.1469111442565918 acc: 0.94628906\n13 160 loss: 0.30953168869018555 acc: 0.93662107\ntest acc: 0.7196\n14 0 loss: 0.13038845360279083 acc: 0.94921875\n14 40 loss: 0.11857560276985168 acc: 0.95839846\n14 80 loss: 0.10904084146022797 acc: 0.962207\n14 120 loss: 0.16645914316177368 acc: 0.95458984\n14 160 loss: 0.13090837001800537 acc: 0.9515625\ntest acc: 0.7239\n15 0 loss: 0.0986453965306282 acc: 0.97265625\n15 40 loss: 0.054580602794885635 acc: 0.9694336\n15 80 loss: 0.13724073767662048 acc: 0.9673828\n15 120 loss: 0.06683742254972458 acc: 0.9675781\n15 160 loss: 0.10931399464607239 acc: 0.96367186\ntest acc: 0.7284\n16 0 loss: 0.09095988422632217 acc: 0.98046875\n16 40 loss: 0.05853820592164993 acc: 0.9681641\n16 80 loss: 0.06805336475372314 acc: 0.974707\n16 120 loss: 0.06268149614334106 acc: 0.97197264\n16 160 loss: 0.12901999056339264 acc: 0.9665039\ntest acc: 0.7185\n17 0 loss: 0.09401203691959381 acc: 0.96875\n17 40 loss: 0.054113518446683884 acc: 0.9794922\n17 80 loss: 0.08443029224872589 acc: 0.984375\n17 120 loss: 0.03003821335732937 acc: 0.977832\n17 160 loss: 0.06588426232337952 acc: 0.98115236\ntest acc: 0.7349\n18 0 loss: 0.018868297338485718 acc: 0.99609375\n18 40 loss: 0.052851125597953796 acc: 0.98232424\n18 80 loss: 0.039351936429739 acc: 0.98115236\n18 120 loss: 0.03374285623431206 acc: 0.9770508\n18 160 loss: 0.07491344213485718 acc: 0.9772461\ntest acc: 0.717\n19 0 loss: 0.13891102373600006 acc: 0.96875\n19 40 loss: 0.0780392587184906 acc: 0.9794922\n19 80 loss: 0.01736537553369999 acc: 0.98447263\n19 120 loss: 0.11770826578140259 acc: 0.9783203\n19 160 loss: 0.04209308326244354 acc: 0.9828125\ntest acc: 0.7356\n20 0 loss: 0.0706963986158371 acc: 0.9765625\n20 40 loss: 0.05874297022819519 acc: 0.98476565\n20 80 loss: 0.029786162078380585 acc: 0.98339844\n20 120 loss: 0.04588184878230095 acc: 0.978418\n20 160 loss: 0.06594923138618469 acc: 0.97734374\ntest acc: 0.7481\n21 0 loss: 0.018585532903671265 acc: 0.9921875\n21 40 loss: 0.060492824763059616 acc: 0.97929686\n21 80 loss: 0.11155892163515091 acc: 0.96875\n21 120 loss: 0.02948717400431633 acc: 0.98408204\n21 160 loss: 0.0421675369143486 acc: 0.98720706\ntest acc: 0.7479\n22 0 loss: 0.01856398582458496 acc: 0.99609375\n22 40 loss: 0.008691851980984211 acc: 0.9861328\n22 80 loss: 0.03654472529888153 acc: 0.9916992\n22 120 loss: 0.05359923094511032 acc: 0.9880859\n22 160 loss: 0.035181231796741486 acc: 0.9849609\ntest acc: 0.7496\n23 0 loss: 0.05795988440513611 acc: 0.98828125\n23 40 loss: 0.02888094261288643 acc: 0.98691404\n23 80 loss: 0.07092493027448654 acc: 0.98583984\n23 120 loss: 0.04462180286645889 acc: 0.9833008\n23 160 loss: 0.021255934610962868 acc: 0.9857422\ntest acc: 0.7392\n24 0 loss: 0.029652295634150505 acc: 0.98828125\n24 40 loss: 0.014216095209121704 acc: 0.98466796\n24 80 loss: 0.02070065587759018 acc: 0.98710936\n24 120 loss: 0.03410573676228523 acc: 0.9899414\n24 160 loss: 0.045873742550611496 acc: 0.9832031\ntest acc: 0.7445\n25 0 loss: 0.04316512495279312 acc: 0.98046875\n25 40 loss: 0.042743705213069916 acc: 0.98779297\n25 80 loss: 0.01682380959391594 acc: 0.9897461\n25 120 loss: 0.029056821018457413 acc: 0.990625\n25 160 loss: 0.10906697064638138 acc: 0.9868164\ntest acc: 0.7457\n26 0 loss: 0.023821890354156494 acc: 0.98828125\n26 40 loss: 0.029164232313632965 acc: 0.9866211\n26 80 loss: 0.07992985844612122 acc: 0.98759764\n26 120 loss: 0.05460200086236 acc: 0.9824219\n26 160 loss: 0.01304364763200283 acc: 0.9854492\ntest acc: 0.7478\n27 0 loss: 0.052568886429071426 acc: 0.9765625\n27 40 loss: 0.05597792565822601 acc: 0.9910156\n27 80 loss: 0.0452156625688076 acc: 0.98955077\n27 120 loss: 0.030418647453188896 acc: 0.9873047\n27 160 loss: 0.04158206284046173 acc: 0.9800781\ntest acc: 0.7399\n28 0 loss: 0.05545854568481445 acc: 0.97265625\n28 40 loss: 0.011869793757796288 acc: 0.9904297\n28 80 loss: 0.013758011162281036 acc: 0.9911133\n28 120 loss: 0.016449248418211937 acc: 0.9898437\n28 160 loss: 0.016701694577932358 acc: 0.98896486\ntest acc: 0.7446\n29 0 loss: 0.02303447760641575 acc: 0.98828125\n29 40 loss: 0.0368584506213665 acc: 0.99013674\n29 80 loss: 0.039340391755104065 acc: 0.9838867\n29 120 loss: 0.018019631505012512 acc: 0.98720706\n29 160 loss: 0.017342964187264442 acc: 0.98525393\ntest acc: 0.7418\n30 0 loss: 0.01537301391363144 acc: 0.99609375\n30 40 loss: 0.12350670248270035 acc: 0.99179685\n30 80 loss: 0.0402299240231514 acc: 0.98769534\n30 120 loss: 0.01496056467294693 acc: 0.98964846\n30 160 loss: 0.014311711303889751 acc: 0.9899414\ntest acc: 0.7438\n31 0 loss: 0.006992125418037176 acc: 1.0\n31 40 loss: 0.038262296468019485 acc: 0.99189454\n31 80 loss: 0.019927121698856354 acc: 0.99052733\n31 120 loss: 0.014972405508160591 acc: 0.98935544\n31 160 loss: 0.030887490138411522 acc: 0.9886719\ntest acc: 0.7421\n32 0 loss: 0.020748550072312355 acc: 0.9921875\n32 40 loss: 0.026475289836525917 acc: 0.9802734\n32 80 loss: 0.0378049835562706 acc: 0.9838867\n32 120 loss: 0.04922155290842056 acc: 0.98769534\n32 160 loss: 0.010296258144080639 acc: 0.99130857\ntest acc: 0.7488\n33 0 loss: 0.013417038135230541 acc: 0.99609375\n33 40 loss: 0.025574713945388794 acc: 0.99189454\n33 80 loss: 0.03841521963477135 acc: 0.990918\n33 120 loss: 0.018400888890028 acc: 0.9859375\n33 160 loss: 0.045603200793266296 acc: 0.99003905\ntest acc: 0.7525\n34 0 loss: 0.04461539909243584 acc: 0.98046875\n34 40 loss: 0.007927443832159042 acc: 0.99257815\n34 80 loss: 0.06059994921088219 acc: 0.9923828\n34 120 loss: 0.007092614192515612 acc: 0.99472654\n34 160 loss: 0.059244029223918915 acc: 0.9880859\ntest acc: 0.7555\n35 0 loss: 0.012271654792129993 acc: 1.0\n35 40 loss: 0.037130206823349 acc: 0.99316406\n35 80 loss: 0.036851681768894196 acc: 0.9928711\n35 120 loss: 0.061075951904058456 acc: 0.99052733\n35 160 loss: 0.011820289306342602 acc: 0.9910156\ntest acc: 0.7511\n36 0 loss: 0.008373185992240906 acc: 1.0\n36 40 loss: 0.008036988787353039 acc: 0.9928711\n36 80 loss: 0.005565176717936993 acc: 0.99375\n36 120 loss: 0.06061221659183502 acc: 0.9897461\n36 160 loss: 0.09633435308933258 acc: 0.9861328\ntest acc: 0.7415\n37 0 loss: 0.0274983998388052 acc: 0.98828125\n37 40 loss: 0.041053660213947296 acc: 0.99248046\n37 80 loss: 0.008937022648751736 acc: 0.9921875\n37 120 loss: 0.007133268751204014 acc: 0.99257815\n37 160 loss: 0.025315620005130768 acc: 0.9867188\ntest acc: 0.7616\n38 0 loss: 0.010041671805083752 acc: 0.99609375\n38 40 loss: 0.009060673415660858 acc: 0.9926758\n38 80 loss: 0.009001070633530617 acc: 0.99306643\n38 120 loss: 0.052127398550510406 acc: 0.98886716\n38 160 loss: 0.057776786386966705 acc: 0.99130857\ntest acc: 0.7485\n39 0 loss: 0.017641469836235046 acc: 0.99609375\n39 40 loss: 0.060879964381456375 acc: 0.9886719\n39 80 loss: 0.043206989765167236 acc: 0.9899414\n39 120 loss: 0.01177421398460865 acc: 0.9921875\n39 160 loss: 0.0626441240310669 acc: 0.9874023\ntest acc: 0.7468\n40 0 loss: 0.03069731965661049 acc: 0.9921875\n40 40 loss: 0.03615622967481613 acc: 0.99296874\n40 80 loss: 0.0328889936208725 acc: 0.9911133\n40 120 loss: 0.027226386591792107 acc: 0.98828125\n40 160 loss: 0.011163835413753986 acc: 0.99257815\ntest acc: 0.7527\n41 0 loss: 0.04685685783624649 acc: 0.98046875\n41 40 loss: 0.016187919303774834 acc: 0.990625\n41 80 loss: 0.04877075180411339 acc: 0.9928711\n41 120 loss: 0.026155782863497734 acc: 0.99052733\n41 160 loss: 0.03190690651535988 acc: 0.9875\ntest acc: 0.7488\n42 0 loss: 0.02407379448413849 acc: 0.984375\n42 40 loss: 0.043994031846523285 acc: 0.99316406\n42 80 loss: 0.021343272179365158 acc: 0.99375\n42 120 loss: 0.0025558362249284983 acc: 0.99296874\n42 160 loss: 0.01808794029057026 acc: 0.99179685\ntest acc: 0.7528\n43 0 loss: 0.01350934524089098 acc: 0.9921875\n43 40 loss: 0.06277118623256683 acc: 0.9916016\n43 80 loss: 0.003377828747034073 acc: 0.9935547\n43 120 loss: 0.014455521479249 acc: 0.9961914\n43 160 loss: 0.0239865742623806 acc: 0.9898437\ntest acc: 0.7492\n44 0 loss: 0.021653609350323677 acc: 0.98828125\n44 40 loss: 0.05545860901474953 acc: 0.9910156\n44 80 loss: 0.00275904918089509 acc: 0.9935547\n44 120 loss: 0.00756032532081008 acc: 0.9933594\n44 160 loss: 0.03469612076878548 acc: 0.99375\ntest acc: 0.7475\n45 0 loss: 0.03854760155081749 acc: 0.98046875\n45 40 loss: 0.05385841056704521 acc: 0.98896486\n45 80 loss: 0.024848826229572296 acc: 0.9935547\n45 120 loss: 0.01240141224116087 acc: 0.99257815\n45 160 loss: 0.03145390748977661 acc: 0.99052733\ntest acc: 0.7663\n46 0 loss: 0.011191440746188164 acc: 0.99609375\n46 40 loss: 0.005799419246613979 acc: 0.9953125\n46 80 loss: 0.002112771850079298 acc: 0.99189454\n46 120 loss: 0.006451375782489777 acc: 0.99140626\n46 160 loss: 0.04690956696867943 acc: 0.9916992\ntest acc: 0.7598\n47 0 loss: 0.0041353050619363785 acc: 1.0\n47 40 loss: 0.005948953330516815 acc: 0.9958008\n47 80 loss: 0.009555880911648273 acc: 0.9946289\n47 120 loss: 0.008851760067045689 acc: 0.9911133\n47 160 loss: 0.014253931120038033 acc: 0.9916016\ntest acc: 0.7508\n48 0 loss: 0.006616435945034027 acc: 1.0\n48 40 loss: 0.021280068904161453 acc: 0.9958008\n48 80 loss: 0.00386999174952507 acc: 0.99365234\n48 120 loss: 0.015950394794344902 acc: 0.990332\n48 160 loss: 0.019876018166542053 acc: 0.99208987\ntest acc: 0.7564\n49 0 loss: 0.011085676960647106 acc: 0.99609375\n49 40 loss: 0.015382936224341393 acc: 0.9928711\n49 80 loss: 0.019487179815769196 acc: 0.99121094\n49 120 loss: 0.025403127074241638 acc: 0.99316406\n49 160 loss: 0.04074309393763542 acc: 0.990332\ntest acc: 0.7552\n50 0 loss: 0.01566425710916519 acc: 0.9921875\n50 40 loss: 0.026060424745082855 acc: 0.9953125\n50 80 loss: 0.010973174124956131 acc: 0.99423826\n50 120 loss: 0.027492541819810867 acc: 0.9928711\n50 160 loss: 0.03447916358709335 acc: 0.99003905\ntest acc: 0.7609\n51 0 loss: 0.010395792312920094 acc: 0.9921875\n51 40 loss: 0.012521565891802311 acc: 0.99208987\n51 80 loss: 0.025715909898281097 acc: 0.9932617\n51 120 loss: 0.023373611271381378 acc: 0.9933594\n51 160 loss: 0.003313407301902771 acc: 0.99306643\ntest acc: 0.7617\n52 0 loss: 0.01501565519720316 acc: 0.9921875\n52 40 loss: 0.028968848288059235 acc: 0.9928711\n52 80 loss: 0.018030479550361633 acc: 0.9944336\n52 120 loss: 0.0024045677855610847 acc: 0.99501956\n52 160 loss: 0.008478173054754734 acc: 0.990332\ntest acc: 0.7554\n53 0 loss: 0.03322487697005272 acc: 0.98828125\n53 40 loss: 0.012551610358059406 acc: 0.9898437\n53 80 loss: 0.004840295296162367 acc: 0.99257815\n53 120 loss: 0.012459816411137581 acc: 0.99482423\n53 160 loss: 0.004580953158438206 acc: 0.9944336\ntest acc: 0.754\n54 0 loss: 0.011744236573576927 acc: 0.9921875\n54 40 loss: 0.00262707332149148 acc: 0.9964844\n54 80 loss: 0.03806065768003464 acc: 0.9945313\n54 120 loss: 0.008705856278538704 acc: 0.9952148\n54 160 loss: 0.010000791400671005 acc: 0.9952148\ntest acc: 0.7623\n55 0 loss: 0.014143958687782288 acc: 0.99609375\n55 40 loss: 0.027741998434066772 acc: 0.99179685\n55 80 loss: 0.012014300562441349 acc: 0.9945313\n55 120 loss: 0.025631703436374664 acc: 0.99423826\n55 160 loss: 0.012124496512115002 acc: 0.9899414\ntest acc: 0.7658\n56 0 loss: 0.0065848263911902905 acc: 1.0\n56 40 loss: 0.014429953880608082 acc: 0.9963867\n56 80 loss: 0.005216210149228573 acc: 0.99492186\n56 120 loss: 0.019561102613806725 acc: 0.99375\n56 160 loss: 0.029159925878047943 acc: 0.9935547\ntest acc: 0.7651\n57 0 loss: 0.010419315658509731 acc: 0.99609375\n57 40 loss: 0.008852312341332436 acc: 0.9927734\n57 80 loss: 0.032714974135160446 acc: 0.9927734\n57 120 loss: 0.030598819255828857 acc: 0.9944336\n57 160 loss: 0.024068547412753105 acc: 0.9935547\ntest acc: 0.7514\n58 0 loss: 0.031352732330560684 acc: 0.98828125\n58 40 loss: 0.012389184907078743 acc: 0.9915039\n58 80 loss: 0.012898522429168224 acc: 0.9932617\n58 120 loss: 0.01305403746664524 acc: 0.99306643\n58 160 loss: 0.033822692930698395 acc: 0.9923828\ntest acc: 0.7664\n59 0 loss: 0.02643873542547226 acc: 0.9921875\n59 40 loss: 0.005829340778291225 acc: 0.9944336\n59 80 loss: 0.02742811292409897 acc: 0.9963867\n59 120 loss: 0.014867011457681656 acc: 0.9964844\n59 160 loss: 0.029353423044085503 acc: 0.9941406\ntest acc: 0.755\n60 0 loss: 0.0031584431417286396 acc: 1.0\n60 40 loss: 0.018109146505594254 acc: 0.99189454\n60 80 loss: 0.03151586279273033 acc: 0.99433595\n60 120 loss: 0.005719190463423729 acc: 0.99248046\n60 160 loss: 0.017440082505345345 acc: 0.99208987\ntest acc: 0.762\n61 0 loss: 0.016591837629675865 acc: 0.9921875\n61 40 loss: 0.01698807254433632 acc: 0.9933594\n61 80 loss: 0.014276877976953983 acc: 0.9945313\n61 120 loss: 0.03390127420425415 acc: 0.99423826\n61 160 loss: 0.04788042604923248 acc: 0.9915039\ntest acc: 0.7656\n62 0 loss: 0.006426368374377489 acc: 1.0\n62 40 loss: 0.002955641597509384 acc: 0.99677736\n62 80 loss: 0.0072655221447348595 acc: 0.996875\n62 120 loss: 0.01430542767047882 acc: 0.99257815\n62 160 loss: 0.01847752556204796 acc: 0.99316406\ntest acc: 0.7631\n63 0 loss: 0.005694523453712463 acc: 1.0\n63 40 loss: 0.012834246270358562 acc: 0.9961914\n63 80 loss: 0.0013550861040130258 acc: 0.9957031\n63 120 loss: 0.04417499527335167 acc: 0.9957031\n63 160 loss: 0.009254954755306244 acc: 0.994043\ntest acc: 0.7539\n64 0 loss: 0.027599161490797997 acc: 0.9921875\n64 40 loss: 0.011589239351451397 acc: 0.9916992\n64 80 loss: 0.01788570173084736 acc: 0.99423826\n64 120 loss: 0.019965214654803276 acc: 0.99501956\n64 160 loss: 0.009246158413589 acc: 0.99609375\ntest acc: 0.7645\n65 0 loss: 0.0042672608979046345 acc: 1.0\n65 40 loss: 0.030791092664003372 acc: 0.99257815\n65 80 loss: 0.0026109241880476475 acc: 0.9939453\n65 120 loss: 0.013551447540521622 acc: 0.99375\n65 160 loss: 0.03301903232932091 acc: 0.99365234\ntest acc: 0.7619\n66 0 loss: 0.0011255068238824606 acc: 1.0\n66 40 loss: 0.004188995808362961 acc: 0.9944336\n66 80 loss: 0.0035012024454772472 acc: 0.9951172\n66 120 loss: 0.029668208211660385 acc: 0.99472654\n66 160 loss: 0.012128990143537521 acc: 0.99306643\ntest acc: 0.7638\n67 0 loss: 0.00942508690059185 acc: 0.99609375\n67 40 loss: 0.017089132219552994 acc: 0.9933594\n67 80 loss: 0.004762632306665182 acc: 0.9927734\n67 120 loss: 0.014744805172085762 acc: 0.99472654\n67 160 loss: 0.003017487470060587 acc: 0.9953125\ntest acc: 0.7647\n68 0 loss: 0.004487504716962576 acc: 0.99609375\n68 40 loss: 0.016036834567785263 acc: 0.9957031\n68 80 loss: 0.026688747107982635 acc: 0.9959961\n68 120 loss: 0.020594926550984383 acc: 0.99736327\n68 160 loss: 0.002187837613746524 acc: 0.9944336\ntest acc: 0.759\n69 0 loss: 0.006624376866966486 acc: 0.99609375\n69 40 loss: 0.018414318561553955 acc: 0.99179685\n69 80 loss: 0.006167959421873093 acc: 0.99541014\n69 120 loss: 0.004005261696875095 acc: 0.99726564\n69 160 loss: 0.0042188107036054134 acc: 0.99492186\ntest acc: 0.7582\n70 0 loss: 0.006884850095957518 acc: 0.99609375\n70 40 loss: 0.005546580534428358 acc: 0.9944336\n70 80 loss: 0.006891958881169558 acc: 0.996875\n70 120 loss: 0.0013201963156461716 acc: 0.9964844\n70 160 loss: 0.035056088119745255 acc: 0.9946289\ntest acc: 0.7581\n71 0 loss: 0.03800627216696739 acc: 0.984375\n71 40 loss: 0.013820920139551163 acc: 0.990918\n71 80 loss: 0.012658306397497654 acc: 0.9959961\n71 120 loss: 0.006438331212848425 acc: 0.9959961\n71 160 loss: 0.008852526545524597 acc: 0.9919922\ntest acc: 0.7714\n72 0 loss: 0.0013818169245496392 acc: 1.0\n72 40 loss: 0.012722481042146683 acc: 0.9981445\n72 80 loss: 0.015843939036130905 acc: 0.9952148\n72 120 loss: 0.0014246394857764244 acc: 0.99746096\n72 160 loss: 0.001994597027078271 acc: 0.99667966\ntest acc: 0.7696\n73 0 loss: 0.005942714866250753 acc: 0.99609375\n73 40 loss: 0.0055116452276706696 acc: 0.9944336\n73 80 loss: 0.00195863819681108 acc: 0.9963867\n73 120 loss: 0.040730033069849014 acc: 0.99482423\n73 160 loss: 0.017424343153834343 acc: 0.9935547\ntest acc: 0.7689\n74 0 loss: 0.005301384720951319 acc: 0.99609375\n74 40 loss: 0.0037576535250991583 acc: 0.99677736\n74 80 loss: 0.06729719787836075 acc: 0.99550784\n74 120 loss: 0.003761366941034794 acc: 0.9961914\n74 160 loss: 0.009053054265677929 acc: 0.9933594\ntest acc: 0.761\n75 0 loss: 0.053241778165102005 acc: 0.98046875\n75 40 loss: 0.016270287334918976 acc: 0.99472654\n75 80 loss: 0.011131719686090946 acc: 0.9958008\n75 120 loss: 0.007614796049892902 acc: 0.99433595\n75 160 loss: 0.04876013100147247 acc: 0.99052733\ntest acc: 0.7654\n76 0 loss: 0.0032641252037137747 acc: 1.0\n76 40 loss: 0.013700463809072971 acc: 0.9957031\n76 80 loss: 0.001325004966929555 acc: 0.99550784\n76 120 loss: 0.008557215332984924 acc: 0.9961914\n76 160 loss: 0.009737218730151653 acc: 0.9957031\ntest acc: 0.7704\n77 0 loss: 0.000974598282482475 acc: 1.0\n77 40 loss: 0.005380300339311361 acc: 0.9951172\n77 80 loss: 0.03105262666940689 acc: 0.9962891\n77 120 loss: 0.002820762572810054 acc: 0.9964844\n77 160 loss: 0.020972123369574547 acc: 0.99296874\ntest acc: 0.751\n78 0 loss: 0.018291322514414787 acc: 0.99609375\n78 40 loss: 0.035368598997592926 acc: 0.99492186\n78 80 loss: 0.0012751782778650522 acc: 0.9962891\n78 120 loss: 0.0014432190218940377 acc: 0.996875\n78 160 loss: 0.004634138196706772 acc: 0.9981445\ntest acc: 0.7695\n79 0 loss: 0.005861255805939436 acc: 1.0\n79 40 loss: 0.0012753959745168686 acc: 0.9963867\n79 80 loss: 0.0007578908698633313 acc: 0.9951172\n79 120 loss: 0.003355995286256075 acc: 0.99492186\n79 160 loss: 0.0366237498819828 acc: 0.9959961\ntest acc: 0.7742\n80 0 loss: 0.0038076534401625395 acc: 1.0\n80 40 loss: 0.004083970561623573 acc: 0.9941406\n80 80 loss: 0.0035938925575464964 acc: 0.9945313\n80 120 loss: 0.00451141269877553 acc: 0.9959961\n80 160 loss: 0.00415085582062602 acc: 0.99306643\ntest acc: 0.7757\n81 0 loss: 0.001184022636152804 acc: 1.0\n81 40 loss: 0.019809283316135406 acc: 0.9962891\n81 80 loss: 0.00223371759057045 acc: 0.9957031\n81 120 loss: 0.013433686457574368 acc: 0.9961914\n81 160 loss: 0.006743726786226034 acc: 0.9952148\ntest acc: 0.7662\n82 0 loss: 0.010781606659293175 acc: 0.9921875\n82 40 loss: 0.006925382651388645 acc: 0.99560547\n82 80 loss: 0.010510286316275597 acc: 0.9958008\n82 120 loss: 0.02532421238720417 acc: 0.99609375\n82 160 loss: 0.0060854945331811905 acc: 0.99482423\ntest acc: 0.7676\n83 0 loss: 0.008527938276529312 acc: 0.99609375\n83 40 loss: 0.00298653868958354 acc: 0.996875\n83 80 loss: 0.006439500954002142 acc: 0.99785155\n83 120 loss: 0.017716635018587112 acc: 0.9963867\n83 160 loss: 0.031053779646754265 acc: 0.9921875\ntest acc: 0.7688\n84 0 loss: 0.025241391733288765 acc: 0.99609375\n84 40 loss: 0.034855153411626816 acc: 0.9959961\n84 80 loss: 0.01794278249144554 acc: 0.99306643\n84 120 loss: 0.015717439353466034 acc: 0.99482423\n84 160 loss: 0.007712395396083593 acc: 0.9962891\ntest acc: 0.7748\n85 0 loss: 0.005879879463464022 acc: 1.0\n85 40 loss: 0.0003293650224804878 acc: 0.99833983\n85 80 loss: 0.0006099016172811389 acc: 0.99902344\n85 120 loss: 0.0035462623927742243 acc: 0.9980469\n85 160 loss: 0.009380953386425972 acc: 0.9958984\ntest acc: 0.7656\n86 0 loss: 0.006810321472585201 acc: 1.0\n86 40 loss: 0.002462011529132724 acc: 0.9962891\n86 80 loss: 0.017634175717830658 acc: 0.99667966\n86 120 loss: 0.0033401506952941418 acc: 0.996875\n86 160 loss: 0.009466873481869698 acc: 0.99433595\ntest acc: 0.7721\n87 0 loss: 0.0049634091556072235 acc: 1.0\n87 40 loss: 0.004761140793561935 acc: 0.99746096\n87 80 loss: 0.021521469578146935 acc: 0.99667966\n87 120 loss: 0.001322212629020214 acc: 0.99736327\n87 160 loss: 0.004024482797831297 acc: 0.9962891\ntest acc: 0.7733\n88 0 loss: 0.0009714682819321752 acc: 1.0\n88 40 loss: 0.020073918625712395 acc: 0.99677736\n88 80 loss: 0.021921329200267792 acc: 0.9941406\n88 120 loss: 0.009687218815088272 acc: 0.993457\n88 160 loss: 0.020185619592666626 acc: 0.99306643\ntest acc: 0.7753\n89 0 loss: 0.003757661674171686 acc: 1.0\n89 40 loss: 0.0011436650529503822 acc: 0.99716794\n89 80 loss: 0.019794907420873642 acc: 0.9964844\n89 120 loss: 0.0034728655591607094 acc: 0.99501956\n89 160 loss: 0.035189129412174225 acc: 0.9941406\ntest acc: 0.7736\n90 0 loss: 0.0028323957230895758 acc: 1.0\n90 40 loss: 0.011283455416560173 acc: 0.9977539\n90 80 loss: 0.00036864372668787837 acc: 0.9977539\n90 120 loss: 0.021158836781978607 acc: 0.99658203\n90 160 loss: 0.018776951357722282 acc: 0.99482423\ntest acc: 0.7714\n91 0 loss: 0.01542063057422638 acc: 0.99609375\n91 40 loss: 0.007752538658678532 acc: 0.99501956\n91 80 loss: 0.009648486040532589 acc: 0.9952148\n91 120 loss: 0.0026702340692281723 acc: 0.99472654\n91 160 loss: 0.03758050501346588 acc: 0.99482423\ntest acc: 0.7811\n92 0 loss: 0.0023535219952464104 acc: 1.0\n92 40 loss: 0.001600121147930622 acc: 0.99794924\n92 80 loss: 0.004052086267620325 acc: 0.99716794\n92 120 loss: 0.0018015195382758975 acc: 0.99677736\n92 160 loss: 0.005338916555047035 acc: 0.9969727\ntest acc: 0.77\n93 0 loss: 0.01186104491353035 acc: 0.9921875\n93 40 loss: 0.012254346162080765 acc: 0.9962891\n93 80 loss: 0.006089480128139257 acc: 0.99492186\n93 120 loss: 0.02690860629081726 acc: 0.9964844\n93 160 loss: 0.0021700598299503326 acc: 0.9969727\ntest acc: 0.7759\n94 0 loss: 0.003119145520031452 acc: 1.0\n94 40 loss: 0.0014414635952562094 acc: 0.99785155\n94 80 loss: 0.009665044024586678 acc: 0.99716794\n94 120 loss: 0.009909928776323795 acc: 0.99658203\n94 160 loss: 0.0042929393239319324 acc: 0.99365234\ntest acc: 0.7744\n95 0 loss: 0.005190638825297356 acc: 0.99609375\n95 40 loss: 0.002670339308679104 acc: 0.99902344\n95 80 loss: 0.018377987667918205 acc: 0.99833983\n95 120 loss: 0.0058417185209691525 acc: 0.9986328\n95 160 loss: 0.004900191444903612 acc: 0.9984375\ntest acc: 0.7689\n96 0 loss: 0.014539407566189766 acc: 0.99609375\n96 40 loss: 0.014139545150101185 acc: 0.99248046\n96 80 loss: 0.008307973854243755 acc: 0.9946289\n96 120 loss: 0.030176423490047455 acc: 0.99560547\n96 160 loss: 0.0012381415581330657 acc: 0.9963867\ntest acc: 0.7793\n97 0 loss: 0.004101609345525503 acc: 0.99609375\n97 40 loss: 0.009426610544323921 acc: 0.9963867\n97 80 loss: 0.01077285036444664 acc: 0.99550784\n97 120 loss: 0.002537730848416686 acc: 0.9951172\n97 160 loss: 0.0076548876240849495 acc: 0.9958984\ntest acc: 0.7787\n98 0 loss: 0.0023424343671649694 acc: 1.0\n98 40 loss: 0.012498706579208374 acc: 0.9977539\n98 80 loss: 0.0034782234579324722 acc: 0.9958984\n98 120 loss: 0.0013970425352454185 acc: 0.9958984\n98 160 loss: 0.0008582636946812272 acc: 0.99716794\ntest acc: 0.7753\n99 0 loss: 0.0014345727395266294 acc: 1.0\n99 40 loss: 0.001441235770471394 acc: 0.9976562\n99 80 loss: 0.003526410786435008 acc: 0.99785155\n99 120 loss: 0.013249916955828667 acc: 0.9959961\n99 160 loss: 0.002000099513679743 acc: 0.9951172\ntest acc: 0.7624\n100 0 loss: 0.04421447589993477 acc: 0.98828125\n100 40 loss: 0.00229349615983665 acc: 0.994043\n100 80 loss: 0.03564392030239105 acc: 0.9962891\n100 120 loss: 0.03116503357887268 acc: 0.9963867\n100 160 loss: 0.05159476026892662 acc: 0.9953125\ntest acc: 0.7747\n101 0 loss: 0.03859259560704231 acc: 0.984375\n101 40 loss: 0.0016341248992830515 acc: 0.99726564\n101 80 loss: 0.03524158522486687 acc: 0.99746096\n101 120 loss: 0.03453376516699791 acc: 0.9963867\n101 160 loss: 0.003524419153109193 acc: 0.99667966\ntest acc: 0.7794\n102 0 loss: 0.007825481705367565 acc: 0.99609375\n102 40 loss: 0.003219272941350937 acc: 0.9987305\n102 80 loss: 0.00026469532167539 acc: 0.9986328\n102 120 loss: 0.03306172043085098 acc: 0.99726564\n102 160 loss: 0.02296397276222706 acc: 0.99609375\ntest acc: 0.7684\n103 0 loss: 0.013648494146764278 acc: 0.9921875\n103 40 loss: 0.00427148025482893 acc: 0.994043\n103 80 loss: 0.0007552230381406844 acc: 0.99433595\n103 120 loss: 0.03148622438311577 acc: 0.99296874\n103 160 loss: 0.027864299714565277 acc: 0.9946289\ntest acc: 0.7773\n104 0 loss: 0.0008552192593924701 acc: 1.0\n104 40 loss: 0.0004783503827638924 acc: 0.99746096\n104 80 loss: 0.002717152703553438 acc: 0.9975586\n104 120 loss: 0.004016539081931114 acc: 0.9977539\n104 160 loss: 0.007442978676408529 acc: 0.9969727\ntest acc: 0.7814\n105 0 loss: 0.0018687034025788307 acc: 1.0\n105 40 loss: 0.0013494990998879075 acc: 0.9962891\n105 80 loss: 0.002060013823211193 acc: 0.9959961\n105 120 loss: 0.0046053859405219555 acc: 0.99609375\n105 160 loss: 0.009128397330641747 acc: 0.9976562\ntest acc: 0.7827\n106 0 loss: 0.0037119018379598856 acc: 1.0\n106 40 loss: 5.584792234003544e-05 acc: 0.99902344\n106 80 loss: 0.003675469197332859 acc: 0.9984375\n106 120 loss: 0.023365743458271027 acc: 0.9987305\n106 160 loss: 0.004519606940448284 acc: 0.99785155\ntest acc: 0.7772\n107 0 loss: 0.020559770986437798 acc: 0.9921875\n107 40 loss: 0.01562662422657013 acc: 0.99550784\n107 80 loss: 0.009665748104453087 acc: 0.9958984\n107 120 loss: 0.012090915814042091 acc: 0.996875\n107 160 loss: 0.0007603630656376481 acc: 0.9961914\ntest acc: 0.7802\n108 0 loss: 0.00045485293958336115 acc: 1.0\n108 40 loss: 0.014263812452554703 acc: 0.99716794\n108 80 loss: 0.0021814489737153053 acc: 0.9976562\n108 120 loss: 0.012400448322296143 acc: 0.99794924\n108 160 loss: 0.015319879166781902 acc: 0.99794924\ntest acc: 0.7712\n109 0 loss: 0.004291889723390341 acc: 0.99609375\n109 40 loss: 0.004926621448248625 acc: 0.9957031\n109 80 loss: 0.0010324623435735703 acc: 0.99746096\n109 120 loss: 0.039200376719236374 acc: 0.9953125\n109 160 loss: 0.0011191368103027344 acc: 0.9961914\ntest acc: 0.7774\n110 0 loss: 0.004405593499541283 acc: 1.0\n110 40 loss: 0.0024649840779602528 acc: 0.9981445\n110 80 loss: 0.0005806679837405682 acc: 0.9988281\n110 120 loss: 0.024150757119059563 acc: 0.9980469\n110 160 loss: 0.019758174195885658 acc: 0.99716794\ntest acc: 0.7785\n111 0 loss: 0.004699473734945059 acc: 1.0\n111 40 loss: 0.0017802289221435785 acc: 0.99550784\n111 80 loss: 0.016931943595409393 acc: 0.99716794\n111 120 loss: 0.005535532254725695 acc: 0.99296874\n111 160 loss: 0.007295219227671623 acc: 0.99492186\ntest acc: 0.774\n112 0 loss: 0.0022404880728572607 acc: 1.0\n112 40 loss: 0.008323470130562782 acc: 0.99560547\n112 80 loss: 0.0006310648750513792 acc: 0.99736327\n112 120 loss: 0.03171052038669586 acc: 0.9976562\n112 160 loss: 0.00046280675451271236 acc: 0.99716794\ntest acc: 0.7724\n113 0 loss: 0.02231149934232235 acc: 0.98828125\n113 40 loss: 0.003871806664392352 acc: 0.9981445\n113 80 loss: 0.014692484401166439 acc: 0.9962891\n113 120 loss: 0.006689149886369705 acc: 0.9970703\n113 160 loss: 0.002612396376207471 acc: 0.99667966\ntest acc: 0.7672\n114 0 loss: 0.03970805183053017 acc: 0.984375\n114 40 loss: 0.004635253921151161 acc: 0.9975586\n114 80 loss: 0.0024108977522701025 acc: 0.9970703\n114 120 loss: 0.021145379170775414 acc: 0.99746096\n114 160 loss: 0.022134358063340187 acc: 0.99736327\ntest acc: 0.7813\n115 0 loss: 0.004371712915599346 acc: 1.0\n115 40 loss: 0.005771897733211517 acc: 0.9959961\n115 80 loss: 0.004311590921133757 acc: 0.99541014\n115 120 loss: 0.005037190858274698 acc: 0.99833983\n115 160 loss: 0.0023320522159337997 acc: 0.9958008\ntest acc: 0.7739\n116 0 loss: 0.006360227707773447 acc: 0.99609375\n116 40 loss: 0.008432119153439999 acc: 0.9977539\n116 80 loss: 0.02504451759159565 acc: 0.9986328\n116 120 loss: 0.001759276376105845 acc: 0.9975586\n116 160 loss: 0.0002485924633219838 acc: 0.9982422\ntest acc: 0.7756\n117 0 loss: 0.0021427113097161055 acc: 1.0\n117 40 loss: 0.0014624600298702717 acc: 0.9970703\n117 80 loss: 0.0024551074020564556 acc: 0.99794924\n117 120 loss: 0.0020818563643842936 acc: 0.9989258\n117 160 loss: 0.0017115201335400343 acc: 0.9970703\ntest acc: 0.7783\n118 0 loss: 0.0027444942388683558 acc: 1.0\n118 40 loss: 0.016362911090254784 acc: 0.99560547\n118 80 loss: 0.018600035458803177 acc: 0.9959961\n118 120 loss: 0.014192413538694382 acc: 0.99667966\n118 160 loss: 0.0017178794369101524 acc: 0.996875\ntest acc: 0.7699\n119 0 loss: 0.01029681321233511 acc: 0.99609375\n119 40 loss: 0.015278168022632599 acc: 0.9959961\n119 80 loss: 0.018665576353669167 acc: 0.9975586\n119 120 loss: 0.0009484373149462044 acc: 0.9980469\n119 160 loss: 0.0042482949793338776 acc: 0.99736327\ntest acc: 0.7789\n120 0 loss: 0.026038318872451782 acc: 0.9921875\n120 40 loss: 0.0004667987232096493 acc: 0.9964844\n120 80 loss: 0.003628354286774993 acc: 0.9980469\n120 120 loss: 0.00047362904297187924 acc: 0.9984375\n120 160 loss: 0.00473049795255065 acc: 0.9975586\ntest acc: 0.7806\n121 0 loss: 0.017553085461258888 acc: 0.99609375\n121 40 loss: 0.0040901158936321735 acc: 0.9976562\n121 80 loss: 0.02173019014298916 acc: 0.9989258\n121 120 loss: 0.000792614184319973 acc: 0.99746096\n121 160 loss: 0.0024566296488046646 acc: 0.9982422\ntest acc: 0.7786\n122 0 loss: 0.0015279261860996485 acc: 1.0\n122 40 loss: 0.0072370488196611404 acc: 0.9957031\n122 80 loss: 0.040892813354730606 acc: 0.99482423\n122 120 loss: 0.012580689042806625 acc: 0.99667966\n122 160 loss: 0.008752302266657352 acc: 0.9958984\ntest acc: 0.788\n123 0 loss: 0.00981203094124794 acc: 0.9921875\n123 40 loss: 0.01362914964556694 acc: 0.9980469\n123 80 loss: 0.02010270766913891 acc: 0.9958984\n123 120 loss: 0.01873859204351902 acc: 0.99667966\n123 160 loss: 0.007827038876712322 acc: 0.99472654\ntest acc: 0.7858\n124 0 loss: 0.0026184208691120148 acc: 1.0\n124 40 loss: 0.0033237829338759184 acc: 0.99785155\n124 80 loss: 0.0033305026590824127 acc: 0.99785155\n124 120 loss: 0.0009796005906537175 acc: 0.9977539\n124 160 loss: 0.004527523182332516 acc: 0.99833983\ntest acc: 0.7883\n125 0 loss: 0.0006947810179553926 acc: 1.0\n125 40 loss: 0.009923510253429413 acc: 0.99794924\n125 80 loss: 0.0007655849913135171 acc: 0.9987305\n125 120 loss: 0.00018146693764720112 acc: 0.9981445\n125 160 loss: 0.028861630707979202 acc: 0.9977539\ntest acc: 0.7857\n126 0 loss: 0.0003999209438916296 acc: 1.0\n126 40 loss: 0.0025364407338202 acc: 0.9977539\n126 80 loss: 0.00034403373138047755 acc: 0.9980469\n126 120 loss: 0.020476046949625015 acc: 0.99677736\n126 160 loss: 0.00782886054366827 acc: 0.9969727\ntest acc: 0.7884\n127 0 loss: 0.0005088405450806022 acc: 1.0\n127 40 loss: 0.00034991835127584636 acc: 0.99902344\n127 80 loss: 0.020706096664071083 acc: 0.9987305\n127 120 loss: 0.005844308529049158 acc: 0.9982422\n127 160 loss: 0.005074366927146912 acc: 0.9982422\ntest acc: 0.782\n128 0 loss: 0.0013466028030961752 acc: 1.0\n128 40 loss: 0.009788711555302143 acc: 0.9976562\n128 80 loss: 0.006536243483424187 acc: 0.99667966\n128 120 loss: 0.0025126091204583645 acc: 0.9969727\n128 160 loss: 0.030883776023983955 acc: 0.9953125\ntest acc: 0.7832\n129 0 loss: 0.017354682087898254 acc: 0.99609375\n129 40 loss: 0.012994084507226944 acc: 0.9964844\n129 80 loss: 0.017758920788764954 acc: 0.99785155\n129 120 loss: 0.015208620578050613 acc: 0.9975586\n129 160 loss: 0.0024151236284524202 acc: 0.99716794\ntest acc: 0.7848\n130 0 loss: 0.001016553957015276 acc: 1.0\n130 40 loss: 0.005656258203089237 acc: 0.9980469\n130 80 loss: 0.0066815391182899475 acc: 0.99716794\n130 120 loss: 0.01228965725749731 acc: 0.9951172\n130 160 loss: 0.0059737456031143665 acc: 0.99609375\ntest acc: 0.7799\n131 0 loss: 0.017564358189702034 acc: 0.99609375\n131 40 loss: 0.0014448747970163822 acc: 0.9970703\n131 80 loss: 0.006409702822566032 acc: 0.9964844\n131 120 loss: 0.0010066777467727661 acc: 0.99746096\n131 160 loss: 0.0004685048188548535 acc: 0.9977539\ntest acc: 0.779\n132 0 loss: 0.0006573484861291945 acc: 1.0\n132 40 loss: 0.025474844500422478 acc: 0.9933594\n132 80 loss: 0.006818454246968031 acc: 0.99677736\n132 120 loss: 0.006824033334851265 acc: 0.99667966\n132 160 loss: 0.0006229563150554895 acc: 0.9969727\ntest acc: 0.7856\n133 0 loss: 0.002634602366015315 acc: 1.0\n133 40 loss: 0.005329182371497154 acc: 0.9984375\n133 80 loss: 0.009165273047983646 acc: 0.9977539\n133 120 loss: 0.0005823232932016253 acc: 0.99716794\n133 160 loss: 0.0006185691454447806 acc: 0.99726564\ntest acc: 0.7812\n134 0 loss: 0.0006293612532317638 acc: 1.0\n134 40 loss: 0.0011695342836901546 acc: 0.99853516\n134 80 loss: 0.00032496670610271394 acc: 0.99912107\n134 120 loss: 0.0051665171049535275 acc: 0.9982422\n134 160 loss: 0.004554418381303549 acc: 0.99609375\ntest acc: 0.785\n135 0 loss: 0.0013780049048364162 acc: 1.0\n135 40 loss: 0.005830955691635609 acc: 0.99384767\n135 80 loss: 0.00222934246994555 acc: 0.9970703\n135 120 loss: 0.0037993420846760273 acc: 0.9962891\n135 160 loss: 0.009171612560749054 acc: 0.99746096\ntest acc: 0.784\n136 0 loss: 0.0061389002948999405 acc: 0.99609375\n136 40 loss: 0.005804785527288914 acc: 0.9988281\n136 80 loss: 0.015488822013139725 acc: 0.99833983\n136 120 loss: 0.0008789994171820581 acc: 0.99785155\n136 160 loss: 0.0002422042889520526 acc: 0.9980469\ntest acc: 0.7888\n137 0 loss: 0.0054780831560492516 acc: 0.99609375\n137 40 loss: 0.0002578322892077267 acc: 0.9994141\n137 80 loss: 0.00020605373720172793 acc: 0.99921876\n137 120 loss: 0.021683383733034134 acc: 0.9993164\n137 160 loss: 0.006675593089312315 acc: 0.9975586\ntest acc: 0.7807\n138 0 loss: 0.0030971579253673553 acc: 1.0\n138 40 loss: 0.002552475081756711 acc: 0.99833983\n138 80 loss: 0.001465096022002399 acc: 0.99785155\n138 120 loss: 0.0009849956259131432 acc: 0.9959961\n138 160 loss: 0.0032717755530029535 acc: 0.9964844\ntest acc: 0.7768\n139 0 loss: 0.003241215366870165 acc: 1.0\n139 40 loss: 0.006677481811493635 acc: 0.9964844\n139 80 loss: 0.0013793115504086018 acc: 0.9969727\n139 120 loss: 0.02554847113788128 acc: 0.9959961\n139 160 loss: 0.001128240954130888 acc: 0.996875\ntest acc: 0.7807\n140 0 loss: 0.006073585711419582 acc: 0.99609375\n140 40 loss: 0.0009688141290098429 acc: 0.9976562\n140 80 loss: 0.0010129095753654838 acc: 0.99794924\n140 120 loss: 0.0006853031227365136 acc: 0.99785155\n140 160 loss: 0.02379509061574936 acc: 0.9969727\ntest acc: 0.7845\n141 0 loss: 0.003451752010732889 acc: 1.0\n141 40 loss: 0.0005860194796696305 acc: 0.99746096\n141 80 loss: 0.0036998982541263103 acc: 0.99667966\n141 120 loss: 0.007486030459403992 acc: 0.9982422\n141 160 loss: 0.0389898307621479 acc: 0.9984375\ntest acc: 0.7845\n142 0 loss: 0.0006726693827658892 acc: 1.0\n142 40 loss: 0.009177731350064278 acc: 0.99677736\n142 80 loss: 0.002171279862523079 acc: 0.99785155\n142 120 loss: 0.006021833047270775 acc: 0.99833983\n142 160 loss: 0.001338472357019782 acc: 0.9969727\ntest acc: 0.7853\n143 0 loss: 0.0049936650320887566 acc: 1.0\n143 40 loss: 0.00033354584593325853 acc: 0.99794924\n143 80 loss: 0.0008301576017402112 acc: 0.9984375\n143 120 loss: 0.008845300413668156 acc: 0.99609375\n143 160 loss: 0.0016899864422157407 acc: 0.9962891\ntest acc: 0.7844\n144 0 loss: 0.01359835546463728 acc: 0.99609375\n144 40 loss: 0.0009691260638646781 acc: 0.99853516\n144 80 loss: 0.00046906585339456797 acc: 0.9964844\n144 120 loss: 0.0036054663360118866 acc: 0.99746096\n144 160 loss: 0.001325364806689322 acc: 0.99853516\ntest acc: 0.7887\n145 0 loss: 0.00498939398676157 acc: 0.99609375\n145 40 loss: 0.0023318615276366472 acc: 0.99746096\n145 80 loss: 0.005758088547736406 acc: 0.9977539\n145 120 loss: 0.0004348095681052655 acc: 0.99794924\n145 160 loss: 0.013757102191448212 acc: 0.99716794\ntest acc: 0.7795\n146 0 loss: 0.006068807560950518 acc: 0.99609375\n146 40 loss: 0.0008933879435062408 acc: 0.99794924\n146 80 loss: 0.001987967872992158 acc: 0.9988281\n146 120 loss: 6.706206477247179e-05 acc: 0.9994141\n146 160 loss: 0.00040947055094875395 acc: 0.99912107\ntest acc: 0.7834\n147 0 loss: 0.007012003101408482 acc: 0.99609375\n147 40 loss: 0.013677147217094898 acc: 0.9984375\n147 80 loss: 0.02460012584924698 acc: 0.996875\n147 120 loss: 0.00226058391854167 acc: 0.9963867\n147 160 loss: 0.0008284443756565452 acc: 0.9981445\ntest acc: 0.7903\n148 0 loss: 0.0040283496491611 acc: 1.0\n148 40 loss: 0.011182181537151337 acc: 0.99833983\n148 80 loss: 0.02745440974831581 acc: 0.9980469\n148 120 loss: 0.004247270058840513 acc: 0.996875\n148 160 loss: 0.003294667461887002 acc: 0.9958984\ntest acc: 0.7849\n149 0 loss: 0.003262333106249571 acc: 1.0\n149 40 loss: 0.0031964846421033144 acc: 0.9980469\n149 80 loss: 0.003366719698533416 acc: 0.9975586\n149 120 loss: 0.004055531229823828 acc: 0.9986328\n149 160 loss: 0.010713474825024605 acc: 0.9981445\ntest acc: 0.7923\n150 0 loss: 0.00020864132966380566 acc: 1.0\n150 40 loss: 0.006910393014550209 acc: 0.99609375\n150 80 loss: 0.006511318031698465 acc: 0.9957031\n150 120 loss: 0.004461823962628841 acc: 0.9975586\n150 160 loss: 0.0015861624851822853 acc: 0.9981445\ntest acc: 0.7845\n151 0 loss: 0.04176739230751991 acc: 0.98828125\n151 40 loss: 0.0013857003068551421 acc: 0.99794924\n151 80 loss: 0.031216902658343315 acc: 0.9986328\n151 120 loss: 0.003533264622092247 acc: 0.99785155\n151 160 loss: 0.002799549838528037 acc: 0.99794924\ntest acc: 0.7838\n152 0 loss: 0.011878001503646374 acc: 0.99609375\n152 40 loss: 0.013502752408385277 acc: 0.99482423\n152 80 loss: 0.011390390805900097 acc: 0.99746096\n152 120 loss: 0.001943836803548038 acc: 0.99833983\n152 160 loss: 0.008818471804261208 acc: 0.9970703\ntest acc: 0.7818\n153 0 loss: 0.0042167711071670055 acc: 1.0\n153 40 loss: 0.0001548534637549892 acc: 0.9989258\n153 80 loss: 0.0013282267609611154 acc: 0.9975586\n153 120 loss: 0.004965863656252623 acc: 0.99853516\n153 160 loss: 0.03243515267968178 acc: 0.99746096\ntest acc: 0.775\n154 0 loss: 0.02013358660042286 acc: 0.9921875\n154 40 loss: 0.016282709315419197 acc: 0.99716794\n154 80 loss: 0.0006265369011089206 acc: 0.9969727\n154 120 loss: 0.0022778024431318045 acc: 0.9977539\n154 160 loss: 0.0020083491690456867 acc: 0.99726564\ntest acc: 0.7832\n155 0 loss: 0.025288185104727745 acc: 0.984375\n155 40 loss: 0.004059116821736097 acc: 0.99736327\n155 80 loss: 0.0009644350502640009 acc: 0.9984375\n155 120 loss: 0.0063433656468987465 acc: 0.9969727\n155 160 loss: 0.00180801039095968 acc: 0.996875\ntest acc: 0.7848\n156 0 loss: 0.00018633685249369591 acc: 1.0\n156 40 loss: 0.00014601116708945483 acc: 0.9987305\n156 80 loss: 0.0003379205008968711 acc: 0.99833983\n156 120 loss: 0.0005378660862334073 acc: 0.99902344\n156 160 loss: 0.0023049688898026943 acc: 0.9986328\ntest acc: 0.7853\n157 0 loss: 0.0006585349328815937 acc: 1.0\n157 40 loss: 0.004671501461416483 acc: 0.9958008\n157 80 loss: 0.012760678306221962 acc: 0.99833983\n157 120 loss: 0.018220899626612663 acc: 0.9977539\n157 160 loss: 0.012890278361737728 acc: 0.9964844\ntest acc: 0.7814\n158 0 loss: 0.0008018247899599373 acc: 1.0\n158 40 loss: 0.02106012962758541 acc: 0.9980469\n158 80 loss: 0.006305808667093515 acc: 0.9982422\n158 120 loss: 0.0004776332643814385 acc: 0.9977539\n158 160 loss: 0.002010478638112545 acc: 0.99785155\ntest acc: 0.7799\n159 0 loss: 0.00399798434227705 acc: 1.0\n159 40 loss: 0.007283056620508432 acc: 0.9975586\n159 80 loss: 0.009517228230834007 acc: 0.99785155\n159 120 loss: 0.0020248021464794874 acc: 0.99785155\n159 160 loss: 0.0004123584658373147 acc: 0.99785155\ntest acc: 0.7887\n160 0 loss: 0.0007114059990271926 acc: 1.0\n160 40 loss: 0.0030121770687401295 acc: 0.9984375\n160 80 loss: 0.000695098948199302 acc: 0.99912107\n160 120 loss: 0.0015267775161191821 acc: 0.99960935\n160 160 loss: 0.0003965660580433905 acc: 0.99853516\ntest acc: 0.786\n161 0 loss: 0.00036661175545305014 acc: 1.0\n161 40 loss: 0.00035431189462542534 acc: 0.9981445\n161 80 loss: 0.0010520202340558171 acc: 0.99833983\n161 120 loss: 0.004508230369538069 acc: 0.9984375\n161 160 loss: 0.0007066361722536385 acc: 0.99794924\ntest acc: 0.782\n162 0 loss: 0.00048027525190263987 acc: 1.0\n162 40 loss: 0.0010125550907105207 acc: 0.99902344\n162 80 loss: 0.0009747008443810046 acc: 0.99785155\n162 120 loss: 0.012761778198182583 acc: 0.99726564\n162 160 loss: 0.06237177923321724 acc: 0.99560547\ntest acc: 0.788\n163 0 loss: 0.004940986633300781 acc: 1.0\n163 40 loss: 0.0008718845201656222 acc: 0.9989258\n163 80 loss: 0.0005952655337750912 acc: 0.99853516\n163 120 loss: 0.0012460545403882861 acc: 0.99785155\n163 160 loss: 0.015490350313484669 acc: 0.99667966\ntest acc: 0.7779\n164 0 loss: 0.0025457097217440605 acc: 1.0\n164 40 loss: 0.006475196219980717 acc: 0.9952148\n164 80 loss: 0.0017507867887616158 acc: 0.99667966\n164 120 loss: 0.002314418787136674 acc: 0.99726564\n164 160 loss: 0.0024401317350566387 acc: 0.9962891\ntest acc: 0.789\n165 0 loss: 0.0042047398164868355 acc: 1.0\n165 40 loss: 0.00013084520469419658 acc: 0.9994141\n165 80 loss: 0.02448344975709915 acc: 0.9982422\n165 120 loss: 0.0010741179576143622 acc: 0.99736327\n165 160 loss: 0.005560370162129402 acc: 0.9975586\ntest acc: 0.7889\n166 0 loss: 0.0007356023415923119 acc: 1.0\n166 40 loss: 0.0023095388896763325 acc: 0.99921876\n166 80 loss: 0.0031115177553147078 acc: 0.9995117\n166 120 loss: 4.201853516860865e-05 acc: 0.9995117\n166 160 loss: 0.009212170727550983 acc: 0.9995117\ntest acc: 0.789\n167 0 loss: 0.0012536447029560804 acc: 1.0\n167 40 loss: 0.0008462732075713575 acc: 0.99853516\n167 80 loss: 0.002185448771342635 acc: 0.99736327\n167 120 loss: 0.0019087028922513127 acc: 0.99560547\n167 160 loss: 0.006907563656568527 acc: 0.99746096\ntest acc: 0.79\n168 0 loss: 0.02605363167822361 acc: 0.99609375\n168 40 loss: 0.0150552187114954 acc: 0.9959961\n168 80 loss: 0.004940234124660492 acc: 0.99833983\n168 120 loss: 0.0010840684408321977 acc: 0.9984375\n168 160 loss: 0.00042043154826387763 acc: 0.9988281\ntest acc: 0.7877\n169 0 loss: 0.00289987213909626 acc: 1.0\n169 40 loss: 0.008541934192180634 acc: 0.9987305\n169 80 loss: 0.008885099552571774 acc: 0.9981445\n169 120 loss: 0.0010917121544480324 acc: 0.99746096\n169 160 loss: 0.0005622637108899653 acc: 0.99794924\ntest acc: 0.7911\n170 0 loss: 0.0015249139396473765 acc: 1.0\n170 40 loss: 0.00038613739889115095 acc: 0.9982422\n170 80 loss: 0.0008483418496325612 acc: 0.9984375\n170 120 loss: 0.0027900247368961573 acc: 0.9989258\n170 160 loss: 0.0012113595148548484 acc: 0.9988281\ntest acc: 0.7825\n171 0 loss: 0.014473618008196354 acc: 0.99609375\n171 40 loss: 0.005850637797266245 acc: 0.99658203\n171 80 loss: 0.011525164358317852 acc: 0.99716794\n171 120 loss: 0.007962430827319622 acc: 0.9970703\n171 160 loss: 0.007201896980404854 acc: 0.99785155\ntest acc: 0.7848\n172 0 loss: 0.00032679096329957247 acc: 1.0\n172 40 loss: 0.0010103634558618069 acc: 0.9977539\n172 80 loss: 0.005234385374933481 acc: 0.99833983\n172 120 loss: 0.0025019273161888123 acc: 0.9989258\n172 160 loss: 9.799193503567949e-05 acc: 0.9984375\ntest acc: 0.789\n173 0 loss: 0.011427091434597969 acc: 0.99609375\n173 40 loss: 0.002892204327508807 acc: 0.9970703\n173 80 loss: 0.04705943912267685 acc: 0.9964844\n173 120 loss: 0.0004919907660223544 acc: 0.99726564\n173 160 loss: 0.005603580269962549 acc: 0.99785155\ntest acc: 0.7882\n174 0 loss: 0.0013690553605556488 acc: 1.0\n174 40 loss: 0.0015404304722324014 acc: 0.99833983\n174 80 loss: 0.00017165852477774024 acc: 0.99794924\n174 120 loss: 0.0018018835689872503 acc: 0.99560547\n174 160 loss: 0.009922449477016926 acc: 0.9964844\ntest acc: 0.7904\n175 0 loss: 0.0064990282990038395 acc: 0.99609375\n175 40 loss: 0.0048452033661305904 acc: 0.99726564\n175 80 loss: 0.007159728556871414 acc: 0.9976562\n175 120 loss: 0.014100078493356705 acc: 0.9976562\n175 160 loss: 0.0017135883681476116 acc: 0.9981445\ntest acc: 0.7869\n176 0 loss: 0.0037848884239792824 acc: 1.0\n176 40 loss: 0.0031222582329064608 acc: 0.9988281\n176 80 loss: 0.0007186302682384849 acc: 0.99853516\n176 120 loss: 0.000932242488488555 acc: 0.9988281\n176 160 loss: 0.0010386515641584992 acc: 0.9989258\ntest acc: 0.7957\n177 0 loss: 0.00020909009617753327 acc: 1.0\n177 40 loss: 0.008944690227508545 acc: 0.9994141\n177 80 loss: 0.03582145646214485 acc: 0.99794924\n177 120 loss: 0.005565495230257511 acc: 0.99833983\n177 160 loss: 0.021641578525304794 acc: 0.9986328\ntest acc: 0.7867\n178 0 loss: 0.0014123363653197885 acc: 1.0\n178 40 loss: 0.002449017483741045 acc: 0.9969727\n178 80 loss: 0.0022414475679397583 acc: 0.9975586\n178 120 loss: 0.00749783543869853 acc: 0.9976562\n178 160 loss: 0.003938619047403336 acc: 0.9964844\ntest acc: 0.792\n179 0 loss: 0.014450468122959137 acc: 0.99609375\n179 40 loss: 0.0006047729402780533 acc: 0.9995117\n179 80 loss: 0.00035028380807489157 acc: 0.99921876\n179 120 loss: 0.0005668096127919853 acc: 0.99853516\n179 160 loss: 0.0026456466875970364 acc: 0.99833983\ntest acc: 0.7897\n180 0 loss: 0.0005942287389189005 acc: 1.0\n180 40 loss: 0.01717798411846161 acc: 0.9984375\n180 80 loss: 0.0065118190832436085 acc: 0.9988281\n180 120 loss: 0.002057385630905628 acc: 0.9984375\n180 160 loss: 0.007142331451177597 acc: 0.9970703\ntest acc: 0.7903\n181 0 loss: 0.0006623049848712981 acc: 1.0\n181 40 loss: 0.003369190264493227 acc: 0.99902344\n181 80 loss: 0.00023239993606694043 acc: 0.99912107\n181 120 loss: 0.0007281719008460641 acc: 0.9976562\n181 160 loss: 0.005422790069133043 acc: 0.99833983\ntest acc: 0.7809\n182 0 loss: 0.004322147462517023 acc: 0.99609375\n182 40 loss: 0.01150200329720974 acc: 0.9989258\n182 80 loss: 0.00024874895461834967 acc: 0.99912107\n182 120 loss: 0.001236640615388751 acc: 0.9982422\n182 160 loss: 0.00022704721777699888 acc: 0.99833983\ntest acc: 0.7898\n183 0 loss: 0.019138678908348083 acc: 0.99609375\n183 40 loss: 0.0007635683286935091 acc: 0.9986328\n183 80 loss: 0.0004619474639184773 acc: 0.99833983\n183 120 loss: 1.97270364878932e-05 acc: 0.9993164\n183 160 loss: 0.004843172617256641 acc: 0.9994141\ntest acc: 0.7934\n184 0 loss: 0.00016085479001048952 acc: 1.0\n184 40 loss: 0.0002929447218775749 acc: 0.99960935\n184 80 loss: 0.0070029073394834995 acc: 0.9986328\n184 120 loss: 0.0016694929217919707 acc: 0.99902344\n184 160 loss: 0.002442461671307683 acc: 0.9980469\ntest acc: 0.7827\n185 0 loss: 0.028611373156309128 acc: 0.98828125\n185 40 loss: 0.0039789131842553616 acc: 0.99853516\n185 80 loss: 0.0008496934897266328 acc: 0.99794924\n185 120 loss: 0.021385803818702698 acc: 0.9975586\n185 160 loss: 0.0012015821412205696 acc: 0.9962891\ntest acc: 0.7872\n186 0 loss: 0.02129433862864971 acc: 0.9921875\n186 40 loss: 0.019553007557988167 acc: 0.9981445\n186 80 loss: 0.005171182565391064 acc: 0.9975586\n186 120 loss: 0.007994772866368294 acc: 0.9984375\n186 160 loss: 0.0005364062963053584 acc: 0.99912107\ntest acc: 0.7929\n187 0 loss: 0.00026112093473784626 acc: 1.0\n187 40 loss: 0.01486237347126007 acc: 0.9961914\n187 80 loss: 0.004161990247666836 acc: 0.99658203\n187 120 loss: 0.007039463613182306 acc: 0.99658203\n187 160 loss: 0.0032373713329434395 acc: 0.9987305\ntest acc: 0.7931\n188 0 loss: 0.0004804843629244715 acc: 1.0\n188 40 loss: 0.008012939244508743 acc: 0.9977539\n188 80 loss: 0.026321113109588623 acc: 0.9970703\n188 120 loss: 0.0005902827251702547 acc: 0.9989258\n188 160 loss: 0.00044116255594417453 acc: 0.99833983\ntest acc: 0.7952\n189 0 loss: 0.0006147820968180895 acc: 1.0\n189 40 loss: 0.004351723939180374 acc: 0.994043\n189 80 loss: 0.011094692163169384 acc: 0.9975586\n189 120 loss: 0.006557289976626635 acc: 0.99833983\n189 160 loss: 0.0004311519442126155 acc: 0.9981445\ntest acc: 0.7908\n190 0 loss: 0.00023766845697537065 acc: 1.0\n190 40 loss: 0.009731899946928024 acc: 0.99785155\n190 80 loss: 0.007069965824484825 acc: 0.9982422\n190 120 loss: 0.0002690129913389683 acc: 0.99833983\n190 160 loss: 0.010982589796185493 acc: 0.9982422\ntest acc: 0.7928\n191 0 loss: 0.0064367190934717655 acc: 0.99609375\n191 40 loss: 0.023909147828817368 acc: 0.9987305\n191 80 loss: 0.0019875401630997658 acc: 0.99794924\n191 120 loss: 0.0014025025302544236 acc: 0.9987305\n191 160 loss: 0.0030696557369083166 acc: 0.99833983\ntest acc: 0.787\n192 0 loss: 0.007029357831925154 acc: 0.99609375\n192 40 loss: 0.003907086327672005 acc: 0.9988281\n192 80 loss: 0.004431527573615313 acc: 0.9993164\n192 120 loss: 0.004155534785240889 acc: 0.9984375\n192 160 loss: 0.00037112238351255655 acc: 0.99912107\ntest acc: 0.7827\n193 0 loss: 0.006106193643063307 acc: 0.99609375\n193 40 loss: 0.0002962703292723745 acc: 0.99833983\n193 80 loss: 0.006293651647865772 acc: 0.9989258\n193 120 loss: 0.0049087959341704845 acc: 0.9982422\n193 160 loss: 0.00030985160265117884 acc: 0.99716794\ntest acc: 0.793\n194 0 loss: 9.481372399022803e-05 acc: 1.0\n194 40 loss: 0.0034416147973388433 acc: 0.9975586\n194 80 loss: 0.0010148701258003712 acc: 0.99726564\n194 120 loss: 0.0017185284523293376 acc: 0.99785155\n194 160 loss: 0.00026169445482082665 acc: 0.99794924\ntest acc: 0.7923\n195 0 loss: 0.0017047730507329106 acc: 1.0\n195 40 loss: 0.009478055872023106 acc: 0.99794924\n195 80 loss: 0.0008242379990406334 acc: 0.99921876\n195 120 loss: 0.0029458210337907076 acc: 0.9988281\n195 160 loss: 0.0026000437792390585 acc: 0.99833983\ntest acc: 0.7906\n196 0 loss: 0.0004577405925374478 acc: 1.0\n196 40 loss: 0.0006477923016063869 acc: 0.9981445\n196 80 loss: 9.0770838141907e-05 acc: 0.99921876\n196 120 loss: 0.013136930763721466 acc: 0.9980469\n196 160 loss: 0.0026792739517986774 acc: 0.99726564\ntest acc: 0.7996\n197 0 loss: 6.690534064546227e-05 acc: 1.0\n197 40 loss: 8.607828931417316e-05 acc: 0.9998047\n197 80 loss: 0.00011479573731776327 acc: 1.0\n197 120 loss: 0.00859627965837717 acc: 0.9998047\n197 160 loss: 0.00021998208831064403 acc: 0.9995117\ntest acc: 0.7903\n198 0 loss: 6.108196976128966e-05 acc: 1.0\n198 40 loss: 0.00028059256146661937 acc: 0.9994141\n198 80 loss: 0.00020827207481488585 acc: 0.9988281\n198 120 loss: 0.03046354092657566 acc: 0.99785155\n198 160 loss: 0.014243816025555134 acc: 0.9952148\ntest acc: 0.7886\n199 0 loss: 0.0030252174474298954 acc: 1.0\n199 40 loss: 0.0011223361361771822 acc: 0.99833983\n199 80 loss: 0.0014947839081287384 acc: 0.9981445\n199 120 loss: 0.0032915412448346615 acc: 0.9980469\n199 160 loss: 0.00123143894597888 acc: 0.9984375\ntest acc: 0.7937\n200 0 loss: 0.007246157620102167 acc: 0.99609375\n200 40 loss: 0.0003715747152455151 acc: 0.9989258\n200 80 loss: 0.00021546721109189093 acc: 0.9994141\n200 120 loss: 0.00016927000251598656 acc: 0.99912107\n200 160 loss: 0.0017129267798736691 acc: 0.99677736\ntest acc: 0.7931\n201 0 loss: 0.002836206927895546 acc: 1.0\n201 40 loss: 0.0007067460683174431 acc: 0.99912107\n201 80 loss: 0.002820734865963459 acc: 0.9982422\n201 120 loss: 0.0010746014304459095 acc: 0.9981445\n201 160 loss: 0.0010841661132872105 acc: 0.99853516\ntest acc: 0.7921\n202 0 loss: 0.0065425788052380085 acc: 0.99609375\n202 40 loss: 0.00026675607659853995 acc: 0.9982422\n202 80 loss: 0.0009709149599075317 acc: 0.9993164\n202 120 loss: 0.0004770644591189921 acc: 0.9994141\n202 160 loss: 0.012963500805199146 acc: 0.99736327\ntest acc: 0.788\n203 0 loss: 0.0060713752172887325 acc: 0.99609375\n203 40 loss: 0.023977041244506836 acc: 0.9977539\n203 80 loss: 0.00048000505194067955 acc: 0.9981445\n203 120 loss: 0.009898909367620945 acc: 0.9976562\n203 160 loss: 0.0007566040731035173 acc: 0.99726564\ntest acc: 0.7892\n204 0 loss: 0.004187747836112976 acc: 0.99609375\n204 40 loss: 0.004061521962285042 acc: 0.99716794\n204 80 loss: 0.0018803916173055768 acc: 0.9982422\n204 120 loss: 0.027236931025981903 acc: 0.99853516\n204 160 loss: 0.0009565411019138992 acc: 0.9989258\ntest acc: 0.7949\n205 0 loss: 0.0005726338713429868 acc: 1.0\n205 40 loss: 0.005863744765520096 acc: 0.9986328\n205 80 loss: 0.0008412068127654493 acc: 0.99912107\n205 120 loss: 0.006747703533619642 acc: 0.9964844\n205 160 loss: 0.04241766035556793 acc: 0.9987305\ntest acc: 0.7928\n206 0 loss: 0.0003558214521035552 acc: 1.0\n206 40 loss: 0.011684706434607506 acc: 0.99902344\n206 80 loss: 0.000542444409802556 acc: 0.9981445\n206 120 loss: 0.00019024498760700226 acc: 0.9982422\n206 160 loss: 0.0005276624578982592 acc: 0.9976562\ntest acc: 0.792\n207 0 loss: 0.0004023289948236197 acc: 1.0\n207 40 loss: 0.00017157907132059336 acc: 0.9976562\n207 80 loss: 0.007024403661489487 acc: 0.9982422\n207 120 loss: 0.013719787821173668 acc: 0.9980469\n207 160 loss: 0.002481679432094097 acc: 0.99833983\ntest acc: 0.7918\n208 0 loss: 0.0008933903882279992 acc: 1.0\n208 40 loss: 0.014958732761442661 acc: 0.99677736\n208 80 loss: 0.004487155936658382 acc: 0.99736327\n208 120 loss: 0.0005559566197916865 acc: 0.996875\n208 160 loss: 0.001480491366237402 acc: 0.9988281\ntest acc: 0.7959\n209 0 loss: 6.1701939557679e-05 acc: 1.0\n209 40 loss: 2.4559461962780915e-05 acc: 0.9989258\n209 80 loss: 0.0007804845809005201 acc: 0.99912107\n209 120 loss: 0.0013105344260111451 acc: 0.99902344\n209 160 loss: 0.0002602935128379613 acc: 0.9984375\ntest acc: 0.7869\n210 0 loss: 0.0015412622597068548 acc: 1.0\n210 40 loss: 0.002863951725885272 acc: 0.99746096\n210 80 loss: 0.005715453065931797 acc: 0.9987305\n210 120 loss: 0.0009607039391994476 acc: 0.9988281\n210 160 loss: 0.006254793144762516 acc: 0.9987305\ntest acc: 0.7874\n211 0 loss: 0.0033976843114942312 acc: 1.0\n211 40 loss: 0.005520829930901527 acc: 0.9977539\n211 80 loss: 0.00022700897534377873 acc: 0.9987305\n211 120 loss: 3.827307591564022e-05 acc: 0.99921876\n211 160 loss: 0.0002914022479671985 acc: 0.9988281\ntest acc: 0.7912\n212 0 loss: 0.00011481661931611598 acc: 1.0\n212 40 loss: 0.0019614032935351133 acc: 0.99921876\n212 80 loss: 0.006122007500380278 acc: 0.99794924\n212 120 loss: 0.0007607967709191144 acc: 0.9988281\n212 160 loss: 0.035226475447416306 acc: 0.99794924\ntest acc: 0.7924\n213 0 loss: 0.0014509008033201098 acc: 1.0\n213 40 loss: 8.389244612772018e-05 acc: 0.99794924\n213 80 loss: 0.0003786624292843044 acc: 0.9995117\n213 120 loss: 0.00044591567711904645 acc: 0.99794924\n213 160 loss: 0.012547926045954227 acc: 0.99833983\ntest acc: 0.7908\n214 0 loss: 0.00034260135726071894 acc: 1.0\n214 40 loss: 0.0005553631926886737 acc: 0.99902344\n214 80 loss: 0.004805904347449541 acc: 0.9976562\n214 120 loss: 0.0017262469045817852 acc: 0.9982422\n214 160 loss: 0.008882416412234306 acc: 0.99667966\ntest acc: 0.7915\n215 0 loss: 0.012916063889861107 acc: 0.99609375\n215 40 loss: 0.0024464617017656565 acc: 0.9975586\n215 80 loss: 0.03474941849708557 acc: 0.9984375\n215 120 loss: 0.001919076661579311 acc: 0.9989258\n215 160 loss: 0.002752904547378421 acc: 0.99960935\ntest acc: 0.7971\n216 0 loss: 5.320311174727976e-05 acc: 1.0\n216 40 loss: 0.03482285514473915 acc: 0.99912107\n216 80 loss: 0.0005318353651091456 acc: 0.9989258\n216 120 loss: 0.0010736002586781979 acc: 0.99794924\n216 160 loss: 0.006318355444818735 acc: 0.99726564\ntest acc: 0.7917\n217 0 loss: 0.002454238710924983 acc: 1.0\n217 40 loss: 0.013115731999278069 acc: 0.9989258\n217 80 loss: 0.0003414391539990902 acc: 0.9988281\n217 120 loss: 0.0009496306302025914 acc: 0.9986328\n217 160 loss: 0.00036325084511190653 acc: 0.99921876\ntest acc: 0.8027\n218 0 loss: 4.157367584411986e-05 acc: 1.0\n218 40 loss: 0.002638777019456029 acc: 0.9987305\n218 80 loss: 0.0031580058857798576 acc: 0.9988281\n218 120 loss: 0.0012137467274442315 acc: 0.99833983\n218 160 loss: 0.0014818212948739529 acc: 0.9989258\ntest acc: 0.7898\n219 0 loss: 0.0003125018847640604 acc: 1.0\n219 40 loss: 0.0029864718671888113 acc: 0.9981445\n219 80 loss: 0.0069148968905210495 acc: 0.99853516\n219 120 loss: 0.0014495415380224586 acc: 0.9982422\n219 160 loss: 0.00023131692432798445 acc: 0.9975586\ntest acc: 0.7878\n220 0 loss: 0.0002587097987998277 acc: 1.0\n220 40 loss: 0.0032346211373806 acc: 0.99912107\n220 80 loss: 0.00022631634783465415 acc: 0.99902344\n220 120 loss: 0.016791122034192085 acc: 0.9982422\n220 160 loss: 0.004133438225835562 acc: 0.99716794\ntest acc: 0.784\n221 0 loss: 0.0029527468141168356 acc: 1.0\n221 40 loss: 0.0025473651476204395 acc: 0.9987305\n221 80 loss: 0.0056698014959692955 acc: 0.9980469\n221 120 loss: 0.003043043427169323 acc: 0.9964844\n221 160 loss: 0.006891967728734016 acc: 0.99667966\ntest acc: 0.7919\n222 0 loss: 0.007116648368537426 acc: 0.99609375\n222 40 loss: 0.0002548364573158324 acc: 0.9984375\n222 80 loss: 0.02607993222773075 acc: 0.9986328\n222 120 loss: 0.006694052368402481 acc: 0.9986328\n222 160 loss: 0.012385412119328976 acc: 0.99833983\ntest acc: 0.804\n223 0 loss: 4.563328548101708e-05 acc: 1.0\n223 40 loss: 0.0001924811367643997 acc: 0.99902344\n223 80 loss: 0.005595226306468248 acc: 0.9987305\n223 120 loss: 0.0006050391239114106 acc: 0.99960935\n223 160 loss: 0.0002562921436037868 acc: 0.99960935\ntest acc: 0.7986\n224 0 loss: 6.384527659974992e-05 acc: 1.0\n224 40 loss: 0.0009214123128913343 acc: 0.99990237\n224 80 loss: 0.00012555975990835577 acc: 1.0\n224 120 loss: 3.601276330300607e-05 acc: 0.99990237\n224 160 loss: 0.00020820769714191556 acc: 1.0\ntest acc: 0.7979\n225 0 loss: 1.4571405699825846e-05 acc: 1.0\n225 40 loss: 0.00027392234187573195 acc: 0.99960935\n225 80 loss: 0.0003402921138331294 acc: 0.99912107\n225 120 loss: 0.005556056275963783 acc: 0.9989258\n225 160 loss: 0.003594143781810999 acc: 0.99658203\ntest acc: 0.7901\n226 0 loss: 0.005320174619555473 acc: 1.0\n226 40 loss: 0.0007838128367438912 acc: 0.9988281\n226 80 loss: 0.004578161519020796 acc: 0.9986328\n226 120 loss: 0.014844771474599838 acc: 0.9982422\n226 160 loss: 0.015016308054327965 acc: 0.99667966\ntest acc: 0.7899\n227 0 loss: 0.0006233195308595896 acc: 1.0\n227 40 loss: 0.0011872428003698587 acc: 0.9982422\n227 80 loss: 0.00041938244248740375 acc: 0.99833983\n227 120 loss: 0.00453039538115263 acc: 0.9981445\n227 160 loss: 0.00090534653281793 acc: 0.99794924\ntest acc: 0.7895\n228 0 loss: 3.4676228096941486e-05 acc: 1.0\n228 40 loss: 0.008402765728533268 acc: 0.9976562\n228 80 loss: 0.0005678644520230591 acc: 0.9984375\n228 120 loss: 0.003342800075188279 acc: 0.9981445\n228 160 loss: 0.018082330003380775 acc: 0.9987305\ntest acc: 0.7924\n229 0 loss: 0.0025914048310369253 acc: 1.0\n229 40 loss: 0.0006762153934687376 acc: 0.99970704\n229 80 loss: 0.002693280577659607 acc: 0.9988281\n229 120 loss: 0.018074916675686836 acc: 0.9986328\n229 160 loss: 0.0020939393434673548 acc: 0.99492186\ntest acc: 0.7976\n230 0 loss: 0.0022656871005892754 acc: 1.0\n230 40 loss: 0.002061335137113929 acc: 0.9988281\n230 80 loss: 0.030177447944879532 acc: 0.9987305\n230 120 loss: 7.2590570198372e-05 acc: 0.99853516\n230 160 loss: 0.0004908875562250614 acc: 0.9988281\ntest acc: 0.787\n231 0 loss: 0.003470230847597122 acc: 1.0\n231 40 loss: 0.004867255222052336 acc: 0.9970703\n231 80 loss: 0.0025012746918946505 acc: 0.9984375\n231 120 loss: 0.02275991067290306 acc: 0.99833983\n231 160 loss: 0.0005231858813203871 acc: 0.9987305\ntest acc: 0.7948\n232 0 loss: 0.00012371185584925115 acc: 1.0\n232 40 loss: 1.2797287126886658e-05 acc: 0.99990237\n232 80 loss: 0.0016528781270608306 acc: 0.99921876\n232 120 loss: 7.554683543276042e-05 acc: 0.9988281\n232 160 loss: 2.552405021560844e-05 acc: 0.9988281\ntest acc: 0.7974\n233 0 loss: 0.00017098266107495874 acc: 1.0\n233 40 loss: 1.2974375749763567e-05 acc: 0.99960935\n233 80 loss: 0.0009979394963011146 acc: 0.9994141\n233 120 loss: 0.0011962901335209608 acc: 0.99921876\n233 160 loss: 0.0002522570139262825 acc: 0.99902344\ntest acc: 0.7887\n234 0 loss: 0.00036082061706110835 acc: 1.0\n234 40 loss: 0.000510965590365231 acc: 0.9989258\n234 80 loss: 0.004469662439078093 acc: 0.9986328\n234 120 loss: 0.02149498276412487 acc: 0.9969727\n234 160 loss: 0.0006757666124030948 acc: 0.9961914\ntest acc: 0.7889\n235 0 loss: 0.001030359766446054 acc: 1.0\n235 40 loss: 0.00038658810080960393 acc: 0.9981445\n235 80 loss: 0.0014585559256374836 acc: 0.99921876\n235 120 loss: 0.0009525520144961774 acc: 0.9995117\n235 160 loss: 0.0002458928502164781 acc: 0.99902344\ntest acc: 0.7962\n236 0 loss: 0.01047573797404766 acc: 0.99609375\n236 40 loss: 0.0003861712757498026 acc: 0.9998047\n236 80 loss: 1.0524051504035015e-05 acc: 0.9994141\n236 120 loss: 0.00045453349594026804 acc: 0.99902344\n236 160 loss: 0.0021698581986129284 acc: 0.99921876\ntest acc: 0.7872\n237 0 loss: 0.00047281390288844705 acc: 1.0\n237 40 loss: 0.0007001480553299189 acc: 0.99921876\n237 80 loss: 0.0008305773371830583 acc: 0.9988281\n237 120 loss: 0.002162937307730317 acc: 0.99736327\n237 160 loss: 0.009148608893156052 acc: 0.9961914\ntest acc: 0.7818\n238 0 loss: 0.0011340646306052804 acc: 1.0\n238 40 loss: 0.008738154545426369 acc: 0.9981445\n238 80 loss: 0.00971598643809557 acc: 0.99833983\n238 120 loss: 0.0056562600657343864 acc: 0.99785155\n238 160 loss: 0.00011375304165994748 acc: 0.9976562\ntest acc: 0.792\n239 0 loss: 0.004381933715194464 acc: 0.99609375\n239 40 loss: 0.00044327182695269585 acc: 0.9988281\n239 80 loss: 0.01890546642243862 acc: 0.99746096\n239 120 loss: 0.000999543583020568 acc: 0.99746096\n239 160 loss: 0.009921655058860779 acc: 0.9982422\ntest acc: 0.7944\n240 0 loss: 0.0002086627355311066 acc: 1.0\n240 40 loss: 7.869950059102848e-05 acc: 0.9998047\n240 80 loss: 0.0003176631289534271 acc: 0.9995117\n240 120 loss: 0.0033713881857693195 acc: 0.99853516\n240 160 loss: 0.00041853846050798893 acc: 0.9986328\ntest acc: 0.7874\n241 0 loss: 0.0026903494726866484 acc: 1.0\n241 40 loss: 0.0015488561475649476 acc: 0.99853516\n241 80 loss: 0.0044441609643399715 acc: 0.9989258\n241 120 loss: 1.8631169950822368e-05 acc: 0.99960935\n241 160 loss: 6.205443060025573e-05 acc: 0.9988281\ntest acc: 0.7927\n242 0 loss: 0.0004836803418584168 acc: 1.0\n242 40 loss: 0.0001613193890079856 acc: 0.9994141\n242 80 loss: 1.608881211723201e-05 acc: 0.99921876\n242 120 loss: 4.08938794862479e-05 acc: 0.9994141\n242 160 loss: 0.0004420598561409861 acc: 0.9993164\ntest acc: 0.7914\n243 0 loss: 0.05197246000170708 acc: 0.9921875\n243 40 loss: 0.05045844614505768 acc: 0.99658203\n243 80 loss: 0.007116831373423338 acc: 0.9981445\n243 120 loss: 0.00131367112044245 acc: 0.9975586\n243 160 loss: 0.0006277372594922781 acc: 0.99658203\ntest acc: 0.8006\n244 0 loss: 0.00048332629376091063 acc: 1.0\n244 40 loss: 5.3228704928187653e-05 acc: 0.9989258\n244 80 loss: 0.02461179718375206 acc: 0.9977539\n244 120 loss: 0.0010220640106126666 acc: 0.9981445\n244 160 loss: 0.002740113064646721 acc: 0.9987305\ntest acc: 0.7954\n245 0 loss: 0.000572085496969521 acc: 1.0\n245 40 loss: 7.649316103197634e-05 acc: 0.9989258\n245 80 loss: 0.009881420992314816 acc: 0.9989258\n245 120 loss: 0.0009424222516827285 acc: 0.99902344\n245 160 loss: 0.003798051504418254 acc: 0.9987305\ntest acc: 0.787\n246 0 loss: 0.0006255022017285228 acc: 1.0\n246 40 loss: 0.00977006834000349 acc: 0.99785155\n246 80 loss: 0.0012367608724161983 acc: 0.99609375\n246 120 loss: 0.002736473921686411 acc: 0.99902344\n246 160 loss: 2.5806448320508935e-05 acc: 0.9977539\ntest acc: 0.7912\n247 0 loss: 0.000324419786920771 acc: 1.0\n247 40 loss: 0.001006235950626433 acc: 0.99736327\n247 80 loss: 0.00012630126730073243 acc: 0.99902344\n247 120 loss: 8.832466846797615e-05 acc: 0.99912107\n247 160 loss: 0.00043313283822499216 acc: 0.9995117\ntest acc: 0.8024\n248 0 loss: 0.00011258150334469974 acc: 1.0\n248 40 loss: 0.00010387849761173129 acc: 0.9998047\n248 80 loss: 0.0008005393319763243 acc: 0.9998047\n248 120 loss: 0.01014317013323307 acc: 0.9998047\n248 160 loss: 0.00037410834920592606 acc: 0.9988281\ntest acc: 0.7977\n249 0 loss: 0.0004629493923857808 acc: 1.0\n249 40 loss: 8.441712270723656e-05 acc: 0.99902344\n249 80 loss: 0.00025057222228497267 acc: 0.99833983\n249 120 loss: 0.0002013592020375654 acc: 0.99833983\n249 160 loss: 0.0005876730429008603 acc: 0.99902344\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}