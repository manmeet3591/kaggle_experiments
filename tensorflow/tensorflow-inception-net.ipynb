{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-18T04:29:53.138273Z","iopub.execute_input":"2021-09-18T04:29:53.138629Z","iopub.status.idle":"2021-09-18T04:29:53.149766Z","shell.execute_reply.started":"2021-09-18T04:29:53.138543Z","shell.execute_reply":"2021-09-18T04:29:53.149062Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras","metadata":{"execution":{"iopub.status.busy":"2021-09-18T04:30:06.399128Z","iopub.execute_input":"2021-09-18T04:30:06.399648Z","iopub.status.idle":"2021-09-18T04:30:11.028789Z","shell.execute_reply.started":"2021-09-18T04:30:06.399611Z","shell.execute_reply":"2021-09-18T04:30:11.028045Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2021-09-18 04:30:07.003574: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","output_type":"stream"}]},{"cell_type":"code","source":"tf.random.set_seed(22)\nnp.random.seed(22)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nassert tf.__version__.startswith('2.')","metadata":{"execution":{"iopub.status.busy":"2021-09-18T04:30:21.231459Z","iopub.execute_input":"2021-09-18T04:30:21.231734Z","iopub.status.idle":"2021-09-18T04:30:21.238768Z","shell.execute_reply.started":"2021-09-18T04:30:21.231705Z","shell.execute_reply":"2021-09-18T04:30:21.238095Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train.astype(np.float32)/255., x_test.astype(np.float32)/255.\n# [b, 28, 28] => [b, 28, 28, 1]\nx_train, x_test = np.expand_dims(x_train, axis=3), np.expand_dims(x_test, axis=3)\n\ndb_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(256)\ndb_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(256)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T04:30:35.013504Z","iopub.execute_input":"2021-09-18T04:30:35.013786Z","iopub.status.idle":"2021-09-18T04:30:37.397192Z","shell.execute_reply.started":"2021-09-18T04:30:35.013754Z","shell.execute_reply":"2021-09-18T04:30:37.395202Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11493376/11490434 [==============================] - 0s 0us/step\n","output_type":"stream"},{"name":"stderr","text":"2021-09-18 04:30:35.488469: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2021-09-18 04:30:35.491909: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n2021-09-18 04:30:35.547139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-18 04:30:35.547767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \npciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\ncoreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n2021-09-18 04:30:35.547822: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n2021-09-18 04:30:35.575806: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n2021-09-18 04:30:35.575881: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n2021-09-18 04:30:35.594805: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n2021-09-18 04:30:35.628179: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n2021-09-18 04:30:35.658706: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n2021-09-18 04:30:35.666741: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n2021-09-18 04:30:35.669345: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n2021-09-18 04:30:35.669526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-18 04:30:35.670199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-18 04:30:35.671688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n2021-09-18 04:30:35.673337: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2021-09-18 04:30:35.674513: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2021-09-18 04:30:35.674678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-18 04:30:35.675287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \npciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\ncoreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n2021-09-18 04:30:35.675331: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n2021-09-18 04:30:35.675355: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n2021-09-18 04:30:35.675370: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n2021-09-18 04:30:35.675385: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n2021-09-18 04:30:35.675415: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n2021-09-18 04:30:35.675431: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n2021-09-18 04:30:35.675447: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n2021-09-18 04:30:35.675462: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n2021-09-18 04:30:35.675551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-18 04:30:35.676173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-18 04:30:35.676705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n2021-09-18 04:30:35.678126: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n2021-09-18 04:30:37.206971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n2021-09-18 04:30:37.207037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n2021-09-18 04:30:37.207048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n2021-09-18 04:30:37.209280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-18 04:30:37.210004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-18 04:30:37.210689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-18 04:30:37.211264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14957 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(x_train.shape, type(x_train))","metadata":{"execution":{"iopub.status.busy":"2021-09-18T04:31:00.698709Z","iopub.execute_input":"2021-09-18T04:31:00.698977Z","iopub.status.idle":"2021-09-18T04:31:00.704167Z","shell.execute_reply.started":"2021-09-18T04:31:00.698948Z","shell.execute_reply":"2021-09-18T04:31:00.703353Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(60000, 28, 28, 1) <class 'numpy.ndarray'>\n","output_type":"stream"}]},{"cell_type":"code","source":"print(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T04:31:14.830788Z","iopub.execute_input":"2021-09-18T04:31:14.831071Z","iopub.status.idle":"2021-09-18T04:31:14.839598Z","shell.execute_reply.started":"2021-09-18T04:31:14.831040Z","shell.execute_reply":"2021-09-18T04:31:14.838934Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"(60000, 28, 28, 1) (60000,)\n(10000, 28, 28, 1) (10000,)\n","output_type":"stream"}]},{"cell_type":"code","source":"class ConvBNRelu(keras.Model):\n    \n    def __init__(self, ch, kernelsz=3, strides=1, padding='same'):\n        super(ConvBNRelu, self).__init__()\n        \n        self.model = keras.models.Sequential([\n            keras.layers.Conv2D(ch, kernelsz, strides=strides, padding=padding),\n            keras.layers.BatchNormalization(),\n            keras.layers.ReLU()\n        ])\n        \n        \n    def call(self, x, training=None):\n        \n        x = self.model(x, training=training)\n        \n        return x ","metadata":{"execution":{"iopub.status.busy":"2021-09-18T04:31:58.678815Z","iopub.execute_input":"2021-09-18T04:31:58.679088Z","iopub.status.idle":"2021-09-18T04:31:58.685710Z","shell.execute_reply.started":"2021-09-18T04:31:58.679057Z","shell.execute_reply":"2021-09-18T04:31:58.684918Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class InceptionBlk(keras.Model):\n    \n    def __init__(self, ch, strides=1):\n        super(InceptionBlk, self).__init__()\n        \n        self.ch = ch\n        self.strides = strides\n\n        self.conv1 = ConvBNRelu(ch, strides=strides)\n        self.conv2 = ConvBNRelu(ch, kernelsz=3, strides=strides)\n        self.conv3_1 = ConvBNRelu(ch, kernelsz=3, strides=strides)\n        self.conv3_2 = ConvBNRelu(ch, kernelsz=3, strides=1)\n        \n        self.pool = keras.layers.MaxPooling2D(3, strides=1, padding='same')\n        self.pool_conv = ConvBNRelu(ch, strides=strides)\n        \n        \n    def call(self, x, training=None):\n        \n        \n        x1 = self.conv1(x, training=training)\n\n        x2 = self.conv2(x, training=training)\n                \n        x3_1 = self.conv3_1(x, training=training)\n        x3_2 = self.conv3_2(x3_1, training=training)\n                \n        x4 = self.pool(x)\n        x4 = self.pool_conv(x4, training=training)\n        \n        # concat along axis=channel\n        x = tf.concat([x1, x2, x3_2, x4], axis=3)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2021-09-18T04:36:42.883584Z","iopub.execute_input":"2021-09-18T04:36:42.883844Z","iopub.status.idle":"2021-09-18T04:36:42.892998Z","shell.execute_reply.started":"2021-09-18T04:36:42.883815Z","shell.execute_reply":"2021-09-18T04:36:42.892091Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Inception(keras.Model):\n    \n    def __init__(self, num_layers, num_classes, init_ch=16, **kwargs):\n        super(Inception, self).__init__(**kwargs)\n        \n        self.in_channels = init_ch\n        self.out_channels = init_ch\n        self.num_layers = num_layers\n        self.init_ch = init_ch\n        \n        self.conv1 = ConvBNRelu(init_ch)\n        \n        self.blocks = keras.models.Sequential(name='dynamic-blocks')\n        \n        for block_id in range(num_layers):\n            \n            for layer_id in range(2):\n                \n                if layer_id == 0:\n                    \n                    block = InceptionBlk(self.out_channels, strides=2)\n                    \n                else:\n                    block = InceptionBlk(self.out_channels, strides=1)\n                    \n                self.blocks.add(block)\n            \n            # enlarger out_channels per block    \n            self.out_channels *= 2\n            \n        self.avg_pool = keras.layers.GlobalAveragePooling2D()\n        self.fc = keras.layers.Dense(num_classes)\n        \n        \n    def call(self, x, training=None):\n        \n        out = self.conv1(x, training=training)\n        \n        out = self.blocks(out, training=training)\n        \n        out = self.avg_pool(out)\n        out = self.fc(out)\n        \n        return out    ","metadata":{"execution":{"iopub.status.busy":"2021-09-18T04:37:34.277471Z","iopub.execute_input":"2021-09-18T04:37:34.277736Z","iopub.status.idle":"2021-09-18T04:37:34.287077Z","shell.execute_reply.started":"2021-09-18T04:37:34.277706Z","shell.execute_reply":"2021-09-18T04:37:34.286109Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# build model and optimizer\nbatch_size = 32\nepochs = 100\nmodel = Inception(2, 10)\n# derive input shape for every layers.\nmodel.build(input_shape=(None, 28, 28, 1))\nmodel.summary()\n\noptimizer = keras.optimizers.Adam(learning_rate=1e-3)\ncriteon = keras.losses.CategoricalCrossentropy(from_logits=True)\n\nacc_meter = keras.metrics.Accuracy()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T04:38:27.693381Z","iopub.execute_input":"2021-09-18T04:38:27.693640Z","iopub.status.idle":"2021-09-18T04:38:28.933103Z","shell.execute_reply.started":"2021-09-18T04:38:27.693611Z","shell.execute_reply":"2021-09-18T04:38:28.932401Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Model: \"inception\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv_bn_relu (ConvBNRelu)    multiple                  224       \n_________________________________________________________________\ndynamic-blocks (Sequential)  (None, 7, 7, 128)         292704    \n_________________________________________________________________\nglobal_average_pooling2d (Gl multiple                  0         \n_________________________________________________________________\ndense (Dense)                multiple                  1290      \n=================================================================\nTotal params: 294,218\nTrainable params: 293,226\nNon-trainable params: 992\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(100):\n\n    for step, (x, y) in enumerate(db_train):\n\n        with tf.GradientTape() as tape:\n            # print(x.shape, y.shape)\n            # [b, 10]\n            logits = model(x)\n            # [b] vs [b, 10]\n            loss = criteon(tf.one_hot(y, depth=10), logits)\n\n        grads = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n        if step % 10 == 0:\n            print(epoch, step, 'loss:', loss.numpy())\n\n\n    acc_meter.reset_states()\n    for x, y in db_test:\n        # [b, 10]\n        logits = model(x, training=False)\n        # [b, 10] => [b]\n        pred = tf.argmax(logits, axis=1)\n        # [b] vs [b, 10]\n        acc_meter.update_state(y, pred)\n\n    print(epoch, 'evaluation acc:', acc_meter.result().numpy())","metadata":{"execution":{"iopub.status.busy":"2021-09-18T04:38:50.738886Z","iopub.execute_input":"2021-09-18T04:38:50.739564Z","iopub.status.idle":"2021-09-18T05:06:03.519409Z","shell.execute_reply.started":"2021-09-18T04:38:50.739526Z","shell.execute_reply":"2021-09-18T05:06:03.518645Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"2021-09-18 04:38:50.963871: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n2021-09-18 04:38:56.117363: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n2021-09-18 04:38:56.848805: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n","output_type":"stream"},{"name":"stdout","text":"0 0 loss: 2.3067927\n0 10 loss: 2.1030269\n0 20 loss: 1.5699424\n0 30 loss: 1.3794844\n0 40 loss: 0.9233192\n0 50 loss: 0.8301166\n0 60 loss: 0.5781583\n0 70 loss: 0.35383868\n0 80 loss: 0.56293434\n0 90 loss: 0.4557651\n0 100 loss: 0.39482278\n0 110 loss: 0.3665469\n0 120 loss: 0.33006558\n0 130 loss: 0.20413844\n0 140 loss: 0.2431204\n0 150 loss: 0.21798\n0 160 loss: 0.23220327\n0 170 loss: 0.18612988\n0 180 loss: 0.30192673\n0 190 loss: 0.13770968\n0 200 loss: 0.2680411\n0 210 loss: 0.17704089\n0 220 loss: 0.16800207\n0 230 loss: 0.036778033\n0 evaluation acc: 0.9539\n1 0 loss: 0.17814125\n1 10 loss: 0.15480125\n1 20 loss: 0.2226929\n1 30 loss: 0.26450413\n1 40 loss: 0.34233713\n1 50 loss: 0.2944184\n1 60 loss: 0.22319412\n1 70 loss: 0.12525977\n1 80 loss: 0.14425951\n1 90 loss: 0.12286508\n1 100 loss: 0.16167735\n1 110 loss: 0.21939492\n1 120 loss: 0.119434066\n1 130 loss: 0.12410558\n1 140 loss: 0.13017684\n1 150 loss: 0.10345536\n1 160 loss: 0.1109103\n1 170 loss: 0.08285951\n1 180 loss: 0.16340841\n1 190 loss: 0.06397399\n1 200 loss: 0.13432372\n1 210 loss: 0.120642684\n1 220 loss: 0.17711012\n1 230 loss: 0.008996003\n1 evaluation acc: 0.9647\n2 0 loss: 0.11860058\n2 10 loss: 0.124603234\n2 20 loss: 0.123314574\n2 30 loss: 0.1552553\n2 40 loss: 0.15103222\n2 50 loss: 0.105213515\n2 60 loss: 0.23782863\n2 70 loss: 0.12669763\n2 80 loss: 0.10285472\n2 90 loss: 0.08091356\n2 100 loss: 0.07981502\n2 110 loss: 0.11074647\n2 120 loss: 0.07956472\n2 130 loss: 0.1312347\n2 140 loss: 0.07650067\n2 150 loss: 0.13992138\n2 160 loss: 0.07846276\n2 170 loss: 0.06871519\n2 180 loss: 0.11577436\n2 190 loss: 0.05298968\n2 200 loss: 0.08310844\n2 210 loss: 0.08639879\n2 220 loss: 0.10610679\n2 230 loss: 0.0082346415\n2 evaluation acc: 0.9792\n3 0 loss: 0.07766408\n3 10 loss: 0.12776148\n3 20 loss: 0.09600549\n3 30 loss: 0.11545778\n3 40 loss: 0.09345624\n3 50 loss: 0.050548133\n3 60 loss: 0.1282134\n3 70 loss: 0.08410532\n3 80 loss: 0.08377013\n3 90 loss: 0.056569524\n3 100 loss: 0.054311886\n3 110 loss: 0.05585887\n3 120 loss: 0.056583203\n3 130 loss: 0.11503066\n3 140 loss: 0.069516346\n3 150 loss: 0.10582651\n3 160 loss: 0.053893194\n3 170 loss: 0.068582624\n3 180 loss: 0.07266177\n3 190 loss: 0.024565432\n3 200 loss: 0.06856845\n3 210 loss: 0.06880678\n3 220 loss: 0.09158788\n3 230 loss: 0.006612623\n3 evaluation acc: 0.9743\n4 0 loss: 0.07867999\n4 10 loss: 0.08022136\n4 20 loss: 0.054319173\n4 30 loss: 0.07840061\n4 40 loss: 0.07239738\n4 50 loss: 0.047358748\n4 60 loss: 0.11137362\n4 70 loss: 0.09330805\n4 80 loss: 0.07679327\n4 90 loss: 0.03379924\n4 100 loss: 0.043185465\n4 110 loss: 0.035686266\n4 120 loss: 0.04398813\n4 130 loss: 0.08545553\n4 140 loss: 0.054392032\n4 150 loss: 0.07267473\n4 160 loss: 0.06252536\n4 170 loss: 0.06564927\n4 180 loss: 0.05073774\n4 190 loss: 0.017015275\n4 200 loss: 0.05575989\n4 210 loss: 0.058024973\n4 220 loss: 0.0855534\n4 230 loss: 0.008056836\n4 evaluation acc: 0.9767\n5 0 loss: 0.06866842\n5 10 loss: 0.10028709\n5 20 loss: 0.04313211\n5 30 loss: 0.07743082\n5 40 loss: 0.040193744\n5 50 loss: 0.038463082\n5 60 loss: 0.10090688\n5 70 loss: 0.08329534\n5 80 loss: 0.061731905\n5 90 loss: 0.022956705\n5 100 loss: 0.056619067\n5 110 loss: 0.027099311\n5 120 loss: 0.04860618\n5 130 loss: 0.066741504\n5 140 loss: 0.045677062\n5 150 loss: 0.05504298\n5 160 loss: 0.06579293\n5 170 loss: 0.049695104\n5 180 loss: 0.04838889\n5 190 loss: 0.014339036\n5 200 loss: 0.047696587\n5 210 loss: 0.044577003\n5 220 loss: 0.064619854\n5 230 loss: 0.0016434984\n5 evaluation acc: 0.9893\n6 0 loss: 0.025852658\n6 10 loss: 0.08826248\n6 20 loss: 0.032587375\n6 30 loss: 0.06326892\n6 40 loss: 0.025150357\n6 50 loss: 0.030853705\n6 60 loss: 0.062250927\n6 70 loss: 0.06882226\n6 80 loss: 0.06033294\n6 90 loss: 0.018193098\n6 100 loss: 0.04621449\n6 110 loss: 0.016696194\n6 120 loss: 0.04319129\n6 130 loss: 0.0544447\n6 140 loss: 0.04143585\n6 150 loss: 0.044037346\n6 160 loss: 0.056828648\n6 170 loss: 0.036441233\n6 180 loss: 0.031662233\n6 190 loss: 0.0148841655\n6 200 loss: 0.040567502\n6 210 loss: 0.046035748\n6 220 loss: 0.036838774\n6 230 loss: 0.0007999977\n6 evaluation acc: 0.9914\n7 0 loss: 0.018590588\n7 10 loss: 0.089033596\n7 20 loss: 0.022219991\n7 30 loss: 0.046070587\n7 40 loss: 0.032630783\n7 50 loss: 0.02757011\n7 60 loss: 0.04473725\n7 70 loss: 0.06411733\n7 80 loss: 0.05871225\n7 90 loss: 0.007414188\n7 100 loss: 0.029945238\n7 110 loss: 0.021816017\n7 120 loss: 0.029190153\n7 130 loss: 0.05196512\n7 140 loss: 0.025433494\n7 150 loss: 0.048596855\n7 160 loss: 0.049796965\n7 170 loss: 0.03197189\n7 180 loss: 0.023427708\n7 190 loss: 0.011991035\n7 200 loss: 0.035929188\n7 210 loss: 0.03316153\n7 220 loss: 0.034102835\n7 230 loss: 0.0016783936\n7 evaluation acc: 0.9893\n8 0 loss: 0.01759916\n8 10 loss: 0.08406664\n8 20 loss: 0.02375801\n8 30 loss: 0.03330309\n8 40 loss: 0.051247794\n8 50 loss: 0.030390814\n8 60 loss: 0.048933\n8 70 loss: 0.056556422\n8 80 loss: 0.059747897\n8 90 loss: 0.005939094\n8 100 loss: 0.031044185\n8 110 loss: 0.03835625\n8 120 loss: 0.02260328\n8 130 loss: 0.052110188\n8 140 loss: 0.04917224\n8 150 loss: 0.036607534\n8 160 loss: 0.058727548\n8 170 loss: 0.026822604\n8 180 loss: 0.045680143\n8 190 loss: 0.011926948\n8 200 loss: 0.02759424\n8 210 loss: 0.01997622\n8 220 loss: 0.026326967\n8 230 loss: 0.00024197757\n8 evaluation acc: 0.9872\n9 0 loss: 0.02032712\n9 10 loss: 0.07568863\n9 20 loss: 0.024756199\n9 30 loss: 0.023688631\n9 40 loss: 0.04780095\n9 50 loss: 0.026176585\n9 60 loss: 0.037338573\n9 70 loss: 0.059288654\n9 80 loss: 0.04747723\n9 90 loss: 0.0045796796\n9 100 loss: 0.037921045\n9 110 loss: 0.0154773295\n9 120 loss: 0.030226978\n9 130 loss: 0.045119464\n9 140 loss: 0.04189795\n9 150 loss: 0.023508435\n9 160 loss: 0.04931826\n9 170 loss: 0.027859282\n9 180 loss: 0.039760526\n9 190 loss: 0.013867279\n9 200 loss: 0.023036143\n9 210 loss: 0.027915806\n9 220 loss: 0.021583306\n9 230 loss: 0.00017543245\n9 evaluation acc: 0.9896\n10 0 loss: 0.022961993\n10 10 loss: 0.06664962\n10 20 loss: 0.024833994\n10 30 loss: 0.017620914\n10 40 loss: 0.069975935\n10 50 loss: 0.040129825\n10 60 loss: 0.03624596\n10 70 loss: 0.049344473\n10 80 loss: 0.04310937\n10 90 loss: 0.0025864772\n10 100 loss: 0.029080404\n10 110 loss: 0.014238904\n10 120 loss: 0.05056954\n10 130 loss: 0.031673342\n10 140 loss: 0.022665836\n10 150 loss: 0.012176833\n10 160 loss: 0.021527248\n10 170 loss: 0.030191744\n10 180 loss: 0.020106414\n10 190 loss: 0.008038583\n10 200 loss: 0.024536563\n10 210 loss: 0.006861889\n10 220 loss: 0.02181385\n10 230 loss: 2.9397295e-05\n10 evaluation acc: 0.9922\n11 0 loss: 0.016128115\n11 10 loss: 0.04888744\n11 20 loss: 0.017707815\n11 30 loss: 0.01575962\n11 40 loss: 0.07442266\n11 50 loss: 0.038252532\n11 60 loss: 0.04693734\n11 70 loss: 0.04966612\n11 80 loss: 0.043625977\n11 90 loss: 0.006317072\n11 100 loss: 0.028423337\n11 110 loss: 0.0123566585\n11 120 loss: 0.036041174\n11 130 loss: 0.03605926\n11 140 loss: 0.018502332\n11 150 loss: 0.014348504\n11 160 loss: 0.03714135\n11 170 loss: 0.014262178\n11 180 loss: 0.025005471\n11 190 loss: 0.0023623977\n11 200 loss: 0.020085871\n11 210 loss: 0.012614403\n11 220 loss: 0.020684902\n11 230 loss: 5.2735144e-05\n11 evaluation acc: 0.9914\n12 0 loss: 0.011007025\n12 10 loss: 0.064102255\n12 20 loss: 0.019224903\n12 30 loss: 0.01779686\n12 40 loss: 0.04772967\n12 50 loss: 0.02669859\n12 60 loss: 0.037455972\n12 70 loss: 0.050734926\n12 80 loss: 0.018890193\n12 90 loss: 0.016025832\n12 100 loss: 0.027689248\n12 110 loss: 0.021231556\n12 120 loss: 0.028404448\n12 130 loss: 0.024710108\n12 140 loss: 0.024808729\n12 150 loss: 0.010533391\n12 160 loss: 0.029650487\n12 170 loss: 0.010705344\n12 180 loss: 0.018224558\n12 190 loss: 0.0046208072\n12 200 loss: 0.015526761\n12 210 loss: 0.008521537\n12 220 loss: 0.017066281\n12 230 loss: 6.9612026e-05\n12 evaluation acc: 0.9871\n13 0 loss: 0.02802733\n13 10 loss: 0.04774884\n13 20 loss: 0.015558026\n13 30 loss: 0.013907514\n13 40 loss: 0.03631444\n13 50 loss: 0.021611143\n13 60 loss: 0.040221617\n13 70 loss: 0.025118984\n13 80 loss: 0.017071716\n13 90 loss: 0.0028410344\n13 100 loss: 0.017034067\n13 110 loss: 0.011230032\n13 120 loss: 0.02980667\n13 130 loss: 0.027810894\n13 140 loss: 0.016378222\n13 150 loss: 0.010403375\n13 160 loss: 0.015770523\n13 170 loss: 0.01351442\n13 180 loss: 0.0064667054\n13 190 loss: 0.007405625\n13 200 loss: 0.0330092\n13 210 loss: 0.010792655\n13 220 loss: 0.014572225\n13 230 loss: 2.5986852e-05\n13 evaluation acc: 0.9913\n14 0 loss: 0.01513678\n14 10 loss: 0.028952464\n14 20 loss: 0.04433733\n14 30 loss: 0.0090091955\n14 40 loss: 0.015038052\n14 50 loss: 0.027093824\n14 60 loss: 0.015755653\n14 70 loss: 0.032317255\n14 80 loss: 0.007925127\n14 90 loss: 0.002076696\n14 100 loss: 0.012177944\n14 110 loss: 0.004759728\n14 120 loss: 0.028892469\n14 130 loss: 0.029053448\n14 140 loss: 0.0074712173\n14 150 loss: 0.019041348\n14 160 loss: 0.006926333\n14 170 loss: 0.02940126\n14 180 loss: 0.016531978\n14 190 loss: 0.002987998\n14 200 loss: 0.023138303\n14 210 loss: 0.005761662\n14 220 loss: 0.012797075\n14 230 loss: 1.045295e-05\n14 evaluation acc: 0.99\n15 0 loss: 0.010697711\n15 10 loss: 0.024838526\n15 20 loss: 0.01952235\n15 30 loss: 0.0025015448\n15 40 loss: 0.028166115\n15 50 loss: 0.036431264\n15 60 loss: 0.021078983\n15 70 loss: 0.024355\n15 80 loss: 0.003820084\n15 90 loss: 0.0027252084\n15 100 loss: 0.017278232\n15 110 loss: 0.0015134644\n15 120 loss: 0.0073477617\n15 130 loss: 0.023250679\n15 140 loss: 0.0082811825\n15 150 loss: 0.019765675\n15 160 loss: 0.019270474\n15 170 loss: 0.042367574\n15 180 loss: 0.009053161\n15 190 loss: 0.018107971\n15 200 loss: 0.021721067\n15 210 loss: 0.0074050827\n15 220 loss: 0.0074464628\n15 230 loss: 4.77228e-06\n15 evaluation acc: 0.9906\n16 0 loss: 0.007535076\n16 10 loss: 0.0073219296\n16 20 loss: 0.010584413\n16 30 loss: 0.0029080187\n16 40 loss: 0.015124223\n16 50 loss: 0.022699399\n16 60 loss: 0.023350798\n16 70 loss: 0.024573233\n16 80 loss: 0.009076834\n16 90 loss: 0.0168442\n16 100 loss: 0.016766375\n16 110 loss: 0.009635508\n16 120 loss: 0.019377299\n16 130 loss: 0.023418888\n16 140 loss: 0.0121192895\n16 150 loss: 0.016435288\n16 160 loss: 0.0071305656\n16 170 loss: 0.038461797\n16 180 loss: 0.004305674\n16 190 loss: 0.0003474765\n16 200 loss: 0.025720732\n16 210 loss: 0.010753978\n16 220 loss: 0.012139869\n16 230 loss: 2.8272249e-05\n16 evaluation acc: 0.9916\n17 0 loss: 0.007031818\n17 10 loss: 0.0077350237\n17 20 loss: 0.009998937\n17 30 loss: 0.0057225125\n17 40 loss: 0.004313349\n17 50 loss: 0.0075038723\n17 60 loss: 0.012606835\n17 70 loss: 0.02540319\n17 80 loss: 0.0041261674\n17 90 loss: 0.0021090952\n17 100 loss: 0.017296087\n17 110 loss: 0.018255036\n17 120 loss: 0.016711816\n17 130 loss: 0.009619037\n17 140 loss: 0.0039103124\n17 150 loss: 0.0073272632\n17 160 loss: 0.00539829\n17 170 loss: 0.017252391\n17 180 loss: 0.006147815\n17 190 loss: 0.00023922339\n17 200 loss: 0.017463956\n17 210 loss: 0.0013601101\n17 220 loss: 0.0070137014\n17 230 loss: 3.9606555e-05\n17 evaluation acc: 0.9917\n18 0 loss: 0.0034316648\n18 10 loss: 0.020044062\n18 20 loss: 0.0031273218\n18 30 loss: 0.024902029\n18 40 loss: 0.014448253\n18 50 loss: 0.031981234\n18 60 loss: 0.017228587\n18 70 loss: 0.0054950924\n18 80 loss: 0.008656129\n18 90 loss: 0.0006935479\n18 100 loss: 0.021166898\n18 110 loss: 0.005659248\n18 120 loss: 0.0066775046\n18 130 loss: 0.033837736\n18 140 loss: 0.0025478827\n18 150 loss: 0.011024437\n18 160 loss: 0.00361984\n18 170 loss: 0.008302604\n18 180 loss: 0.01943595\n18 190 loss: 0.0008024663\n18 200 loss: 0.01585869\n18 210 loss: 0.003414617\n18 220 loss: 0.0030511133\n18 230 loss: 0.00034684435\n18 evaluation acc: 0.9906\n19 0 loss: 0.011214821\n19 10 loss: 0.008199519\n19 20 loss: 0.014292908\n19 30 loss: 0.011398914\n19 40 loss: 0.0024255435\n19 50 loss: 0.009237949\n19 60 loss: 0.008353265\n19 70 loss: 0.005776017\n19 80 loss: 0.007657011\n19 90 loss: 0.0026880435\n19 100 loss: 0.012579565\n19 110 loss: 0.0055852165\n19 120 loss: 0.02676716\n19 130 loss: 0.016917147\n19 140 loss: 0.0012291841\n19 150 loss: 0.006052849\n19 160 loss: 0.0102267815\n19 170 loss: 0.010349166\n19 180 loss: 0.0016045939\n19 190 loss: 0.0022046333\n19 200 loss: 0.010531035\n19 210 loss: 0.0013938099\n19 220 loss: 0.0025825787\n19 230 loss: 7.4257555e-06\n19 evaluation acc: 0.991\n20 0 loss: 0.0024261444\n20 10 loss: 0.015428757\n20 20 loss: 0.014383832\n20 30 loss: 0.014762265\n20 40 loss: 0.0019084718\n20 50 loss: 0.014289719\n20 60 loss: 0.000780043\n20 70 loss: 0.018237472\n20 80 loss: 0.0015449002\n20 90 loss: 0.0006078974\n20 100 loss: 0.00857175\n20 110 loss: 0.00535694\n20 120 loss: 0.0135567\n20 130 loss: 0.02211583\n20 140 loss: 0.011843634\n20 150 loss: 0.02068132\n20 160 loss: 0.008850034\n20 170 loss: 0.0078601\n20 180 loss: 0.00072071084\n20 190 loss: 0.0007085414\n20 200 loss: 0.028420819\n20 210 loss: 0.01570309\n20 220 loss: 0.0066939658\n20 230 loss: 1.9618567e-05\n20 evaluation acc: 0.9918\n21 0 loss: 0.013175552\n21 10 loss: 0.0007046479\n21 20 loss: 0.0070141377\n21 30 loss: 0.0017915305\n21 40 loss: 0.0003990683\n21 50 loss: 0.018948616\n21 60 loss: 0.014158758\n21 70 loss: 0.014758581\n21 80 loss: 0.0017336826\n21 90 loss: 0.0044602854\n21 100 loss: 0.024112526\n21 110 loss: 0.0118015\n21 120 loss: 0.03215171\n21 130 loss: 0.020482829\n21 140 loss: 0.011438699\n21 150 loss: 0.011316928\n21 160 loss: 0.009568662\n21 170 loss: 0.0075230054\n21 180 loss: 0.004981915\n21 190 loss: 0.0063111195\n21 200 loss: 0.0035196356\n21 210 loss: 0.001897915\n21 220 loss: 0.001280255\n21 230 loss: 0.00030958946\n21 evaluation acc: 0.9899\n22 0 loss: 0.008031672\n22 10 loss: 0.014331634\n22 20 loss: 0.008039896\n22 30 loss: 0.004282345\n22 40 loss: 0.018036714\n22 50 loss: 0.016018353\n22 60 loss: 0.019751804\n22 70 loss: 0.009987818\n22 80 loss: 0.0037985696\n22 90 loss: 0.041280635\n22 100 loss: 0.008376549\n22 110 loss: 0.0044193985\n22 120 loss: 0.028855905\n22 130 loss: 0.008252491\n22 140 loss: 0.004244278\n22 150 loss: 0.008611326\n22 160 loss: 0.026259288\n22 170 loss: 0.01015063\n22 180 loss: 0.0047476557\n22 190 loss: 0.000631285\n22 200 loss: 0.006215901\n22 210 loss: 0.0061264527\n22 220 loss: 0.007759004\n22 230 loss: 0.000172191\n22 evaluation acc: 0.9924\n23 0 loss: 0.015147513\n23 10 loss: 0.010267375\n23 20 loss: 0.0076647103\n23 30 loss: 0.013432069\n23 40 loss: 0.007258449\n23 50 loss: 0.008403765\n23 60 loss: 0.0116509255\n23 70 loss: 0.0028379008\n23 80 loss: 0.0070381844\n23 90 loss: 0.00052623125\n23 100 loss: 0.019684767\n23 110 loss: 0.007776269\n23 120 loss: 0.012711987\n23 130 loss: 0.006929\n23 140 loss: 0.0008634148\n23 150 loss: 0.006758344\n23 160 loss: 0.0115024\n23 170 loss: 0.01590903\n23 180 loss: 0.0017558773\n23 190 loss: 0.00014084231\n23 200 loss: 0.008447105\n23 210 loss: 0.0018563885\n23 220 loss: 0.002340652\n23 230 loss: 3.191274e-05\n23 evaluation acc: 0.9927\n24 0 loss: 0.008256816\n24 10 loss: 0.013628285\n24 20 loss: 0.021375192\n24 30 loss: 0.00074278365\n24 40 loss: 0.0015126786\n24 50 loss: 0.0017708673\n24 60 loss: 0.0014938009\n24 70 loss: 0.0005854255\n24 80 loss: 0.019543786\n24 90 loss: 0.0015554021\n24 100 loss: 0.008093612\n24 110 loss: 0.002558318\n24 120 loss: 0.028139668\n24 130 loss: 0.01221523\n24 140 loss: 0.0066995346\n24 150 loss: 0.0048233857\n24 160 loss: 0.012159379\n24 170 loss: 0.011398326\n24 180 loss: 0.0018083501\n24 190 loss: 0.002041947\n24 200 loss: 0.027692897\n24 210 loss: 0.010613515\n24 220 loss: 0.012109088\n24 230 loss: 1.5116502e-05\n24 evaluation acc: 0.9905\n25 0 loss: 0.010462906\n25 10 loss: 0.007945138\n25 20 loss: 0.006685281\n25 30 loss: 0.011390703\n25 40 loss: 0.006987889\n25 50 loss: 0.037475478\n25 60 loss: 0.0017318312\n25 70 loss: 0.0043371967\n25 80 loss: 0.004084783\n25 90 loss: 0.0052014254\n25 100 loss: 0.0055143726\n25 110 loss: 0.0007397015\n25 120 loss: 0.009505289\n25 130 loss: 0.0105817905\n25 140 loss: 0.013703638\n25 150 loss: 0.013278702\n25 160 loss: 0.0011763955\n25 170 loss: 6.277091e-05\n25 180 loss: 0.0010072992\n25 190 loss: 0.00062980613\n25 200 loss: 0.005423916\n25 210 loss: 0.0013592537\n25 220 loss: 0.0017262059\n25 230 loss: 4.065111e-07\n25 evaluation acc: 0.9908\n26 0 loss: 0.010234102\n26 10 loss: 0.0014686255\n26 20 loss: 0.0025748366\n26 30 loss: 0.00041556772\n26 40 loss: 0.00010803515\n26 50 loss: 0.0004836218\n26 60 loss: 0.0052615497\n26 70 loss: 0.004426648\n26 80 loss: 0.0015054769\n26 90 loss: 0.0010294126\n26 100 loss: 0.009728888\n26 110 loss: 0.0001363193\n26 120 loss: 0.0007837175\n26 130 loss: 0.006853513\n26 140 loss: 0.003198387\n26 150 loss: 0.0021287084\n26 160 loss: 0.001198453\n26 170 loss: 0.002841014\n26 180 loss: 0.0046765776\n26 190 loss: 0.001472368\n26 200 loss: 0.016299244\n26 210 loss: 0.0048686797\n26 220 loss: 0.002669434\n26 230 loss: 4.273753e-06\n26 evaluation acc: 0.9894\n27 0 loss: 0.022926815\n27 10 loss: 0.023397446\n27 20 loss: 0.009641494\n27 30 loss: 0.0011576151\n27 40 loss: 0.008800122\n27 50 loss: 0.008929054\n27 60 loss: 0.009611166\n27 70 loss: 0.004245868\n27 80 loss: 0.0025617196\n27 90 loss: 0.0009859226\n27 100 loss: 0.004874669\n27 110 loss: 0.027162896\n27 120 loss: 0.007091465\n27 130 loss: 0.0063102813\n27 140 loss: 0.0052403416\n27 150 loss: 0.0022928987\n27 160 loss: 0.0048466367\n27 170 loss: 0.00095021666\n27 180 loss: 0.0042610136\n27 190 loss: 0.012154534\n27 200 loss: 0.053809874\n27 210 loss: 0.0094329305\n27 220 loss: 0.019733528\n27 230 loss: 7.022012e-07\n27 evaluation acc: 0.9929\n28 0 loss: 0.0038917803\n28 10 loss: 0.009353878\n28 20 loss: 0.0010703653\n28 30 loss: 0.00041054914\n28 40 loss: 0.014015747\n28 50 loss: 0.0009499659\n28 60 loss: 0.0006299338\n28 70 loss: 0.001795941\n28 80 loss: 0.0013718596\n28 90 loss: 2.2785427e-05\n28 100 loss: 0.0064649885\n28 110 loss: 0.001099889\n28 120 loss: 0.010929189\n28 130 loss: 0.0088518895\n28 140 loss: 0.0049719163\n28 150 loss: 0.010523807\n28 160 loss: 0.0003984906\n28 170 loss: 0.0053926804\n28 180 loss: 0.0017606124\n28 190 loss: 4.2903037e-05\n28 200 loss: 0.0013836252\n28 210 loss: 0.0035171316\n28 220 loss: 0.00067084987\n28 230 loss: 2.6728705e-07\n28 evaluation acc: 0.9899\n29 0 loss: 0.0038924061\n29 10 loss: 0.0045772884\n29 20 loss: 0.00033759093\n29 30 loss: 8.811101e-05\n29 40 loss: 0.0035676216\n29 50 loss: 0.0283351\n29 60 loss: 0.00023677564\n29 70 loss: 8.379711e-05\n29 80 loss: 0.0027734577\n29 90 loss: 0.009289228\n29 100 loss: 0.002409196\n29 110 loss: 0.007649615\n29 120 loss: 0.0116564315\n29 130 loss: 0.0035263984\n29 140 loss: 0.004948218\n29 150 loss: 0.011769274\n29 160 loss: 0.009902562\n29 170 loss: 0.020515827\n29 180 loss: 0.01612035\n29 190 loss: 0.0022045986\n29 200 loss: 0.0007393182\n29 210 loss: 0.0012770605\n29 220 loss: 0.0044079283\n29 230 loss: 3.558899e-05\n29 evaluation acc: 0.9922\n30 0 loss: 0.0033261455\n30 10 loss: 0.0005883706\n30 20 loss: 0.0028820806\n30 30 loss: 0.014336735\n30 40 loss: 0.024550905\n30 50 loss: 0.021060497\n30 60 loss: 0.0029818492\n30 70 loss: 9.8212564e-05\n30 80 loss: 0.00016745293\n30 90 loss: 5.6692224e-05\n30 100 loss: 0.006100835\n30 110 loss: 0.007906335\n30 120 loss: 0.0063930163\n30 130 loss: 0.013550396\n30 140 loss: 0.0021114722\n30 150 loss: 0.0032279654\n30 160 loss: 0.021878157\n30 170 loss: 0.0012090711\n30 180 loss: 0.016004372\n30 190 loss: 0.0019418974\n30 200 loss: 0.004008972\n30 210 loss: 0.003320768\n30 220 loss: 0.0027551882\n30 230 loss: 8.026095e-06\n30 evaluation acc: 0.9937\n31 0 loss: 0.0070101772\n31 10 loss: 0.011005677\n31 20 loss: 0.0021770599\n31 30 loss: 0.009756176\n31 40 loss: 0.0009599962\n31 50 loss: 0.004092845\n31 60 loss: 0.00016204084\n31 70 loss: 0.00049669406\n31 80 loss: 0.0007213038\n31 90 loss: 0.0011310298\n31 100 loss: 0.0065496266\n31 110 loss: 0.0003442117\n31 120 loss: 0.00072355574\n31 130 loss: 0.012753457\n31 140 loss: 0.0046527977\n31 150 loss: 0.00083770667\n31 160 loss: 0.009340188\n31 170 loss: 0.00023057118\n31 180 loss: 0.0027497476\n31 190 loss: 0.00571212\n31 200 loss: 0.014558274\n31 210 loss: 0.0025039315\n31 220 loss: 0.0067499224\n31 230 loss: 0.00017404396\n31 evaluation acc: 0.9918\n32 0 loss: 0.0012954973\n32 10 loss: 0.00061582297\n32 20 loss: 0.0022167289\n32 30 loss: 0.0036097814\n32 40 loss: 0.053828645\n32 50 loss: 0.02542013\n32 60 loss: 0.023807362\n32 70 loss: 0.093486816\n32 80 loss: 0.0058289752\n32 90 loss: 0.004024225\n32 100 loss: 0.006440218\n32 110 loss: 0.0005382522\n32 120 loss: 0.0046274317\n32 130 loss: 0.0051965797\n32 140 loss: 0.0013773661\n32 150 loss: 0.00043243845\n32 160 loss: 0.0017821976\n32 170 loss: 0.002462383\n32 180 loss: 0.0007402579\n32 190 loss: 0.0022905672\n32 200 loss: 0.010664534\n32 210 loss: 0.0020726514\n32 220 loss: 0.005985528\n32 230 loss: 4.375938e-06\n32 evaluation acc: 0.9917\n33 0 loss: 0.0062385183\n33 10 loss: 0.0013659942\n33 20 loss: 0.002003414\n33 30 loss: 8.9885696e-05\n33 40 loss: 7.911755e-05\n33 50 loss: 0.0006344112\n33 60 loss: 0.00031388557\n33 70 loss: 0.024658332\n33 80 loss: 0.003683048\n33 90 loss: 0.00919476\n33 100 loss: 0.01073949\n33 110 loss: 0.001702336\n33 120 loss: 0.0024833155\n33 130 loss: 0.0012623975\n33 140 loss: 0.00021609369\n33 150 loss: 0.0035444603\n33 160 loss: 0.0006865773\n33 170 loss: 1.514213e-05\n33 180 loss: 0.0005836126\n33 190 loss: 0.0023217567\n33 200 loss: 0.017338706\n33 210 loss: 0.0008708277\n33 220 loss: 0.0061823903\n33 230 loss: 3.46517e-05\n33 evaluation acc: 0.9925\n34 0 loss: 0.009831505\n34 10 loss: 0.0003554133\n34 20 loss: 0.003013648\n34 30 loss: 0.00024963776\n34 40 loss: 0.000247703\n34 50 loss: 0.00570636\n34 60 loss: 0.0031756083\n34 70 loss: 0.00036273297\n34 80 loss: 0.0014799158\n34 90 loss: 1.6568032e-05\n34 100 loss: 0.0005166236\n34 110 loss: 0.001386073\n34 120 loss: 0.0082773045\n34 130 loss: 0.0019510766\n34 140 loss: 0.0076781823\n34 150 loss: 0.0074801045\n34 160 loss: 0.001489743\n34 170 loss: 0.0007604354\n34 180 loss: 0.005953001\n34 190 loss: 0.00016870917\n34 200 loss: 0.0031138614\n34 210 loss: 0.08640819\n34 220 loss: 0.01248136\n34 230 loss: 0.0016466316\n34 evaluation acc: 0.9932\n35 0 loss: 0.009322179\n35 10 loss: 0.005861075\n35 20 loss: 0.002147376\n35 30 loss: 0.006782002\n35 40 loss: 0.0023977826\n35 50 loss: 0.00069295836\n35 60 loss: 0.0064935377\n35 70 loss: 0.0004346937\n35 80 loss: 0.017684823\n35 90 loss: 0.00044922103\n35 100 loss: 0.0010914139\n35 110 loss: 0.0010541233\n35 120 loss: 0.002039832\n35 130 loss: 7.999357e-05\n35 140 loss: 0.00056385197\n35 150 loss: 0.0035862108\n35 160 loss: 0.00013915099\n35 170 loss: 0.0020175462\n35 180 loss: 0.00014543475\n35 190 loss: 0.00025331794\n35 200 loss: 0.0073323417\n35 210 loss: 0.0002984326\n35 220 loss: 0.005487892\n35 230 loss: 6.134047e-05\n35 evaluation acc: 0.9854\n36 0 loss: 0.013060142\n36 10 loss: 0.0002738925\n36 20 loss: 0.0016641766\n36 30 loss: 0.002855803\n36 40 loss: 0.014200642\n36 50 loss: 0.0012226707\n36 60 loss: 0.0026735354\n36 70 loss: 0.0036809375\n36 80 loss: 0.003186884\n36 90 loss: 3.7352664e-05\n36 100 loss: 0.005972574\n36 110 loss: 0.019235138\n36 120 loss: 0.00032000034\n36 130 loss: 0.0055397577\n36 140 loss: 0.00064182904\n36 150 loss: 0.0046750423\n36 160 loss: 0.0017436884\n36 170 loss: 0.004036936\n36 180 loss: 0.0028255978\n36 190 loss: 0.00010688761\n36 200 loss: 0.004253552\n36 210 loss: 0.0016022521\n36 220 loss: 0.01697223\n36 230 loss: 0.00019312621\n36 evaluation acc: 0.9901\n37 0 loss: 0.009622732\n37 10 loss: 0.0005718826\n37 20 loss: 0.0053884042\n37 30 loss: 0.001987589\n37 40 loss: 0.00055495644\n37 50 loss: 0.005177958\n37 60 loss: 0.0009174478\n37 70 loss: 0.0044344366\n37 80 loss: 0.0035950455\n37 90 loss: 0.0005832218\n37 100 loss: 4.0841867e-05\n37 110 loss: 0.0003179894\n37 120 loss: 0.00084321597\n37 130 loss: 0.00074848183\n37 140 loss: 1.3266634e-05\n37 150 loss: 0.00015454211\n37 160 loss: 0.00034657598\n37 170 loss: 0.00027744018\n37 180 loss: 0.0013167185\n37 190 loss: 1.4074682e-05\n37 200 loss: 0.009446789\n37 210 loss: 0.0009646912\n37 220 loss: 2.3306246e-05\n37 230 loss: 0.00015707675\n37 evaluation acc: 0.9893\n38 0 loss: 0.033330098\n38 10 loss: 0.024514157\n38 20 loss: 0.004053131\n38 30 loss: 0.018212762\n38 40 loss: 0.0034101564\n38 50 loss: 0.0006871953\n38 60 loss: 0.0018238131\n38 70 loss: 0.00017805761\n38 80 loss: 0.0011223113\n38 90 loss: 0.0056265583\n38 100 loss: 0.030753626\n38 110 loss: 0.042111997\n38 120 loss: 0.021319041\n38 130 loss: 0.0018714217\n38 140 loss: 0.02777294\n38 150 loss: 0.004197946\n38 160 loss: 0.0033729142\n38 170 loss: 0.0004436303\n38 180 loss: 0.0008545956\n38 190 loss: 0.00017687884\n38 200 loss: 0.002466584\n38 210 loss: 0.00048484138\n38 220 loss: 0.002913563\n38 230 loss: 8.034744e-06\n38 evaluation acc: 0.9942\n39 0 loss: 0.0001232906\n39 10 loss: 0.004096143\n39 20 loss: 0.0027970953\n39 30 loss: 0.00082725444\n39 40 loss: 5.0985105e-05\n39 50 loss: 8.32644e-05\n39 60 loss: 0.00012911095\n39 70 loss: 0.013415085\n39 80 loss: 7.956124e-05\n39 90 loss: 6.941846e-05\n39 100 loss: 0.0040877587\n39 110 loss: 0.0017756234\n39 120 loss: 0.0027549204\n39 130 loss: 0.00025623478\n39 140 loss: 0.009369067\n39 150 loss: 0.00040679998\n39 160 loss: 0.000107173706\n39 170 loss: 8.741727e-05\n39 180 loss: 6.1361985e-05\n39 190 loss: 0.0006428484\n39 200 loss: 0.00069278333\n39 210 loss: 0.00087690324\n39 220 loss: 0.00066500256\n39 230 loss: 9.1539005e-07\n39 evaluation acc: 0.9929\n40 0 loss: 5.5743403e-05\n40 10 loss: 0.0005359047\n40 20 loss: 0.0067992066\n40 30 loss: 0.0046661766\n40 40 loss: 6.793125e-05\n40 50 loss: 0.01498862\n40 60 loss: 5.905333e-05\n40 70 loss: 0.01227216\n40 80 loss: 0.00046248036\n40 90 loss: 0.0064246384\n40 100 loss: 4.148336e-05\n40 110 loss: 0.00029214902\n40 120 loss: 0.025291435\n40 130 loss: 0.000983086\n40 140 loss: 0.00045785581\n40 150 loss: 0.025048925\n40 160 loss: 0.0006979235\n40 170 loss: 5.3311327e-05\n40 180 loss: 0.009147516\n40 190 loss: 0.00028677547\n40 200 loss: 0.015338323\n40 210 loss: 0.0051305993\n40 220 loss: 0.00012987928\n40 230 loss: 4.530704e-07\n40 evaluation acc: 0.9929\n41 0 loss: 0.0013517066\n41 10 loss: 5.794591e-05\n41 20 loss: 5.409173e-05\n41 30 loss: 5.9133865e-05\n41 40 loss: 5.5590943e-05\n41 50 loss: 7.6227836e-05\n41 60 loss: 0.0011291116\n41 70 loss: 8.111805e-05\n41 80 loss: 0.00011906836\n41 90 loss: 0.003131136\n41 100 loss: 0.009144147\n41 110 loss: 0.006106368\n41 120 loss: 0.0021412047\n41 130 loss: 0.013815011\n41 140 loss: 0.0049862214\n41 150 loss: 0.0043987636\n41 160 loss: 0.00047210985\n41 170 loss: 0.0014670097\n41 180 loss: 0.0034851192\n41 190 loss: 0.00012211544\n41 200 loss: 0.00043346177\n41 210 loss: 0.00024797258\n41 220 loss: 0.004003975\n41 230 loss: 7.100692e-07\n41 evaluation acc: 0.9923\n42 0 loss: 0.0011622384\n42 10 loss: 0.00573311\n42 20 loss: 0.00060445355\n42 30 loss: 0.0019927921\n42 40 loss: 0.00022281021\n42 50 loss: 0.00021951049\n42 60 loss: 0.0004996804\n42 70 loss: 5.176845e-05\n42 80 loss: 0.00042559236\n42 90 loss: 8.083611e-07\n42 100 loss: 0.0022993912\n42 110 loss: 2.797958e-05\n42 120 loss: 6.5616616e-05\n42 130 loss: 0.0010924813\n42 140 loss: 0.00010916002\n42 150 loss: 0.00035761044\n42 160 loss: 0.0017924551\n42 170 loss: 0.006714953\n42 180 loss: 0.012913328\n42 190 loss: 0.00029010352\n42 200 loss: 0.0018502709\n42 210 loss: 0.0553409\n42 220 loss: 0.0008195583\n42 230 loss: 2.7124259e-05\n42 evaluation acc: 0.9902\n43 0 loss: 0.014122752\n43 10 loss: 0.0039586658\n43 20 loss: 0.029702256\n43 30 loss: 0.004898243\n43 40 loss: 0.04497628\n43 50 loss: 0.0024572157\n43 60 loss: 0.00025171007\n43 70 loss: 0.0008963094\n43 80 loss: 0.003948748\n43 90 loss: 0.00064617285\n43 100 loss: 0.00093521137\n43 110 loss: 0.008264861\n43 120 loss: 0.0004975132\n43 130 loss: 0.00055859017\n43 140 loss: 0.00015194592\n43 150 loss: 0.0015427305\n43 160 loss: 0.00040811123\n43 170 loss: 0.000316214\n43 180 loss: 5.5058073e-05\n43 190 loss: 0.00071868574\n43 200 loss: 0.0019845401\n43 210 loss: 0.00025630998\n43 220 loss: 0.00024978953\n43 230 loss: 2.4027642e-07\n43 evaluation acc: 0.9924\n44 0 loss: 0.00031736094\n44 10 loss: 0.046638396\n44 20 loss: 0.021290261\n44 30 loss: 0.014570211\n44 40 loss: 3.0267473e-05\n44 50 loss: 0.0077525377\n44 60 loss: 0.0023599209\n44 70 loss: 0.00047428714\n44 80 loss: 0.0031707576\n44 90 loss: 0.00036581824\n44 100 loss: 0.0001199902\n44 110 loss: 4.405365e-05\n44 120 loss: 0.0001933923\n44 130 loss: 0.0074222707\n44 140 loss: 0.0003169908\n44 150 loss: 0.006368824\n44 160 loss: 0.0170315\n44 170 loss: 0.00015641286\n44 180 loss: 4.3231405e-05\n44 190 loss: 0.0005580932\n44 200 loss: 3.5033885e-05\n44 210 loss: 6.147983e-05\n44 220 loss: 0.0003815635\n44 230 loss: 2.421434e-08\n44 evaluation acc: 0.9926\n45 0 loss: 0.008643278\n45 10 loss: 0.0001217409\n45 20 loss: 0.00041721607\n45 30 loss: 0.00011598869\n45 40 loss: 0.00039739057\n45 50 loss: 0.0025346829\n45 60 loss: 2.8506589e-05\n45 70 loss: 0.00025148084\n45 80 loss: 0.00030833625\n45 90 loss: 0.00018273162\n45 100 loss: 0.0019026789\n45 110 loss: 0.000108112436\n45 120 loss: 0.0008582974\n45 130 loss: 0.0014110315\n45 140 loss: 0.0001227895\n45 150 loss: 2.8863007e-05\n45 160 loss: 2.3488885e-05\n45 170 loss: 0.0004563991\n45 180 loss: 0.004010342\n45 190 loss: 1.3084874e-05\n45 200 loss: 0.002316665\n45 210 loss: 0.001246932\n45 220 loss: 0.00017739712\n45 230 loss: 7.3568935e-07\n45 evaluation acc: 0.9836\n46 0 loss: 0.040313765\n46 10 loss: 0.00067721715\n46 20 loss: 0.0028856024\n46 30 loss: 7.422141e-05\n46 40 loss: 9.580054e-05\n46 50 loss: 9.11223e-05\n46 60 loss: 0.0007790324\n46 70 loss: 0.0046604266\n46 80 loss: 0.00024735674\n46 90 loss: 0.0150958765\n46 100 loss: 0.010653463\n46 110 loss: 0.0008562235\n46 120 loss: 0.00038374768\n46 130 loss: 0.0051214737\n46 140 loss: 0.0040298593\n46 150 loss: 0.0042714556\n46 160 loss: 4.224536e-05\n46 170 loss: 0.004297475\n46 180 loss: 9.9185505e-05\n46 190 loss: 0.011703544\n46 200 loss: 0.00084781594\n46 210 loss: 0.0029509512\n46 220 loss: 0.00023537713\n46 230 loss: 7.0310244e-07\n46 evaluation acc: 0.9928\n47 0 loss: 0.0010177835\n47 10 loss: 0.0010073379\n47 20 loss: 0.00012896185\n47 30 loss: 0.0014962781\n47 40 loss: 9.77496e-05\n47 50 loss: 0.015679143\n47 60 loss: 0.0005290011\n47 70 loss: 7.935343e-05\n47 80 loss: 0.0005139757\n47 90 loss: 0.00034898167\n47 100 loss: 0.00090754457\n47 110 loss: 0.002333931\n47 120 loss: 0.012884157\n47 130 loss: 0.059124857\n47 140 loss: 0.00049190543\n47 150 loss: 0.0037345695\n47 160 loss: 6.5507054e-05\n47 170 loss: 0.0002573144\n47 180 loss: 0.0014229927\n47 190 loss: 1.2482839e-05\n47 200 loss: 0.0061866785\n47 210 loss: 0.0016782006\n47 220 loss: 0.00068196096\n47 230 loss: 4.7184944e-06\n47 evaluation acc: 0.9936\n48 0 loss: 0.00016720996\n48 10 loss: 0.0002232699\n48 20 loss: 0.00048956764\n48 30 loss: 7.57985e-05\n48 40 loss: 0.0008920673\n48 50 loss: 0.0016379589\n48 60 loss: 0.000687784\n48 70 loss: 0.00016177296\n48 80 loss: 0.0026625188\n48 90 loss: 0.00045561045\n48 100 loss: 0.0014939398\n48 110 loss: 0.00944053\n48 120 loss: 0.029942239\n48 130 loss: 0.0016186214\n48 140 loss: 0.01647925\n48 150 loss: 0.007038683\n48 160 loss: 0.00031706603\n48 170 loss: 0.0062040472\n48 180 loss: 0.0010279913\n48 190 loss: 0.0006536612\n48 200 loss: 0.0026070906\n48 210 loss: 7.614538e-05\n48 220 loss: 0.0026458004\n48 230 loss: 2.4958868e-07\n48 evaluation acc: 0.9917\n49 0 loss: 0.00010018376\n49 10 loss: 5.4988413e-06\n49 20 loss: 0.0023932862\n49 30 loss: 0.00062121026\n49 40 loss: 8.855305e-06\n49 50 loss: 0.00010579497\n49 60 loss: 0.00012890852\n49 70 loss: 0.003007133\n49 80 loss: 0.0003618464\n49 90 loss: 0.00010963599\n49 100 loss: 9.992705e-06\n49 110 loss: 0.0037447987\n49 120 loss: 5.4970184e-05\n49 130 loss: 0.002740219\n49 140 loss: 6.39243e-05\n49 150 loss: 0.00011486634\n49 160 loss: 3.7880865e-05\n49 170 loss: 0.0002930034\n49 180 loss: 0.057983004\n49 190 loss: 0.0002843349\n49 200 loss: 0.037968937\n49 210 loss: 0.0021333306\n49 220 loss: 0.0029037977\n49 230 loss: 4.268639e-05\n49 evaluation acc: 0.9918\n50 0 loss: 0.0079765525\n50 10 loss: 0.0029821016\n50 20 loss: 0.00011437907\n50 30 loss: 0.0005042304\n50 40 loss: 3.1451596e-05\n50 50 loss: 0.0015682692\n50 60 loss: 0.0007997677\n50 70 loss: 0.00037682985\n50 80 loss: 0.0054827053\n50 90 loss: 0.0002486178\n50 100 loss: 0.00072014256\n50 110 loss: 0.0010678116\n50 120 loss: 0.00014319477\n50 130 loss: 1.6573093e-05\n50 140 loss: 4.9480994e-05\n50 150 loss: 0.008939817\n50 160 loss: 0.0013438616\n50 170 loss: 1.8001476e-05\n50 180 loss: 0.0021114668\n50 190 loss: 0.000343687\n50 200 loss: 3.4892597e-05\n50 210 loss: 0.0003387414\n50 220 loss: 0.00042055905\n50 230 loss: 3.3197117e-05\n50 evaluation acc: 0.9942\n51 0 loss: 0.00019564158\n51 10 loss: 4.3039516e-05\n51 20 loss: 0.0026454867\n51 30 loss: 8.605332e-05\n51 40 loss: 0.00042699205\n51 50 loss: 0.00012865862\n51 60 loss: 0.0011461822\n51 70 loss: 0.0120730065\n51 80 loss: 0.0071092737\n51 90 loss: 0.0015614368\n51 100 loss: 3.280788e-05\n51 110 loss: 0.00052071095\n51 120 loss: 0.001480614\n51 130 loss: 0.006787909\n51 140 loss: 0.0016579614\n51 150 loss: 0.00684176\n51 160 loss: 0.0062758857\n51 170 loss: 0.00068761094\n51 180 loss: 0.001884257\n51 190 loss: 0.00015060788\n51 200 loss: 5.891855e-05\n51 210 loss: 0.011621884\n51 220 loss: 0.0074851536\n51 230 loss: 1.4854483e-07\n51 evaluation acc: 0.9902\n52 0 loss: 0.029488247\n52 10 loss: 5.808757e-05\n52 20 loss: 0.001504958\n52 30 loss: 0.00014806245\n52 40 loss: 1.9534786e-05\n52 50 loss: 0.006441874\n52 60 loss: 0.00013543441\n52 70 loss: 6.340571e-06\n52 80 loss: 3.5414167e-05\n52 90 loss: 2.6030728e-05\n52 100 loss: 1.7477938e-05\n52 110 loss: 1.4769202e-05\n52 120 loss: 0.00010872852\n52 130 loss: 0.00029112137\n52 140 loss: 8.452801e-05\n52 150 loss: 0.00022819698\n52 160 loss: 0.001795358\n52 170 loss: 5.9211343e-05\n52 180 loss: 6.448361e-05\n52 190 loss: 6.814316e-06\n52 200 loss: 0.0250336\n52 210 loss: 0.020885183\n52 220 loss: 0.00010269799\n52 230 loss: 0.00012878366\n52 evaluation acc: 0.9922\n53 0 loss: 0.00021347433\n53 10 loss: 5.8667625e-05\n53 20 loss: 0.00053676835\n53 30 loss: 2.8687544e-05\n53 40 loss: 7.088358e-05\n53 50 loss: 0.0001588065\n53 60 loss: 5.7248497e-05\n53 70 loss: 5.1621577e-05\n53 80 loss: 0.00040691468\n53 90 loss: 0.00012355924\n53 100 loss: 0.00057608006\n53 110 loss: 2.2140832e-06\n53 120 loss: 5.293941e-05\n53 130 loss: 4.58839e-05\n53 140 loss: 0.00012152908\n53 150 loss: 0.0006233739\n53 160 loss: 0.005334269\n53 170 loss: 1.5721786e-05\n53 180 loss: 0.0005413986\n53 190 loss: 0.017032316\n53 200 loss: 0.0008105263\n53 210 loss: 1.7027814e-05\n53 220 loss: 9.597174e-05\n53 230 loss: 0.0074228765\n53 evaluation acc: 0.9911\n54 0 loss: 0.00023227034\n54 10 loss: 0.00023590712\n54 20 loss: 0.00053331547\n54 30 loss: 0.00076171255\n54 40 loss: 0.0032672074\n54 50 loss: 3.0273804e-05\n54 60 loss: 0.0021154303\n54 70 loss: 9.0079186e-05\n54 80 loss: 0.0006232335\n54 90 loss: 0.00014364945\n54 100 loss: 0.024340713\n54 110 loss: 0.0018163532\n54 120 loss: 0.0050963694\n54 130 loss: 0.02763821\n54 140 loss: 4.03032e-05\n54 150 loss: 0.008017678\n54 160 loss: 0.00010476897\n54 170 loss: 0.0441356\n54 180 loss: 0.0024513134\n54 190 loss: 0.000937646\n54 200 loss: 0.01183892\n54 210 loss: 0.0005116734\n54 220 loss: 0.00032193903\n54 230 loss: 2.130388e-05\n54 evaluation acc: 0.9896\n55 0 loss: 0.0016387941\n55 10 loss: 0.0109450845\n55 20 loss: 0.0014374481\n55 30 loss: 0.00024337742\n55 40 loss: 0.0035113143\n55 50 loss: 0.0117857475\n55 60 loss: 8.1229344e-05\n55 70 loss: 0.0003810359\n55 80 loss: 0.0003944621\n55 90 loss: 2.2789718e-05\n55 100 loss: 0.037409637\n55 110 loss: 6.383759e-05\n55 120 loss: 0.001328389\n55 130 loss: 0.015188707\n55 140 loss: 0.00035104077\n55 150 loss: 0.02472894\n55 160 loss: 7.685798e-05\n55 170 loss: 0.0003138794\n55 180 loss: 0.009890249\n55 190 loss: 0.0006620045\n55 200 loss: 0.0132910125\n55 210 loss: 0.00031916852\n55 220 loss: 0.0060843737\n55 230 loss: 6.6123654e-08\n55 evaluation acc: 0.9915\n56 0 loss: 0.00070014544\n56 10 loss: 0.0031806028\n56 20 loss: 0.0014081185\n56 30 loss: 0.0006471708\n56 40 loss: 0.0004894738\n56 50 loss: 7.359157e-05\n56 60 loss: 0.009494989\n56 70 loss: 0.00011131793\n56 80 loss: 0.0008441494\n56 90 loss: 0.00022065715\n56 100 loss: 2.2318582e-05\n56 110 loss: 0.00030625766\n56 120 loss: 0.0013100376\n56 130 loss: 0.0041915365\n56 140 loss: 0.00030260388\n56 150 loss: 0.0023425499\n56 160 loss: 0.0005522805\n56 170 loss: 0.0020521695\n56 180 loss: 0.00010818825\n56 190 loss: 3.3192487e-06\n56 200 loss: 0.00015278762\n56 210 loss: 0.00011099542\n56 220 loss: 0.00016196328\n56 230 loss: 1.9092097e-08\n56 evaluation acc: 0.9931\n57 0 loss: 0.0002590212\n57 10 loss: 5.0002225e-05\n57 20 loss: 0.00021704375\n57 30 loss: 0.00014424778\n57 40 loss: 0.00012520753\n57 50 loss: 8.356089e-05\n57 60 loss: 7.061815e-05\n57 70 loss: 0.00012649508\n57 80 loss: 0.0111175785\n57 90 loss: 0.00014099173\n57 100 loss: 0.0003136099\n57 110 loss: 0.00010552243\n57 120 loss: 0.0017099036\n57 130 loss: 4.0057446e-05\n57 140 loss: 0.0016416511\n57 150 loss: 0.00024733035\n57 160 loss: 0.0016042701\n57 170 loss: 3.9885508e-05\n57 180 loss: 9.039752e-05\n57 190 loss: 0.00015311496\n57 200 loss: 0.00091430475\n57 210 loss: 0.0005243729\n57 220 loss: 0.00012346845\n57 230 loss: 8.215867e-06\n57 evaluation acc: 0.9945\n58 0 loss: 0.0003930687\n58 10 loss: 0.0006272388\n58 20 loss: 1.3310999e-05\n58 30 loss: 0.018186936\n58 40 loss: 5.848414e-05\n58 50 loss: 3.818669e-06\n58 60 loss: 0.0018950496\n58 70 loss: 0.0010914889\n58 80 loss: 0.00086939055\n58 90 loss: 9.1228e-05\n58 100 loss: 4.8999893e-05\n58 110 loss: 9.516194e-05\n58 120 loss: 0.0028269484\n58 130 loss: 0.00031705844\n58 140 loss: 0.00024207849\n58 150 loss: 0.00061846233\n58 160 loss: 5.8059006e-05\n58 170 loss: 0.0001201083\n58 180 loss: 0.00029900935\n58 190 loss: 2.7156852e-05\n58 200 loss: 0.00023498335\n58 210 loss: 0.004058959\n58 220 loss: 0.0005467946\n58 230 loss: 6.5192554e-09\n58 evaluation acc: 0.994\n59 0 loss: 0.0027648886\n59 10 loss: 0.00012814577\n59 20 loss: 0.0005446663\n59 30 loss: 5.2843112e-05\n59 40 loss: 6.3672494e-05\n59 50 loss: 1.541893e-05\n59 60 loss: 2.0100651e-05\n59 70 loss: 4.1609954e-05\n59 80 loss: 2.3925479e-05\n59 90 loss: 3.0085392e-05\n59 100 loss: 6.7188366e-06\n59 110 loss: 0.00014986312\n59 120 loss: 7.660184e-06\n59 130 loss: 0.00022352401\n59 140 loss: 0.0012980208\n59 150 loss: 0.00021222806\n59 160 loss: 0.009119819\n59 170 loss: 2.0668642e-05\n59 180 loss: 0.00043101687\n59 190 loss: 0.000103338076\n59 200 loss: 2.441545e-05\n59 210 loss: 4.816539e-05\n59 220 loss: 0.00014088264\n59 230 loss: 1.4668055e-07\n59 evaluation acc: 0.9943\n60 0 loss: 0.00038862356\n60 10 loss: 3.0245537e-05\n60 20 loss: 8.1070505e-07\n60 30 loss: 5.1331886e-06\n60 40 loss: 1.7935163e-05\n60 50 loss: 9.096052e-05\n60 60 loss: 1.7749223e-06\n60 70 loss: 2.8451586e-07\n60 80 loss: 0.0021102624\n60 90 loss: 1.5594015e-06\n60 100 loss: 6.763984e-05\n60 110 loss: 5.2293482e-05\n60 120 loss: 1.7384882e-05\n60 130 loss: 0.00030593787\n60 140 loss: 1.3129539e-05\n60 150 loss: 2.865822e-05\n60 160 loss: 7.901761e-06\n60 170 loss: 2.3526818e-05\n60 180 loss: 0.0004273077\n60 190 loss: 0.02120095\n60 200 loss: 0.0011692477\n60 210 loss: 0.00031500717\n60 220 loss: 0.001024077\n60 230 loss: 1.3969836e-09\n60 evaluation acc: 0.9916\n61 0 loss: 0.00018396482\n61 10 loss: 0.0008028618\n61 20 loss: 0.00012579779\n61 30 loss: 2.1340364e-05\n61 40 loss: 0.00031129087\n61 50 loss: 0.0016864014\n61 60 loss: 0.0004752918\n61 70 loss: 0.00019645845\n61 80 loss: 0.0031029312\n61 90 loss: 0.0031986686\n61 100 loss: 0.006278155\n61 110 loss: 0.0012937153\n61 120 loss: 0.004050658\n61 130 loss: 0.0007154929\n61 140 loss: 0.0012031542\n61 150 loss: 6.8834444e-05\n61 160 loss: 5.6668385e-05\n61 170 loss: 0.000471131\n61 180 loss: 0.00018382842\n61 190 loss: 2.8916638e-05\n61 200 loss: 0.0015048734\n61 210 loss: 0.0001492275\n61 220 loss: 0.00031492693\n61 230 loss: 1.5040607e-07\n61 evaluation acc: 0.994\n62 0 loss: 0.0031223956\n62 10 loss: 0.0019534081\n62 20 loss: 0.00010663962\n62 30 loss: 0.00029817247\n62 40 loss: 0.00018261292\n62 50 loss: 6.119345e-05\n62 60 loss: 0.004211075\n62 70 loss: 4.3696464e-06\n62 80 loss: 0.00017560564\n62 90 loss: 5.2838248e-05\n62 100 loss: 2.3579232e-06\n62 110 loss: 0.00011369387\n62 120 loss: 0.004002772\n62 130 loss: 9.497622e-05\n62 140 loss: 0.0002957663\n62 150 loss: 0.0001645692\n62 160 loss: 0.00025396195\n62 170 loss: 0.00035203504\n62 180 loss: 0.00010088732\n62 190 loss: 0.0007438235\n62 200 loss: 0.0007930918\n62 210 loss: 0.021345165\n62 220 loss: 6.166442e-05\n62 230 loss: 3.7252782e-08\n62 evaluation acc: 0.9913\n63 0 loss: 0.0004041224\n63 10 loss: 0.0001798699\n63 20 loss: 0.00026224757\n63 30 loss: 0.012194877\n63 40 loss: 1.2398999e-05\n63 50 loss: 3.062754e-05\n63 60 loss: 0.0003856368\n63 70 loss: 0.0008217391\n63 80 loss: 0.008469495\n63 90 loss: 0.0007191384\n63 100 loss: 2.1420074e-05\n63 110 loss: 0.0011284867\n63 120 loss: 0.00958129\n63 130 loss: 5.8764428e-05\n63 140 loss: 0.014476866\n63 150 loss: 8.322142e-05\n63 160 loss: 0.0015295591\n63 170 loss: 8.984554e-06\n63 180 loss: 0.00038708892\n63 190 loss: 3.4769673e-05\n63 200 loss: 4.786002e-06\n63 210 loss: 3.993224e-06\n63 220 loss: 1.2993434e-05\n63 230 loss: 6.0535945e-09\n63 evaluation acc: 0.9926\n64 0 loss: 0.016160542\n64 10 loss: 2.7919625e-05\n64 20 loss: 0.0002117654\n64 30 loss: 0.002605509\n64 40 loss: 2.573271e-05\n64 50 loss: 0.0052299765\n64 60 loss: 1.635668e-05\n64 70 loss: 0.00030521417\n64 80 loss: 0.00013311999\n64 90 loss: 0.00023795202\n64 100 loss: 0.008393313\n64 110 loss: 0.0040473305\n64 120 loss: 0.0013350844\n64 130 loss: 0.0022339036\n64 140 loss: 2.0375659e-05\n64 150 loss: 0.0011647999\n64 160 loss: 3.1709373e-05\n64 170 loss: 0.0006527598\n64 180 loss: 4.9301176e-05\n64 190 loss: 0.0021782136\n64 200 loss: 3.7553204e-05\n64 210 loss: 0.00062299083\n64 220 loss: 0.0292576\n64 230 loss: 6.0950543e-07\n64 evaluation acc: 0.9932\n65 0 loss: 4.8691614e-05\n65 10 loss: 0.006738737\n65 20 loss: 0.0043590195\n65 30 loss: 0.0015145249\n65 40 loss: 0.019044783\n65 50 loss: 0.007111377\n65 60 loss: 0.0100044245\n65 70 loss: 0.0012139288\n65 80 loss: 0.018453984\n65 90 loss: 7.793597e-05\n65 100 loss: 1.8374209e-05\n65 110 loss: 0.036507204\n65 120 loss: 0.00018841162\n65 130 loss: 0.0020074146\n65 140 loss: 0.0011338814\n65 150 loss: 0.0032276015\n65 160 loss: 0.0011914459\n65 170 loss: 0.00023304348\n65 180 loss: 0.00010416319\n65 190 loss: 5.0111605e-05\n65 200 loss: 0.00022306803\n65 210 loss: 0.0084189465\n65 220 loss: 0.00027990216\n65 230 loss: 1.601844e-07\n65 evaluation acc: 0.9931\n66 0 loss: 8.989117e-05\n66 10 loss: 7.045954e-05\n66 20 loss: 0.000998176\n66 30 loss: 0.00033096984\n66 40 loss: 0.00066007866\n66 50 loss: 1.50658625e-05\n66 60 loss: 0.00023398409\n66 70 loss: 0.00063993863\n66 80 loss: 0.00035550667\n66 90 loss: 2.9659179e-05\n66 100 loss: 6.522224e-05\n66 110 loss: 9.8943005e-05\n66 120 loss: 0.00011047308\n66 130 loss: 0.00017139301\n66 140 loss: 0.00010793222\n66 150 loss: 0.00058192486\n66 160 loss: 0.0010332579\n66 170 loss: 0.000996881\n66 180 loss: 0.0001525275\n66 190 loss: 2.3562163e-05\n66 200 loss: 0.0026694858\n66 210 loss: 0.00014386672\n66 220 loss: 0.002525857\n66 230 loss: 2.873072e-07\n66 evaluation acc: 0.9934\n67 0 loss: 1.9753392e-05\n67 10 loss: 0.0037876389\n67 20 loss: 0.0022931006\n67 30 loss: 0.00028111396\n67 40 loss: 0.0065411963\n67 50 loss: 0.011099456\n67 60 loss: 0.0001973871\n67 70 loss: 3.7366426e-05\n67 80 loss: 8.539129e-05\n67 90 loss: 0.0008912308\n67 100 loss: 3.57339e-05\n67 110 loss: 0.00020470665\n67 120 loss: 5.350859e-05\n67 130 loss: 0.000358262\n67 140 loss: 0.0064081503\n67 150 loss: 5.4357584e-05\n67 160 loss: 4.7605754e-05\n67 170 loss: 5.0328617e-06\n67 180 loss: 2.0769454e-05\n67 190 loss: 6.309531e-07\n67 200 loss: 0.00029242574\n67 210 loss: 0.0005935454\n67 220 loss: 9.9998084e-05\n67 230 loss: 4.563455e-08\n67 evaluation acc: 0.9921\n68 0 loss: 0.01768684\n68 10 loss: 2.0768684e-05\n68 20 loss: 0.005254645\n68 30 loss: 5.973437e-06\n68 40 loss: 0.00016759304\n68 50 loss: 0.0050432784\n68 60 loss: 0.000656528\n68 70 loss: 0.024054019\n68 80 loss: 0.00019888252\n68 90 loss: 0.015060346\n68 100 loss: 0.00013865631\n68 110 loss: 7.128375e-05\n68 120 loss: 0.006359709\n68 130 loss: 0.00047161058\n68 140 loss: 3.7982485e-05\n68 150 loss: 0.002779401\n68 160 loss: 0.011394834\n68 170 loss: 0.0013119432\n68 180 loss: 0.008800299\n68 190 loss: 0.00146817\n68 200 loss: 0.00040987143\n68 210 loss: 0.009765638\n68 220 loss: 1.3743111e-05\n68 230 loss: 5.2010927e-07\n68 evaluation acc: 0.9927\n69 0 loss: 0.00016752022\n69 10 loss: 7.365922e-05\n69 20 loss: 0.0045171743\n69 30 loss: 0.000103406084\n69 40 loss: 0.00028437303\n69 50 loss: 1.0094794e-05\n69 60 loss: 0.0002614719\n69 70 loss: 4.310223e-05\n69 80 loss: 0.0005631946\n69 90 loss: 4.2546235e-05\n69 100 loss: 4.3160133e-05\n69 110 loss: 0.0012888091\n69 120 loss: 0.015281513\n69 130 loss: 0.00019382552\n69 140 loss: 0.0001367526\n69 150 loss: 0.002148816\n69 160 loss: 0.009451849\n69 170 loss: 0.009103027\n69 180 loss: 0.016915523\n69 190 loss: 0.0005766313\n69 200 loss: 6.646858e-05\n69 210 loss: 0.001060872\n69 220 loss: 0.00034151378\n69 230 loss: 0.00034741318\n69 evaluation acc: 0.9917\n70 0 loss: 7.446118e-05\n70 10 loss: 0.00014744615\n70 20 loss: 0.00043217943\n70 30 loss: 0.00036965936\n70 40 loss: 0.00818271\n70 50 loss: 0.0018280848\n70 60 loss: 0.0005837057\n70 70 loss: 0.00010676221\n70 80 loss: 0.00059313356\n70 90 loss: 6.890483e-05\n70 100 loss: 0.0003123849\n70 110 loss: 2.2782782e-05\n70 120 loss: 0.00025826506\n70 130 loss: 3.9792503e-05\n70 140 loss: 3.15676e-05\n70 150 loss: 3.0436491e-05\n70 160 loss: 2.2605627e-06\n70 170 loss: 0.00061968446\n70 180 loss: 0.00018226042\n70 190 loss: 2.1988551e-05\n70 200 loss: 0.00019343394\n70 210 loss: 0.00061691354\n70 220 loss: 7.7810324e-07\n70 230 loss: 3.2827745e-07\n70 evaluation acc: 0.9939\n71 0 loss: 0.00019564365\n71 10 loss: 0.000106732274\n71 20 loss: 0.00034185054\n71 30 loss: 4.3190357e-05\n71 40 loss: 6.136701e-05\n71 50 loss: 1.67401e-06\n71 60 loss: 4.6892137e-06\n71 70 loss: 0.0016571535\n71 80 loss: 5.6278463e-06\n71 90 loss: 1.67751e-05\n71 100 loss: 1.9118786e-06\n71 110 loss: 9.07596e-06\n71 120 loss: 2.5180596e-05\n71 130 loss: 1.891739e-05\n71 140 loss: 3.0265357e-05\n71 150 loss: 5.6153167e-06\n71 160 loss: 2.0768317e-07\n71 170 loss: 6.1091646e-07\n71 180 loss: 2.2094127e-06\n71 190 loss: 4.610043e-08\n71 200 loss: 2.4167017e-05\n71 210 loss: 6.491202e-06\n71 220 loss: 4.2886805e-07\n71 230 loss: 3.7252885e-09\n71 evaluation acc: 0.9947\n72 0 loss: 7.124457e-07\n72 10 loss: 2.412014e-06\n72 20 loss: 5.222183e-06\n72 30 loss: 2.3487477e-05\n72 40 loss: 4.841723e-06\n72 50 loss: 1.7670167e-06\n72 60 loss: 3.726535e-06\n72 70 loss: 6.7842916e-06\n72 80 loss: 3.3807737e-05\n72 90 loss: 2.1963003e-06\n72 100 loss: 4.4235235e-06\n72 110 loss: 1.8387525e-06\n72 120 loss: 8.480551e-06\n72 130 loss: 1.8345832e-05\n72 140 loss: 7.4151035e-06\n72 150 loss: 4.214869e-06\n72 160 loss: 8.521582e-08\n72 170 loss: 6.961167e-07\n72 180 loss: 1.1584573e-06\n72 190 loss: 3.1664946e-08\n72 200 loss: 1.0268535e-05\n72 210 loss: 2.9385283e-06\n72 220 loss: 1.6996556e-07\n72 230 loss: 1.3969836e-09\n72 evaluation acc: 0.9949\n73 0 loss: 8.0044543e-07\n73 10 loss: 1.5965159e-06\n73 20 loss: 3.0688507e-06\n73 30 loss: 9.849289e-06\n73 40 loss: 3.3007402e-06\n73 50 loss: 9.326778e-07\n73 60 loss: 5.798894e-06\n73 70 loss: 3.2129578e-06\n73 80 loss: 1.8725025e-06\n73 90 loss: 1.8503733e-06\n73 100 loss: 5.927727e-07\n73 110 loss: 1.4555611e-06\n73 120 loss: 5.393465e-06\n73 130 loss: 7.4319387e-06\n73 140 loss: 6.0453867e-06\n73 150 loss: 2.7214414e-06\n73 160 loss: 6.100147e-08\n73 170 loss: 5.0987256e-07\n73 180 loss: 8.4745227e-07\n73 190 loss: 2.0023428e-08\n73 200 loss: 8.557779e-06\n73 210 loss: 2.6577732e-06\n73 220 loss: 1.182776e-07\n73 230 loss: 9.3132246e-10\n73 evaluation acc: 0.9948\n74 0 loss: 5.741481e-07\n74 10 loss: 1.4144848e-06\n74 20 loss: 2.186584e-06\n74 30 loss: 7.0621722e-06\n74 40 loss: 2.685395e-06\n74 50 loss: 8.181336e-07\n74 60 loss: 4.7005574e-06\n74 70 loss: 2.544649e-06\n74 80 loss: 1.3525455e-06\n74 90 loss: 1.4955947e-06\n74 100 loss: 4.6472027e-07\n74 110 loss: 1.1114782e-06\n74 120 loss: 3.490744e-06\n74 130 loss: 6.2283034e-06\n74 140 loss: 4.7677586e-06\n74 150 loss: 2.193018e-06\n74 160 loss: 4.7963006e-08\n74 170 loss: 3.9300247e-07\n74 180 loss: 6.8262733e-07\n74 190 loss: 1.3969834e-08\n74 200 loss: 7.101341e-06\n74 210 loss: 2.330927e-06\n74 220 loss: 9.220071e-08\n74 230 loss: 9.3132246e-10\n74 evaluation acc: 0.9948\n75 0 loss: 4.4423393e-07\n75 10 loss: 1.2822646e-06\n75 20 loss: 1.6590632e-06\n75 30 loss: 5.479929e-06\n75 40 loss: 2.218965e-06\n75 50 loss: 7.203512e-07\n75 60 loss: 3.8911485e-06\n75 70 loss: 2.1834605e-06\n75 80 loss: 1.0653087e-06\n75 90 loss: 1.2194927e-06\n75 100 loss: 3.8928533e-07\n75 110 loss: 8.7354715e-07\n75 120 loss: 2.3896712e-06\n75 130 loss: 5.3481335e-06\n75 140 loss: 3.8030646e-06\n75 150 loss: 1.8303308e-06\n75 160 loss: 3.9115474e-08\n75 170 loss: 3.101208e-07\n75 180 loss: 5.6761934e-07\n75 190 loss: 1.0710208e-08\n75 200 loss: 5.932965e-06\n75 210 loss: 2.0385312e-06\n75 220 loss: 7.403999e-08\n75 230 loss: 4.6566126e-10\n75 evaluation acc: 0.9948\n76 0 loss: 3.548297e-07\n76 10 loss: 1.1700637e-06\n76 20 loss: 1.3033343e-06\n76 30 loss: 4.416393e-06\n76 40 loss: 1.8498058e-06\n76 50 loss: 6.397967e-07\n76 60 loss: 3.268801e-06\n76 70 loss: 1.9293104e-06\n76 80 loss: 8.707039e-07\n76 90 loss: 9.978618e-07\n76 100 loss: 3.3573576e-07\n76 110 loss: 6.980054e-07\n76 120 loss: 1.7296416e-06\n76 130 loss: 4.653939e-06\n76 140 loss: 3.0518504e-06\n76 150 loss: 1.5714627e-06\n76 160 loss: 3.1664918e-08\n76 170 loss: 2.4493198e-07\n76 180 loss: 4.7542505e-07\n76 190 loss: 6.9849184e-09\n76 200 loss: 5.0179224e-06\n76 210 loss: 1.7777925e-06\n76 220 loss: 6.053586e-08\n76 230 loss: 4.6566126e-10\n76 evaluation acc: 0.9948\n77 0 loss: 2.9196684e-07\n77 10 loss: 1.0708982e-06\n77 20 loss: 1.0579495e-06\n77 30 loss: 3.627388e-06\n77 40 loss: 1.5458066e-06\n77 50 loss: 5.708827e-07\n77 60 loss: 2.7725694e-06\n77 70 loss: 1.7063395e-06\n77 80 loss: 7.2264925e-07\n77 90 loss: 8.2278876e-07\n77 100 loss: 2.9289606e-07\n77 110 loss: 5.666963e-07\n77 120 loss: 1.2841376e-06\n77 130 loss: 4.0397013e-06\n77 140 loss: 2.4848823e-06\n77 150 loss: 1.3610139e-06\n77 160 loss: 2.6076998e-08\n77 170 loss: 1.9464277e-07\n77 180 loss: 4.023207e-07\n77 190 loss: 5.587935e-09\n77 200 loss: 4.2683814e-06\n77 210 loss: 1.5542992e-06\n77 220 loss: 4.982568e-08\n77 230 loss: 4.6566126e-10\n77 evaluation acc: 0.9949\n78 0 loss: 2.4400458e-07\n78 10 loss: 9.777842e-07\n78 20 loss: 8.707647e-07\n78 30 loss: 3.0189508e-06\n78 40 loss: 1.3037157e-06\n78 50 loss: 5.1035e-07\n78 60 loss: 2.3656976e-06\n78 70 loss: 1.515481e-06\n78 80 loss: 6.0625104e-07\n78 90 loss: 6.8496337e-07\n78 100 loss: 2.607662e-07\n78 110 loss: 4.64721e-07\n78 120 loss: 9.838521e-07\n78 130 loss: 3.506378e-06\n78 140 loss: 2.0542666e-06\n78 150 loss: 1.1882767e-06\n78 160 loss: 2.1420394e-08\n78 170 loss: 1.5785686e-07\n78 180 loss: 3.4318475e-07\n78 190 loss: 4.190951e-09\n78 200 loss: 3.6481101e-06\n78 210 loss: 1.3615336e-06\n78 220 loss: 4.2840785e-08\n78 230 loss: 4.6566126e-10\n78 evaluation acc: 0.9949\n79 0 loss: 2.081492e-07\n79 10 loss: 8.89325e-07\n79 20 loss: 7.222258e-07\n79 30 loss: 2.5380405e-06\n79 40 loss: 1.1081751e-06\n79 50 loss: 4.572673e-07\n79 60 loss: 2.0328314e-06\n79 70 loss: 1.3418409e-06\n79 80 loss: 5.145269e-07\n79 90 loss: 5.741431e-07\n79 100 loss: 2.3329281e-07\n79 110 loss: 3.850959e-07\n79 120 loss: 7.6735563e-07\n79 130 loss: 3.0484093e-06\n79 140 loss: 1.7172024e-06\n79 150 loss: 1.0471987e-06\n79 160 loss: 1.722945e-08\n79 170 loss: 1.2852101e-07\n79 180 loss: 2.9522369e-07\n79 190 loss: 3.2596288e-09\n79 200 loss: 3.1236455e-06\n79 210 loss: 1.1976343e-06\n79 220 loss: 3.5390222e-08\n79 230 loss: 4.6566126e-10\n79 evaluation acc: 0.995\n80 0 loss: 1.788129e-07\n80 10 loss: 8.0877976e-07\n80 20 loss: 6.0628065e-07\n80 30 loss: 2.1525514e-06\n80 40 loss: 9.461511e-07\n80 50 loss: 4.1023767e-07\n80 60 loss: 1.752098e-06\n80 70 loss: 1.1905413e-06\n80 80 loss: 4.3909762e-07\n80 90 loss: 4.838097e-07\n80 100 loss: 2.1187287e-07\n80 110 loss: 3.2037084e-07\n80 120 loss: 6.071889e-07\n80 130 loss: 2.6592957e-06\n80 140 loss: 1.4373896e-06\n80 150 loss: 9.2707165e-07\n80 160 loss: 1.3969827e-08\n80 170 loss: 1.0570413e-07\n80 180 loss: 2.5517826e-07\n80 190 loss: 2.7939675e-09\n80 200 loss: 2.6763921e-06\n80 210 loss: 1.0565492e-06\n80 220 loss: 3.026795e-08\n80 230 loss: 0.0\n80 evaluation acc: 0.9949\n81 0 loss: 1.5506441e-07\n81 10 loss: 7.3614865e-07\n81 20 loss: 5.13617e-07\n81 30 loss: 1.8350252e-06\n81 40 loss: 8.088003e-07\n81 50 loss: 3.6926122e-07\n81 60 loss: 1.5188458e-06\n81 70 loss: 1.0564628e-06\n81 80 loss: 3.7577325e-07\n81 90 loss: 4.1023858e-07\n81 100 loss: 1.9045292e-07\n81 110 loss: 2.6635544e-07\n81 120 loss: 4.870604e-07\n81 130 loss: 2.3255498e-06\n81 140 loss: 1.2139043e-06\n81 150 loss: 8.2184323e-07\n81 160 loss: 1.2572846e-08\n81 170 loss: 8.8940645e-08\n81 180 loss: 2.2118607e-07\n81 190 loss: 2.3283062e-09\n81 200 loss: 2.3045059e-06\n81 210 loss: 9.340881e-07\n81 220 loss: 2.607701e-08\n81 230 loss: 0.0\n81 evaluation acc: 0.9949\n82 0 loss: 1.3504116e-07\n82 10 loss: 6.695692e-07\n82 20 loss: 4.3678497e-07\n82 30 loss: 1.5659127e-06\n82 40 loss: 6.8913954e-07\n82 50 loss: 3.324754e-07\n82 60 loss: 1.3153857e-06\n82 70 loss: 9.3169217e-07\n82 80 loss: 3.245545e-07\n82 90 loss: 3.4830813e-07\n82 100 loss: 1.7368949e-07\n82 110 loss: 2.2398112e-07\n82 120 loss: 3.990576e-07\n82 130 loss: 2.0392674e-06\n82 140 loss: 1.0276616e-06\n82 150 loss: 7.291859e-07\n82 160 loss: 1.07102025e-08\n82 170 loss: 7.3574036e-08\n82 180 loss: 1.9324719e-07\n82 190 loss: 1.3969838e-09\n82 200 loss: 1.9884517e-06\n82 210 loss: 8.274578e-07\n82 220 loss: 2.2351728e-08\n82 230 loss: 0.0\n82 evaluation acc: 0.9949\n83 0 loss: 1.18743145e-07\n83 10 loss: 6.104386e-07\n83 20 loss: 3.739221e-07\n83 30 loss: 1.3391641e-06\n83 40 loss: 5.885673e-07\n83 50 loss: 3.0034593e-07\n83 60 loss: 1.143116e-06\n83 70 loss: 8.139023e-07\n83 80 loss: 2.7985394e-07\n83 90 loss: 2.9662152e-07\n83 100 loss: 1.5739168e-07\n83 110 loss: 1.8859146e-07\n83 120 loss: 3.2735042e-07\n83 130 loss: 1.7925421e-06\n83 140 loss: 8.721461e-07\n83 150 loss: 6.504963e-07\n83 160 loss: 8.847559e-09\n83 170 loss: 6.239831e-08\n83 180 loss: 1.685677e-07\n83 190 loss: 9.313225e-10\n83 200 loss: 1.7156717e-06\n83 210 loss: 7.3293324e-07\n83 220 loss: 1.9092102e-08\n83 230 loss: 0.0\n83 evaluation acc: 0.9949\n84 0 loss: 1.0477341e-07\n84 10 loss: 5.550321e-07\n84 20 loss: 3.213034e-07\n84 30 loss: 1.1464012e-06\n84 40 loss: 5.0289316e-07\n84 50 loss: 2.7054458e-07\n84 60 loss: 9.973829e-07\n84 70 loss: 7.0681824e-07\n84 80 loss: 2.412062e-07\n84 90 loss: 2.5238498e-07\n84 100 loss: 1.4342211e-07\n84 110 loss: 1.6065218e-07\n84 120 loss: 2.7007712e-07\n84 130 loss: 1.5718776e-06\n84 140 loss: 7.445653e-07\n84 150 loss: 5.825155e-07\n84 160 loss: 7.450576e-09\n84 170 loss: 5.2153858e-08\n84 180 loss: 1.4668203e-07\n84 190 loss: 4.6566126e-10\n84 200 loss: 1.4791893e-06\n84 210 loss: 6.5098067e-07\n84 220 loss: 1.5832475e-08\n84 230 loss: 0.0\n84 evaluation acc: 0.9948\n85 0 loss: 9.1269314e-08\n85 10 loss: 5.028844e-07\n85 20 loss: 2.7660064e-07\n85 30 loss: 9.834348e-07\n85 40 loss: 4.349119e-07\n85 50 loss: 2.4260575e-07\n85 60 loss: 8.707375e-07\n85 70 loss: 6.136997e-07\n85 80 loss: 2.0954283e-07\n85 90 loss: 2.1559865e-07\n85 100 loss: 1.294525e-07\n85 110 loss: 1.3550675e-07\n85 120 loss: 2.2351303e-07\n85 130 loss: 1.3786735e-06\n85 140 loss: 6.393331e-07\n85 150 loss: 5.1965606e-07\n85 160 loss: 6.519255e-09\n85 170 loss: 4.470334e-08\n85 180 loss: 1.2805592e-07\n85 190 loss: 4.6566126e-10\n85 200 loss: 1.2762157e-06\n85 210 loss: 5.7554655e-07\n85 220 loss: 1.4435493e-08\n85 230 loss: 0.0\n85 evaluation acc: 0.9947\n86 0 loss: 8.102482e-08\n86 10 loss: 4.5539207e-07\n86 20 loss: 2.3841697e-07\n86 30 loss: 8.460756e-07\n86 40 loss: 3.7577684e-07\n86 50 loss: 2.1746075e-07\n86 60 loss: 7.6131835e-07\n86 70 loss: 5.2942556e-07\n86 80 loss: 1.8160435e-07\n86 90 loss: 1.8440002e-07\n86 100 loss: 1.1827683e-07\n86 110 loss: 1.1548349e-07\n86 120 loss: 1.8579581e-07\n86 130 loss: 1.2073453e-06\n86 140 loss: 5.499314e-07\n86 150 loss: 4.6331496e-07\n86 160 loss: 5.587933e-09\n86 170 loss: 3.7718465e-08\n86 180 loss: 1.12223645e-07\n86 190 loss: 0.0\n86 200 loss: 1.1044269e-06\n86 210 loss: 5.0989064e-07\n86 220 loss: 1.2572849e-08\n86 230 loss: 0.0\n86 evaluation acc: 0.9947\n87 0 loss: 6.984901e-08\n87 10 loss: 4.1348682e-07\n87 20 loss: 2.053554e-07\n87 30 loss: 7.2640876e-07\n87 40 loss: 3.2688524e-07\n87 50 loss: 1.9417828e-07\n87 60 loss: 6.6772895e-07\n87 70 loss: 4.5492797e-07\n87 80 loss: 1.5692518e-07\n87 90 loss: 1.5832347e-07\n87 100 loss: 1.0710113e-07\n87 110 loss: 9.965114e-08\n87 120 loss: 1.5459743e-07\n87 130 loss: 1.0546356e-06\n87 140 loss: 4.7496366e-07\n87 150 loss: 4.13027e-07\n87 160 loss: 4.656611e-09\n87 170 loss: 3.2130558e-08\n87 180 loss: 9.825398e-08\n87 190 loss: 0.0\n87 200 loss: 9.601015e-07\n87 210 loss: 4.512192e-07\n87 220 loss: 1.0710205e-08\n87 230 loss: 0.0\n87 evaluation acc: 0.9947\n88 0 loss: 6.286412e-08\n88 10 loss: 3.743748e-07\n88 20 loss: 1.778817e-07\n88 30 loss: 6.258319e-07\n88 40 loss: 2.8358093e-07\n88 50 loss: 1.7229269e-07\n88 60 loss: 5.843822e-07\n88 70 loss: 3.9160406e-07\n88 80 loss: 1.3597109e-07\n88 90 loss: 1.3550643e-07\n88 100 loss: 9.732239e-08\n88 110 loss: 8.521574e-08\n88 120 loss: 1.2898671e-07\n88 130 loss: 9.2147724e-07\n88 140 loss: 4.0977383e-07\n88 150 loss: 3.6646364e-07\n88 160 loss: 4.1909503e-09\n88 170 loss: 2.7939628e-08\n88 180 loss: 8.5681265e-08\n88 190 loss: 0.0\n88 200 loss: 8.343951e-07\n88 210 loss: 3.9813523e-07\n88 220 loss: 9.313222e-09\n88 230 loss: 0.0\n88 evaluation acc: 0.9947\n89 0 loss: 5.5879227e-08\n89 10 loss: 3.394532e-07\n89 20 loss: 1.5320188e-07\n89 30 loss: 5.38292e-07\n89 40 loss: 2.4586404e-07\n89 50 loss: 1.5226965e-07\n89 60 loss: 5.108129e-07\n89 70 loss: 3.389886e-07\n89 80 loss: 1.1874211e-07\n89 90 loss: 1.1594892e-07\n89 100 loss: 8.894061e-08\n89 110 loss: 7.357428e-08\n89 120 loss: 1.0756675e-07\n89 130 loss: 8.027496e-07\n89 140 loss: 3.5389652e-07\n89 150 loss: 3.250221e-07\n89 160 loss: 3.7252894e-09\n89 170 loss: 2.4214351e-08\n89 180 loss: 7.543681e-08\n89 190 loss: 0.0\n89 200 loss: 7.28241e-07\n89 210 loss: 3.5343285e-07\n89 220 loss: 8.847561e-09\n89 230 loss: 0.0\n89 evaluation acc: 0.9948\n90 0 loss: 4.935999e-08\n90 10 loss: 3.082563e-07\n90 20 loss: 1.3364428e-07\n90 30 loss: 4.623925e-07\n90 40 loss: 2.1233772e-07\n90 50 loss: 1.3597182e-07\n90 60 loss: 4.47487e-07\n90 70 loss: 2.942883e-07\n90 80 loss: 1.02909986e-07\n90 90 loss: 1.0011663e-07\n90 100 loss: 8.242145e-08\n90 110 loss: 6.379544e-08\n90 120 loss: 8.940628e-08\n90 130 loss: 7.0031615e-07\n90 140 loss: 3.0500368e-07\n90 150 loss: 2.8730548e-07\n90 160 loss: 2.793967e-09\n90 170 loss: 2.0954731e-08\n90 180 loss: 6.612366e-08\n90 190 loss: 0.0\n90 200 loss: 6.337246e-07\n90 210 loss: 3.138525e-07\n90 220 loss: 7.450579e-09\n90 230 loss: 0.0\n90 evaluation acc: 0.9948\n91 0 loss: 4.4703395e-08\n91 10 loss: 2.78456e-07\n91 20 loss: 1.154836e-07\n91 30 loss: 3.9859927e-07\n91 40 loss: 1.8439898e-07\n91 50 loss: 1.2107091e-07\n91 60 loss: 3.916108e-07\n91 70 loss: 2.5517517e-07\n91 80 loss: 8.94061e-08\n91 90 loss: 8.568127e-08\n91 100 loss: 7.543661e-08\n91 110 loss: 5.4947915e-08\n91 120 loss: 7.450532e-08\n91 130 loss: 6.099871e-07\n91 140 loss: 2.640266e-07\n91 150 loss: 2.5564202e-07\n91 160 loss: 2.793967e-09\n91 170 loss: 1.816077e-08\n91 180 loss: 5.7741815e-08\n91 190 loss: 0.0\n91 200 loss: 5.4991546e-07\n91 210 loss: 2.7892858e-07\n91 220 loss: 6.9849175e-09\n91 230 loss: 0.0\n91 evaluation acc: 0.9948\n92 0 loss: 3.958114e-08\n92 10 loss: 2.5238066e-07\n92 20 loss: 1.0011688e-07\n92 30 loss: 3.431874e-07\n92 40 loss: 1.5832269e-07\n92 50 loss: 1.0849824e-07\n92 60 loss: 3.4178754e-07\n92 70 loss: 2.2211498e-07\n92 80 loss: 7.7299134e-08\n92 90 loss: 7.3574185e-08\n92 100 loss: 6.9383084e-08\n92 110 loss: 4.7963024e-08\n92 120 loss: 6.286393e-08\n92 130 loss: 5.2896934e-07\n92 140 loss: 2.2724022e-07\n92 150 loss: 2.2630658e-07\n92 160 loss: 2.328306e-09\n92 170 loss: 1.583247e-08\n92 180 loss: 5.1222592e-08\n92 190 loss: 0.0\n92 200 loss: 4.78211e-07\n92 210 loss: 2.4726413e-07\n92 220 loss: 6.053595e-09\n92 230 loss: 0.0\n92 evaluation acc: 0.9949\n93 0 loss: 3.585586e-08\n93 10 loss: 2.2770203e-07\n93 20 loss: 8.707843e-08\n93 30 loss: 2.942943e-07\n93 40 loss: 1.3783414e-07\n93 50 loss: 9.639122e-08\n93 60 loss: 2.9615464e-07\n93 70 loss: 1.9231403e-07\n93 80 loss: 6.705474e-08\n93 90 loss: 6.3329715e-08\n93 100 loss: 6.3795206e-08\n93 110 loss: 4.0512464e-08\n93 120 loss: 5.1688165e-08\n93 130 loss: 4.5865994e-07\n93 140 loss: 1.9697286e-07\n93 150 loss: 2.0023055e-07\n93 160 loss: 2.328306e-09\n93 170 loss: 1.3969828e-08\n93 180 loss: 4.4703373e-08\n93 190 loss: 0.0\n93 200 loss: 4.1581808e-07\n93 210 loss: 2.1885923e-07\n93 220 loss: 4.656612e-09\n93 230 loss: 0.0\n93 evaluation acc: 0.9949\n94 0 loss: 3.3061898e-08\n94 10 loss: 2.0535147e-07\n94 20 loss: 7.543696e-08\n94 30 loss: 2.5238575e-07\n94 40 loss: 1.1874242e-07\n94 50 loss: 8.521548e-08\n94 60 loss: 2.5983445e-07\n94 70 loss: 1.6763497e-07\n94 80 loss: 5.6344668e-08\n94 90 loss: 5.3550885e-08\n94 100 loss: 5.867299e-08\n94 110 loss: 3.539021e-08\n94 120 loss: 4.3771994e-08\n94 130 loss: 3.985937e-07\n94 140 loss: 1.7089629e-07\n94 150 loss: 1.7787957e-07\n94 160 loss: 1.8626449e-09\n94 170 loss: 1.2572846e-08\n94 180 loss: 4.0046785e-08\n94 190 loss: 0.0\n94 200 loss: 3.5994304e-07\n94 210 loss: 1.9464517e-07\n94 220 loss: 4.1909507e-09\n94 230 loss: 0.0\n94 evaluation acc: 0.9949\n95 0 loss: 2.980228e-08\n95 10 loss: 1.8393204e-07\n95 20 loss: 6.5658114e-08\n95 30 loss: 2.1792752e-07\n95 40 loss: 1.02910235e-07\n95 50 loss: 7.590236e-08\n95 60 loss: 2.2630795e-07\n95 70 loss: 1.4668095e-07\n95 80 loss: 4.8894172e-08\n95 90 loss: 4.6566008e-08\n95 100 loss: 5.355076e-08\n95 110 loss: 2.9802287e-08\n95 120 loss: 3.678712e-08\n95 130 loss: 3.4597696e-07\n95 140 loss: 1.4807925e-07\n95 150 loss: 1.5645979e-07\n95 160 loss: 1.8626449e-09\n95 170 loss: 1.1641525e-08\n95 180 loss: 3.5390194e-08\n95 190 loss: 0.0\n95 200 loss: 3.1198323e-07\n95 210 loss: 1.73225e-07\n95 220 loss: 4.1909507e-09\n95 230 loss: 0.0\n95 evaluation acc: 0.9949\n96 0 loss: 2.7008316e-08\n96 10 loss: 1.6437511e-07\n96 20 loss: 5.634492e-08\n96 30 loss: 1.8719439e-07\n96 40 loss: 8.8940624e-08\n96 50 loss: 6.7054884e-08\n96 60 loss: 1.979034e-07\n96 70 loss: 1.275894e-07\n96 80 loss: 4.2840643e-08\n96 90 loss: 4.051244e-08\n96 100 loss: 4.9359848e-08\n96 110 loss: 2.6542665e-08\n96 120 loss: 3.073356e-08\n96 130 loss: 3.017414e-07\n96 140 loss: 1.2805607e-07\n96 150 loss: 1.3876517e-07\n96 160 loss: 1.8626449e-09\n96 170 loss: 8.847559e-09\n96 180 loss: 3.0733595e-08\n96 190 loss: 0.0\n96 200 loss: 2.7100742e-07\n96 210 loss: 1.5413309e-07\n96 220 loss: 3.7252899e-09\n96 230 loss: 0.0\n96 evaluation acc: 0.9949\n97 0 loss: 2.4680014e-08\n97 10 loss: 1.4668066e-07\n97 20 loss: 4.982568e-08\n97 30 loss: 1.6111774e-07\n97 40 loss: 7.683361e-08\n97 50 loss: 5.960437e-08\n97 60 loss: 1.7275835e-07\n97 70 loss: 1.1082605e-07\n97 80 loss: 3.632144e-08\n97 90 loss: 3.492453e-08\n97 100 loss: 4.516894e-08\n97 110 loss: 2.3283041e-08\n97 120 loss: 2.6542633e-08\n97 130 loss: 2.635589e-07\n97 140 loss: 1.1129246e-07\n97 150 loss: 1.2246743e-07\n97 160 loss: 1.3969838e-09\n97 170 loss: 7.916237e-09\n97 180 loss: 2.6542654e-08\n97 190 loss: 0.0\n97 200 loss: 2.3608465e-07\n97 210 loss: 1.3736945e-07\n97 220 loss: 3.2596286e-09\n97 230 loss: 0.0\n97 evaluation acc: 0.9948\n98 0 loss: 2.2817371e-08\n98 10 loss: 1.308487e-07\n98 20 loss: 4.284078e-08\n98 30 loss: 1.3923194e-07\n98 40 loss: 6.75205e-08\n98 50 loss: 5.308517e-08\n98 60 loss: 1.5087275e-07\n98 70 loss: 9.639088e-08\n98 80 loss: 3.166486e-08\n98 90 loss: 3.1199253e-08\n98 100 loss: 4.1909338e-08\n98 110 loss: 2.002342e-08\n98 120 loss: 2.1886038e-08\n98 130 loss: 2.2816994e-07\n98 140 loss: 9.592578e-08\n98 150 loss: 1.084979e-07\n98 160 loss: 9.3132246e-10\n98 170 loss: 6.9849158e-09\n98 180 loss: 2.4214355e-08\n98 190 loss: 0.0\n98 200 loss: 2.048867e-07\n98 210 loss: 1.2293408e-07\n98 220 loss: 3.2596286e-09\n98 230 loss: 0.0\n98 evaluation acc: 0.9948\n99 0 loss: 2.0954731e-08\n99 10 loss: 1.1687929e-07\n99 20 loss: 3.771852e-08\n99 30 loss: 1.1967438e-07\n99 40 loss: 5.7741715e-08\n99 50 loss: 4.703162e-08\n99 60 loss: 1.3084968e-07\n99 70 loss: 8.381826e-08\n99 80 loss: 2.7939596e-08\n99 90 loss: 2.6076993e-08\n99 100 loss: 3.7718415e-08\n99 110 loss: 1.7695116e-08\n99 120 loss: 1.8626421e-08\n99 130 loss: 1.9836862e-07\n99 140 loss: 8.335304e-08\n99 150 loss: 9.639095e-08\n99 160 loss: 9.3132246e-10\n99 170 loss: 6.053594e-09\n99 180 loss: 2.1420394e-08\n99 190 loss: 0.0\n99 200 loss: 1.7834506e-07\n99 210 loss: 1.0989566e-07\n99 220 loss: 2.3283062e-09\n99 230 loss: 0.0\n99 evaluation acc: 0.9948\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}