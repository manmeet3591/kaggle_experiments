{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# High-Dimensional Time Series Forecasting with Convolutional Neural Networks\n\n**Note**: for a written overview on this topic, check out my [blog post](https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_conv/). \n\nThis notebook aims to demonstrate in python/keras code how a **convolutional** sequence-to-sequence neural network can be built for the purpose of high-dimensional time series forecasting. For an introduction to neural network forecasting with an LSTM architecture, check out the [first notebook in this series](https://github.com/JEddy92/TimeSeries_Seq2Seq/blob/master/notebooks/TS_Seq2Seq_Intro.ipynb). I assume working familiarity with 1-dimensional convolutions, and recommend checking out [Chris Olah's blog post](http://colah.github.io/posts/2014-07-Understanding-Convolutions/) if you want a nice primer.  \n\nIn this notebook I'll be using the daily wikipedia web page traffic dataset again, available [here on Kaggle](https://www.kaggle.com/c/web-traffic-time-series-forecasting/data). The corresponding competition called for forecasting 60 days into the future, but for this demonstration we'll simplify to forecasting only 14 days. However, we will use all of the series history available in \"train_1.csv\" for the encoding stage of the model. \n\nOur goal here is to show a relatively simple implementation of the core convolutional seq2seq architecture that can be nicely applied to this problem. In particular, I'll use a stack of **1-dimensional causal convolutions with exponentially increasing dilation rates**, as in the [WaveNet model](https://arxiv.org/pdf/1609.03499.pdf). Don't worry, I'll explain what all that means in section 3! Feel free to skip ahead to that section if you're comfortable with the data setup and formatting (it's the same as in the previous notebook), and want to get right into the neural network.     \n\nHere's a section breakdown of this notebook -- enjoy!\n\n**1. Loading and Previewing the Data**   \n**2. Formatting the Data for Modeling**  \n**3. Building the Model - Training Architecture**  \n**4. Building the Model - Inference Loop**  \n**5. Generating and Plotting Predictions**","metadata":{}},{"cell_type":"markdown","source":"## 1. Loading and Previewing the Data \n\nFirst thing's first, let's load up the data and get a quick feel for it (reminder that the dataset is available [here](https://www.kaggle.com/c/web-traffic-time-series-forecasting/data)). \n\nNote that there are a good number of NaN values in the data that don't disambiguate missing from zero. For the sake of simplicity in this tutorial, we'll naively fill these with 0 later on.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\n\ndf = pd.read_csv('../input/wavenet-data/train_1.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-14T10:44:49.315745Z","iopub.execute_input":"2021-11-14T10:44:49.316114Z","iopub.status.idle":"2021-11-14T10:44:58.626564Z","shell.execute_reply.started":"2021-11-14T10:44:49.316077Z","shell.execute_reply":"2021-11-14T10:44:58.625862Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-14T10:44:58.628244Z","iopub.execute_input":"2021-11-14T10:44:58.628516Z","iopub.status.idle":"2021-11-14T10:44:58.659364Z","shell.execute_reply.started":"2021-11-14T10:44:58.628481Z","shell.execute_reply":"2021-11-14T10:44:58.658554Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"data_start_date = df.columns[1]\ndata_end_date = df.columns[-1]\nprint('Data ranges from %s to %s' % (data_start_date, data_end_date))","metadata":{"execution":{"iopub.status.busy":"2021-11-14T10:44:58.660668Z","iopub.execute_input":"2021-11-14T10:44:58.661071Z","iopub.status.idle":"2021-11-14T10:44:58.669374Z","shell.execute_reply.started":"2021-11-14T10:44:58.661036Z","shell.execute_reply":"2021-11-14T10:44:58.668533Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"We can define a function that lets us visualize some random webpage series as below. For the sake of smoothing out the scale of traffic across different series, we apply a log1p transformation before plotting - i.e. take $\\log(1+x)$ for each value $x$ in a series.","metadata":{}},{"cell_type":"code","source":"def plot_random_series(df, n_series):\n    \n    sample = df.sample(n_series, random_state=8)\n    page_labels = sample['Page'].tolist()\n    series_samples = sample.loc[:,data_start_date:data_end_date]\n    \n    plt.figure(figsize=(10,6))\n    \n    for i in range(series_samples.shape[0]):\n        np.log1p(pd.Series(series_samples.iloc[i]).astype(np.float64)).plot(linewidth=1.5)\n    \n    plt.title('Randomly Selected Wikipedia Page Daily Views Over Time (Log(views) + 1)')\n    plt.legend(page_labels)\n    \nplot_random_series(df, 6)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T10:44:58.671734Z","iopub.execute_input":"2021-11-14T10:44:58.672292Z","iopub.status.idle":"2021-11-14T10:44:59.106564Z","shell.execute_reply.started":"2021-11-14T10:44:58.672253Z","shell.execute_reply":"2021-11-14T10:44:59.105798Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## 2. Formatting the Data for Modeling \n\nSadly we can't just throw the dataframe we've created into keras and let it work its magic. Instead, we have to set up a few data transformation steps to extract nice numpy arrays that we can pass to keras. But even before doing that, we have to know how to appropriately partition the time series into encoding and prediction intervals for the purposes of training and validation. Note that for our simple convolutional model we won't use an encoder-decoder architecture like in the first notebook, but **we'll keep the \"encoding\" and \"decoding\" (prediction) terminology to be consistent** -- in this case, the encoding interval represents the entire series history that we will use for the network's feature learning, but not output any predictions on. \n\nWe'll use a style of **walk-forward validation**, where our validation set spans the same time-range as our training set, but shifted forward in time (in this case by 14 days). This way, we simulate how our model will perform on unseen data that comes in the future. \n\n[Artur Suilin](https://github.com/Arturus/kaggle-web-traffic/blob/master/how_it_works.md) has created a very nice image that visualizes this validation style and contrasts it with traditional validation. I highly recommend checking out his entire repo, as he's implemented a truly state of the art (and competition winning) seq2seq model on this data set. \n\n![architecture](images/ArturSuilin_validation.png)\n\n### Train and Validation Series Partioning\n\nWe need to create 4 sub-segments of the data:\n\n    1. Train encoding period\n    2. Train decoding period (train targets, 14 days)\n    3. Validation encoding period\n    4. Validation decoding period (validation targets, 14 days)\n    \nWe'll do this by finding the appropriate start and end dates for each segment. Starting from the end of the data we've loaded, we'll work backwards to get validation and training prediction intervals. Then we'll work forward from the start to get training and validation encoding intervals. ","metadata":{}},{"cell_type":"code","source":"from datetime import timedelta\n\npred_steps = 14\npred_length=timedelta(pred_steps)\n\nfirst_day = pd.to_datetime(data_start_date) \nlast_day = pd.to_datetime(data_end_date)\n\nval_pred_start = last_day - pred_length + timedelta(1)\nval_pred_end = last_day\n\ntrain_pred_start = val_pred_start - pred_length\ntrain_pred_end = val_pred_start - timedelta(days=1) ","metadata":{"execution":{"iopub.status.busy":"2021-11-14T10:44:59.108042Z","iopub.execute_input":"2021-11-14T10:44:59.108313Z","iopub.status.idle":"2021-11-14T10:44:59.114275Z","shell.execute_reply.started":"2021-11-14T10:44:59.108276Z","shell.execute_reply":"2021-11-14T10:44:59.113429Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"enc_length = train_pred_start - first_day\n\ntrain_enc_start = first_day\ntrain_enc_end = train_enc_start + enc_length - timedelta(1)\n\nval_enc_start = train_enc_start + pred_length\nval_enc_end = val_enc_start + enc_length - timedelta(1) ","metadata":{"execution":{"iopub.status.busy":"2021-11-14T10:44:59.115665Z","iopub.execute_input":"2021-11-14T10:44:59.115927Z","iopub.status.idle":"2021-11-14T10:44:59.124061Z","shell.execute_reply.started":"2021-11-14T10:44:59.115891Z","shell.execute_reply":"2021-11-14T10:44:59.123341Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"print('Train encoding:', train_enc_start, '-', train_enc_end)\nprint('Train prediction:', train_pred_start, '-', train_pred_end, '\\n')\nprint('Val encoding:', val_enc_start, '-', val_enc_end)\nprint('Val prediction:', val_pred_start, '-', val_pred_end)\n\nprint('\\nEncoding interval:', enc_length.days)\nprint('Prediction interval:', pred_length.days)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T10:44:59.125337Z","iopub.execute_input":"2021-11-14T10:44:59.125799Z","iopub.status.idle":"2021-11-14T10:44:59.135899Z","shell.execute_reply.started":"2021-11-14T10:44:59.125761Z","shell.execute_reply":"2021-11-14T10:44:59.135139Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### Keras Data Formatting\n\nNow that we have the time segment dates, we'll define the functions we need to extract the data in keras friendly format. Here are the steps:\n\n* Pull the time series into an array, save a date_to_index mapping as a utility for referencing into the array \n* Create function to extract specified time interval from all the series \n* Create functions to transform all the series. \n    - Here we smooth out the scale by taking log1p and de-meaning each series using the encoder series mean, then reshape to the **(n_series, n_timesteps, n_features) tensor format** that keras will expect. \n    - Note that if we want to generate true predictions instead of log scale ones, we can easily apply a reverse transformation at prediction time. ","metadata":{}},{"cell_type":"code","source":"date_to_index = pd.Series(index=pd.Index([pd.to_datetime(c) for c in df.columns[1:]]),\n                          data=[i for i in range(len(df.columns[1:]))])\n\nseries_array = df[df.columns[1:]].values\n\ndef get_time_block_series(series_array, date_to_index, start_date, end_date):\n    \n    inds = date_to_index[start_date:end_date]\n    return series_array[:,inds]\n\ndef transform_series_encode(series_array):\n    \n    series_array = np.log1p(np.nan_to_num(series_array)) # filling NaN with 0\n    series_mean = series_array.mean(axis=1).reshape(-1,1) \n    series_array = series_array - series_mean\n    series_array = series_array.reshape((series_array.shape[0],series_array.shape[1], 1))\n    \n    return series_array, series_mean\n\ndef transform_series_decode(series_array, encode_series_mean):\n    \n    series_array = np.log1p(np.nan_to_num(series_array)) # filling NaN with 0\n    series_array = series_array - encode_series_mean\n    series_array = series_array.reshape((series_array.shape[0],series_array.shape[1], 1))\n    \n    return series_array","metadata":{"execution":{"iopub.status.busy":"2021-11-14T10:44:59.137178Z","iopub.execute_input":"2021-11-14T10:44:59.138084Z","iopub.status.idle":"2021-11-14T10:44:59.564235Z","shell.execute_reply.started":"2021-11-14T10:44:59.138046Z","shell.execute_reply":"2021-11-14T10:44:59.563457Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## 3. Building the Model - Architecture\n\nThis convolutional architecture is a simplified version of the [WaveNet model](https://deepmind.com/blog/wavenet-generative-model-raw-audio/), designed as a generative model for audio (in particular, for text-to-speech applications). The wavenet model can be abstracted beyond audio to apply to any time series forecasting problem, providing a nice structure for capturing long-term dependencies without an excessive number of learned weights.\n\nThe core building block of the wavenet model is the **dilated causal convolution layer**. It utilizes some other key techniques like *gated activations* and *skip connections*, but for now we'll focus on the central idea of the architecture to keep things simple (check out the next notebook in the series for these). I'll explain this style of convolution (causal and dilated), then show how to implement our simplified WaveNet architecture in keras. \n\n\n### **Causal Convolutions**\n\nIn a traditional 1-dimensional convolution layer, as in the image below taken from [Chris Olah's excellent blog](http://colah.github.io/posts/2014-07-Understanding-Convolutions/), we slide a filter of weights across an input series, sequentially applying it to (usually overlapping) regions of the series. The output shape will depend on the sequence padding used, and is closely related to the connection structure between inputs and outputs. In this example, a filter of width 2, stride of 1, and no padding means that the output sequence will have one fewer entry than the input.      \n\n![1dconv](images/Colah_1DConv.png)\n\nIn the image, imagine that $y_0,..., y_7$ are each prediction outputs for the time steps that follow the series values $x_0,...,x_7$. There is a clear problem - since $x_1$ influences the output $y_0$, **we would be using the future to predict the past, which is cheating!** Letting the future of a sequence influence our interpretation of its past makes sense in a context like text classification where we use a known sequence to predict an outcome, but not in our time series context where we must generate future values in a sequence. \n\nTo solve this problem, we adjust our convolution design to explicitly prohibit the future from influencing the past. In other words, we only allow inputs to connect to future time step outputs in a **causal** structure, as pictured below in a visualization from the WaveNet paper. In practice, this causal 1D structure is easy to implement by shifting traditional convolutional outputs by a number of timesteps. Keras handles it via setting ```padding = 'causal'```.     \n\n![causalconv](images/WaveNet_causalconv.png)\n\n### **Dilated (Causal) Convolutions**\n\nWith causal convolutions we have the proper tool for handling temporal flow, but we need an additional modification to properly handle long-term dependencies. In the simple causal convolution figure above, you can see that only the 5 most recent timesteps can influence the highlighted output. In fact, **we would require one additional layer per timestep** to reach farther back in the series (to use proper terminology, to increase the output's **receptive field**). With a time series that extends for over a year, using simple causal convolutions to learn from the entire history would quickly make our model way too computationally and statistically complex. \n\nInstead of making that mistake, WaveNet uses **dilated convolutions**, which allow the receptive field to increase exponentially as a function of the number of convolutional layers. In a dilated convolution layer, filters are not applied to inputs in a simple sequential manner, but instead skip a constant **dilation rate** inputs in between each of the inputs they process, as in the WaveNet diagram below. By increasing the dilation rate multiplicatively at each layer (e.g. 1, 2, 4, 8, ...), we can achieve the exponential relationship between layer count and receptive field size that we desire. In the diagram, you can see how we now only need 4 layers to connect all of the 16 input series values to the highlighted output (say the 17th time step value).  \n\n![dilatedconv](images/WaveNet_dilatedconv.png)\n\n\n### **Our Architecture**\n\nHere's what we'll use:\n\n* 8 dilated causal convolutional layers\n    * 32 filters of width 2 per layer\n    * Exponentially increasing dilation rate (1, 2, 4, 8, ..., 128) \n* 2 (time distributed) fully connected layers to map to final output \n\nWe'll extract the last 14 steps from the output sequence as our predicted output for training. We'll use teacher forcing again during training. Similarly to the previous notebook, we'll have a separate function that runs an inference loop to generate predictions on unseen data, iteratively filling previous predictions into the history sequence (section 4).","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv1D, Dense, Dropout, Lambda, concatenate\nfrom tensorflow.keras.optimizers import Adam\n\n# convolutional layer parameters\nn_filters = 32 \nfilter_width = 2\ndilation_rates = [2**i for i in range(8)] \n\n# define an input history series and pass it through a stack of dilated causal convolutions. \nhistory_seq = Input(shape=(None, 1))\nx = history_seq\n\nfor dilation_rate in dilation_rates:\n    x = Conv1D(filters=n_filters,\n               kernel_size=filter_width, \n               padding='causal',\n               dilation_rate=dilation_rate)(x)\n\nx = Dense(128, activation='relu')(x)\nx = Dropout(.2)(x)\nx = Dense(1)(x)\n\n# extract the last 14 time steps as the training target\ndef slice(x, seq_length):\n    return x[:,-seq_length:,:]\n\npred_seq_train = Lambda(slice, arguments={'seq_length':14})(x)\n\nmodel = Model(history_seq, pred_seq_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T10:44:59.565458Z","iopub.execute_input":"2021-11-14T10:44:59.565701Z","iopub.status.idle":"2021-11-14T10:44:59.849347Z","shell.execute_reply.started":"2021-11-14T10:44:59.565667Z","shell.execute_reply":"2021-11-14T10:44:59.848680Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-14T10:44:59.852166Z","iopub.execute_input":"2021-11-14T10:44:59.852428Z","iopub.status.idle":"2021-11-14T10:44:59.864923Z","shell.execute_reply.started":"2021-11-14T10:44:59.852392Z","shell.execute_reply":"2021-11-14T10:44:59.864140Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"With our training architecture defined, we're ready to train the model! This will take some time if you're not running fancy hardware (read GPU). We'll leverage the transformer utility functions we defined earlier, and train using mean absolute error loss.\n\nNote that for this simple model, we have fewer total parameters to train than we did with the simple LSTM architecture, and the model appears to converge with significantly fewer epochs (though we are using twice as much training data). But most interesting is that our predictions end up being clearly more expressive than before, indicating that this architecture is more naturally suited for learning the series' patterns (see section 5).\n\nFor better results, you could try using more data, adjusting the hyperparameters, tuning the learning rate and number of epochs, etc.  ","metadata":{}},{"cell_type":"code","source":"first_n_samples = 40000\nbatch_size = 2**11\nepochs = 10\n\n# sample of series from train_enc_start to train_enc_end  \nencoder_input_data = get_time_block_series(series_array, date_to_index, \n                                           train_enc_start, train_enc_end)[:first_n_samples]\nencoder_input_data, encode_series_mean = transform_series_encode(encoder_input_data)\n\n# sample of series from train_pred_start to train_pred_end \ndecoder_target_data = get_time_block_series(series_array, date_to_index, \n                                            train_pred_start, train_pred_end)[:first_n_samples]\ndecoder_target_data = transform_series_decode(decoder_target_data, encode_series_mean)\n\n# we append a lagged history of the target series to the input data, \n# so that we can train with teacher forcing\nlagged_target_history = decoder_target_data[:,:-1,:1]\nencoder_input_data = np.concatenate([encoder_input_data, lagged_target_history], axis=1)\n\nmodel.compile(Adam(), loss='mean_absolute_error')\nhistory = model.fit(encoder_input_data, decoder_target_data,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T10:44:59.866270Z","iopub.execute_input":"2021-11-14T10:44:59.866520Z","iopub.status.idle":"2021-11-14T10:46:12.371764Z","shell.execute_reply.started":"2021-11-14T10:44:59.866485Z","shell.execute_reply":"2021-11-14T10:46:12.371045Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"It's typically a good idea to look at the convergence curve of train/validation loss.","metadata":{}},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\nplt.xlabel('Epoch')\nplt.ylabel('Mean Absolute Error Loss')\nplt.title('Loss Over Time')\nplt.legend(['Train','Valid'])","metadata":{"execution":{"iopub.status.busy":"2021-11-14T10:46:12.374558Z","iopub.execute_input":"2021-11-14T10:46:12.374762Z","iopub.status.idle":"2021-11-14T10:46:12.642454Z","shell.execute_reply.started":"2021-11-14T10:46:12.374736Z","shell.execute_reply":"2021-11-14T10:46:12.641821Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## 4. Building the Model - Inference Loop\n\nUnlike in the previous notebook, we don't need to define a distinct keras model in order to actually generate predictions. Instead, we'll run our model from section 3 in a loop, using each iteration to extract the prediction for the time step one beyond our current history then append it to our history sequence. With 14 iterations, this lets us generate predictions for the full interval we've chosen. \n\nRecall that we designed our model to output predictions for 14 time steps at once in order to use teacher forcing for training. So if we start from a history sequence and want to predict the first future time step, we can run the model on the history sequence and take the last time step of the output, which corresponds to one time step beyond the history sequence.           ","metadata":{}},{"cell_type":"code","source":"def predict_sequence(input_sequence):\n\n    history_sequence = input_sequence.copy()\n    pred_sequence = np.zeros((1,pred_steps,1)) # initialize output (pred_steps time steps)  \n    \n    for i in range(pred_steps):\n        \n        # record next time step prediction (last time step of model output) \n        last_step_pred = model.predict(history_sequence)[0,-1,0]\n        pred_sequence[0,i,0] = last_step_pred\n        \n        # add the next time step prediction to the history sequence\n        history_sequence = np.concatenate([history_sequence, \n                                           last_step_pred.reshape(-1,1,1)], axis=1)\n\n    return pred_sequence","metadata":{"execution":{"iopub.status.busy":"2021-11-14T10:46:12.643595Z","iopub.execute_input":"2021-11-14T10:46:12.643841Z","iopub.status.idle":"2021-11-14T10:46:12.649631Z","shell.execute_reply.started":"2021-11-14T10:46:12.643806Z","shell.execute_reply":"2021-11-14T10:46:12.648953Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## 5. Generating and Plotting Predictions \n\nNow we have everything we need to generate predictions for encoder (history) /target series pairs that we didn't train on (note again we're using \"encoder\"/\"decoder\" terminology to stay consistent with notebook 1 -- here it's more like history/target). We'll pull out our set of validation encoder/target series (recall that these are shifted forward in time). Then using a plotting utility function, we can look at the tail end of the encoder series, the true target series, and the predicted target series. This gives us a feel for how our predictions are doing.  ","metadata":{}},{"cell_type":"code","source":"encoder_input_data = get_time_block_series(series_array, date_to_index, val_enc_start, val_enc_end)\nencoder_input_data, encode_series_mean = transform_series_encode(encoder_input_data)\n\ndecoder_target_data = get_time_block_series(series_array, date_to_index, val_pred_start, val_pred_end)\ndecoder_target_data = transform_series_decode(decoder_target_data, encode_series_mean)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T10:46:12.651020Z","iopub.execute_input":"2021-11-14T10:46:12.651504Z","iopub.status.idle":"2021-11-14T10:46:15.184984Z","shell.execute_reply.started":"2021-11-14T10:46:12.651467Z","shell.execute_reply":"2021-11-14T10:46:15.184238Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def predict_and_plot(encoder_input_data, decoder_target_data, sample_ind, enc_tail_len=50):\n\n    encode_series = encoder_input_data[sample_ind:sample_ind+1,:,:] \n    pred_series = predict_sequence(encode_series)\n    \n    encode_series = encode_series.reshape(-1,1)\n    pred_series = pred_series.reshape(-1,1)   \n    target_series = decoder_target_data[sample_ind,:,:1].reshape(-1,1) \n    \n    encode_series_tail = np.concatenate([encode_series[-enc_tail_len:],target_series[:1]])\n    x_encode = encode_series_tail.shape[0]\n    \n    plt.figure(figsize=(10,6))   \n    \n    plt.plot(range(1,x_encode+1),encode_series_tail)\n    plt.plot(range(x_encode,x_encode+pred_steps),target_series,color='orange')\n    plt.plot(range(x_encode,x_encode+pred_steps),pred_series,color='teal',linestyle='--')\n    \n    plt.title('Encoder Series Tail of Length %d, Target Series, and Predictions' % enc_tail_len)\n    plt.legend(['Encoding Series','Target Series','Predictions'])","metadata":{"execution":{"iopub.status.busy":"2021-11-14T10:46:15.186185Z","iopub.execute_input":"2021-11-14T10:46:15.186470Z","iopub.status.idle":"2021-11-14T10:46:15.194608Z","shell.execute_reply.started":"2021-11-14T10:46:15.186421Z","shell.execute_reply":"2021-11-14T10:46:15.193771Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"Generating some plots as below, we can see that our predictions look better than in the previous notebook. They can effectively anticipate many patterns in the data (e.g. behavior across different week days) and capture some trends nicely. They are definitely more sensitive to the variability in the data than the overly conservative LSTM predictions from the previous notebook. \n\nStill, we would likely stand to gain even more from increasing the sample size for training and expanding on the network architecture/hyperparameter tuning.  \n\n**Check out the next notebook in this series** for further exploration of the WaveNet architecture, including fancier components like gated activations and skip connections. If you're interested in digging even deeper into state of the art WaveNet style architectures, I also highly recommend checking out [Sean Vasquez's model](https://github.com/sjvasquez/web-traffic-forecasting) that was designed for this data set. He implements a customized seq2seq WaveNet architecture in tensorflow.    ","metadata":{}},{"cell_type":"code","source":"predict_and_plot(encoder_input_data, decoder_target_data, 100)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T10:46:15.197162Z","iopub.execute_input":"2021-11-14T10:46:15.197746Z","iopub.status.idle":"2021-11-14T10:46:16.854267Z","shell.execute_reply.started":"2021-11-14T10:46:15.197708Z","shell.execute_reply":"2021-11-14T10:46:16.853573Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"predict_and_plot(encoder_input_data, decoder_target_data, 6007)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T10:46:16.855494Z","iopub.execute_input":"2021-11-14T10:46:16.855839Z","iopub.status.idle":"2021-11-14T10:46:17.747310Z","shell.execute_reply.started":"2021-11-14T10:46:16.855807Z","shell.execute_reply":"2021-11-14T10:46:17.745703Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"predict_and_plot(encoder_input_data, decoder_target_data, 33000)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T10:46:17.748659Z","iopub.execute_input":"2021-11-14T10:46:17.749175Z","iopub.status.idle":"2021-11-14T10:46:18.736583Z","shell.execute_reply.started":"2021-11-14T10:46:17.749133Z","shell.execute_reply":"2021-11-14T10:46:18.735844Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"predict_and_plot(encoder_input_data, decoder_target_data, 110005)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T10:46:18.738086Z","iopub.execute_input":"2021-11-14T10:46:18.738583Z","iopub.status.idle":"2021-11-14T10:46:19.589521Z","shell.execute_reply.started":"2021-11-14T10:46:18.738541Z","shell.execute_reply":"2021-11-14T10:46:19.588855Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"predict_and_plot(encoder_input_data, decoder_target_data, 70000)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T10:46:19.590854Z","iopub.execute_input":"2021-11-14T10:46:19.591320Z","iopub.status.idle":"2021-11-14T10:46:20.405474Z","shell.execute_reply.started":"2021-11-14T10:46:19.591280Z","shell.execute_reply":"2021-11-14T10:46:20.404814Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"This last prediction example is interesting - the model clearly understands the recurring pattern in the series well, but struggles to properly capture the downward trend that's in place.","metadata":{}},{"cell_type":"code","source":"","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}